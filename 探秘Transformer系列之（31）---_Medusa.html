<!DOCTYPE html>
<html lang="zh-cn">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="referrer" content="origin-when-cross-origin" />
    <meta name="keywords" content="001_æœºå™¨å­¦ä¹ ,006_æ·±åº¦å­¦ä¹ ,019_æ¢ç§˜Transformer" />
    <meta name="description" content="ä»é›¶å¼€å§‹è§£æTransformerï¼Œç›®æ ‡æ˜¯ï¼š(1) è§£æTransformerå¦‚ä½•è¿ä½œï¼Œä»¥åŠä¸ºä½•å¦‚æ­¤è¿ä½œï¼Œè®©æ–°åŒå­¦å¯ä»¥å…¥é—¨ï¼›(2) åŠ›äº‰èå…¥ä¸€äº›æ¯”è¾ƒæ–°çš„æˆ–è€…æœ‰ç‰¹è‰²çš„è®ºæ–‡æˆ–è€…ç†å¿µï¼Œè®©è€é¸Ÿä¹Ÿå¯ä»¥æœ‰æ‰€æ”¶è·ã€‚" />
    <meta property="og:description" content="ä»é›¶å¼€å§‹è§£æTransformerï¼Œç›®æ ‡æ˜¯ï¼š(1) è§£æTransformerå¦‚ä½•è¿ä½œï¼Œä»¥åŠä¸ºä½•å¦‚æ­¤è¿ä½œï¼Œè®©æ–°åŒå­¦å¯ä»¥å…¥é—¨ï¼›(2) åŠ›äº‰èå…¥ä¸€äº›æ¯”è¾ƒæ–°çš„æˆ–è€…æœ‰ç‰¹è‰²çš„è®ºæ–‡æˆ–è€…ç†å¿µï¼Œè®©è€é¸Ÿä¹Ÿå¯ä»¥æœ‰æ‰€æ”¶è·ã€‚" />
    <meta http-equiv="Cache-Control" content="no-transform" />
    <meta http-equiv="Cache-Control" content="no-siteapp" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <title>æ¢ç§˜Transformerç³»åˆ—ä¹‹ï¼ˆ31ï¼‰--- Medusa - ç½—è¥¿çš„æ€è€ƒ - åšå®¢å›­</title>
    <link rel="icon" id="favicon" href="https://assets.cnblogs.com/favicon_v3_2.ico" type="image/x-icon" />
    <link rel="canonical" href="https://www.cnblogs.com/rossiXYZ/p/18845475" />
    
    <link rel="stylesheet" href="/css/blog-common.min.css?v=3DArmf-Or-4qxFZkl3OdynS2Am4I6_pcIbQbRZRdGaM" />
    

    <link id="MainCss" rel="stylesheet" href="/skins/lessismoreright/bundle-lessismoreright.min.css?v=O5zHESxCF0tzyVg01nX06fLeohvC5JYxsLWE4NmQOMg" />
        <link id="highlighter-theme-cnblogs" type="text/css" rel="stylesheet" href="/css/hljs/cnblogs.css?v=5J1NDtbnnIr2Rc2SdhEMlMxD4l9Eydj88B31E7_NhS4" />
    
    
    <link id="mobile-style" media="only screen and (max-width: 767px)" type="text/css" rel="stylesheet" href="/skins/lessismoreright/bundle-lessismoreright-mobile.min.css?v=Uw1Hg7i9RFPazLAd0cWltL-cniUkUgHHPLh7ZV9ZL9o" />
    
    <link type="application/rss+xml" rel="alternate" href="https://www.cnblogs.com/rossiXYZ/rss" />
    <link type="application/rsd+xml" rel="EditURI" href="https://www.cnblogs.com/rossiXYZ/rsd.xml" />
    <link type="application/wlwmanifest+xml" rel="wlwmanifest" href="https://www.cnblogs.com/rossiXYZ/wlwmanifest.xml" />
    
    <script type="application/ld&#x2B;json">
    {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "@id": "https://www.cnblogs.com/rossiXYZ/p/18845475",
      "headline": "æ¢ç§˜Transformerç³»åˆ—ä¹‹ï¼ˆ31ï¼‰--- Medusa",
      "description": "æ¢ç§˜Transformerç³»åˆ—ä¹‹ï¼ˆ31ï¼‰ Medusa ç›®å½•æ¢ç§˜Transformerç³»åˆ—ä¹‹ï¼ˆ31ï¼‰ Medusa0x00 æ¦‚è¿°0x01 åŸç†1.1 åŠ¨æœº1.2 å€Ÿé‰´1.3 æ€è·¯1.3.1 å•æ¨¡å‹ \u0026amp; å¤šå¤´1.3.2 Tree éªŒè¯1.3.3 å°ç»“0x02 è®¾è®¡æ ¸å¿ƒç‚¹2.1 æµç¨‹2.2 æ¨¡å‹ç»“æ„2.",
      "image": [
        
      ],
      "author": {
        "@type": "Person",
        "@id": "https://www.cnblogs.com/rossiXYZ/",
        "name": "ç½—è¥¿çš„æ€è€ƒ",
        "url": "https://www.cnblogs.com/rossiXYZ/"
      },
      "publisher": {
        "@type": "Organization",
        "@id": "https://www.cnblogs.com/",
        "name": "åšå®¢å›­",
        "url": "https://www.cnblogs.com/"
      },
      "datePublished": "2025-04-28T20:42:00.0000000&#x2B;08:00",
      "dateModified": "2025-05-01T23:41:00.0000000&#x2B;08:00",
      "wordCount": "57930",
      "isPartOf": {
        "@type": "Blog",
        "@id": "https://www.cnblogs.com/rossiXYZ/",
        "name": "ç½—è¥¿çš„æ€è€ƒ",
        "publisher": {
          "@type": "Organization",
          "@id": "https://www.cnblogs.com/",
          "name": "åšå®¢å›­"
        }
      }
    }
    </script>

    <script>
        var currentBlogId = 556264;
        var currentBlogApp = 'rossiXYZ';
        var isLogined = false;
        var isBlogOwner = false;
        var skinName = 'LessIsMoreRight';
        var visitorUserId = '';
        var hasCustomScript = false;
        window.cb_enable_mathjax = true;
        window.mathEngine = 0;
        window.codeHighlightEngine = 1;
        window.enableCodeLineNumber = false;
        window.codeHighlightTheme = 'cnblogs';
        window.darkModeCodeHighlightTheme = 'vs2015';
        window.isDarkCodeHighlightTheme = false;
        window.isDarkModeCodeHighlightThemeDark = true;
        window.isDisableCodeHighlighter = false;
        window.enableCodeThemeTypeFollowSystem = false;
        window.enableMacStyleCodeBlock = false;
    </script>
        <script>
            window.currentPostId = 18845475;
            window.currentPostDateAdded = '2025-04-28 20:42';
        </script>
    <script src="https://assets.cnblogs.com/scripts/jquery-3.3.1.min.js"></script>
    <script src="https://cdn-www.cnblogs.com/js/blog-common.min.js?v=wZ-j9lgqsnaTqSE7AdWd3J3j9ENiZHPW0sel6vKY_Mo"></script>
    
</head>
<body class="skin-lessismoreright has-navbar mathjax2">
    <a name="top"></a>
        <div id="imagebar" class="imagebar-mobile imagebar-text-mobile formobile">
                <a href="https://www.doubao.com?channel=cnblogs&amp;source=hw_db_cnblogs&amp;type=lunt&amp;theme=bianc" onclick="countCreativeClicks('M2-å­—èŠ‚-è±†åŒ…')" rel="nofollow">
                    <img src="https://img2024.cnblogs.com/blog/35695/202412/35695-20241201073014811-1847930772.jpg" alt="" onload="countCreativeImpressionsOnMobile('M2-å­—èŠ‚-è±†åŒ…')" />
                    <span id="m2_impression" style="display:none"></span>
                </a>
        </div>
    <div id="top_nav" class="navbar forpc">
        <nav id="nav_main" class="navbar-main">
            <ul id="nav_left" class="navbar-list navbar-left">
                <li class="navbar-branding">                    
                    <a href="https://www.cnblogs.com/" title="å¼€å‘è€…çš„ç½‘ä¸Šå®¶å›­" role="banner">
                        <img src="//assets.cnblogs.com/logo.svg" alt="åšå®¢å›­logo" />
                    </a>
                </li>               
                <li><a href="https://cnblogs.vip/">ä¼šå‘˜</a></li>
                <li><a href="https://cnblogs.vip/store">å‘¨è¾¹</a></li>
                <li><a href="https://news.cnblogs.com/" onclick="countClicks('nav', 'skin-navbar-news')">æ–°é—»</a></li>
                <li><a href="https://q.cnblogs.com/" onclick="countClicks('nav', 'skin-navbar-q')">åšé—®</a></li>
                <li><a href="https://ing.cnblogs.com/" onclick="countClicks('nav', 'skin-navbar-ing')">é—ªå­˜</a></li>
                <li><a href="https://www.cnblogs.com/cmt/p/18341478">èµåŠ©å•†</a></li>
                <li><a href="https://chat2db-ai.com/" target="_blank" onclick="countClicks('nav', 'skin-navbar-chat2db')">Chat2DB</a></li>
            </ul>
            <ul id="nav_right" class="navbar-list navbar-right">
                <li>
                    <form id="zzk_search" class="navbar-search dropdown" action="https://zzk.cnblogs.com/s" method="get" role="search">
                        <input name="w" id="zzk_search_input" placeholder="ä»£ç æ”¹å˜ä¸–ç•Œ" type="search" tabindex="3" autocomplete="off" />
                        <button id="zzk_search_button" onclick="window.navbarSearchManager.triggerActiveOption()">
                            <img id="search_icon" class="focus-hidden" src="//assets.cnblogs.com/icons/search.svg" alt="æœç´¢" />
                            <img class="hidden focus-visible" src="//assets.cnblogs.com/icons/enter.svg" alt="æœç´¢" />
                        </button>
                        <ul id="navbar_search_options" class="dropdown-menu quick-search-menu">
                            <li tabindex="0" class="active" onclick="zzkSearch(event, document.getElementById('zzk_search_input').value)">
                                <div class="keyword-wrapper">
                                    <img src="//assets.cnblogs.com/icons/search.svg" alt="æœç´¢" />
                                    <div class="keyword"></div>
                                </div>
                                <span class="search-area">æ‰€æœ‰åšå®¢</span>
                            </li>
                                    <li tabindex="1" onclick="zzkBlogSearch(event, 'rossiXYZ', document.getElementById('zzk_search_input').value)">
                                        <div class="keyword-wrapper">
                                            <img src="//assets.cnblogs.com/icons/search.svg" alt="æœç´¢" />
                                            <div class="keyword"></div>
                                        </div>
                                        <span class="search-area">å½“å‰åšå®¢</span>
                                    </li>
                        </ul>
                    </form>
                </li>
                <li id="navbar_login_status" class="navbar-list">
                    <a class="navbar-user-info navbar-blog" href="https://i.cnblogs.com/EditPosts.aspx?opt=1" alt="å†™éšç¬”" title="å†™éšç¬”">
                        <img id="new_post_icon" class="navbar-icon" src="//assets.cnblogs.com/icons/newpost.svg" alt="å†™éšç¬”" />
                    </a>
                    <a id="navblog-myblog-icon" class="navbar-user-info navbar-blog" href="https://passport.cnblogs.com/GetBlogApplyStatus.aspx" alt="æˆ‘çš„åšå®¢" title="æˆ‘çš„åšå®¢">
                        <img id="myblog_icon" class="navbar-icon" src="//assets.cnblogs.com/icons/myblog.svg" alt="æˆ‘çš„åšå®¢" />
                    </a>
                    <a class="navbar-user-info navbar-message navbar-icon-wrapper" href="https://msg.cnblogs.com/" alt="çŸ­æ¶ˆæ¯" title="çŸ­æ¶ˆæ¯">
                        <img id="msg_icon" class="navbar-icon" src="//assets.cnblogs.com/icons/message.svg" alt="çŸ­æ¶ˆæ¯" />
                        <span id="msg_count" style="display: none"></span>
                    </a>
                    <a id="navbar_lite_mode_indicator" data-current-page="blog" style="display: none" href="javascript:void(0)" alt="ç®€æ´æ¨¡å¼" title="ç®€æ´æ¨¡å¼å¯ç”¨ï¼Œæ‚¨åœ¨è®¿é—®ä»–äººåšå®¢æ—¶ä¼šä½¿ç”¨ç®€æ´æ¬¾çš®è‚¤å±•ç¤º">
                        <img class="navbar-icon" src="//assets.cnblogs.com/icons/lite-mode-on.svg" alt="ç®€æ´æ¨¡å¼" />
                    </a>
                    <div id="user_info" class="navbar-user-info dropdown">
                        <a class="dropdown-button" href="https://home.cnblogs.com/">
                            <img id="user_icon" class="navbar-avatar" src="//assets.cnblogs.com/icons/avatar-default.svg" alt="ç”¨æˆ·å¤´åƒ" />
                        </a>
                        <div class="dropdown-menu">
                            <a id="navblog-myblog-text" href="https://passport.cnblogs.com/GetBlogApplyStatus.aspx">æˆ‘çš„åšå®¢</a>
                            <a href="https://home.cnblogs.com/">æˆ‘çš„å›­å­</a>
                            <a href="https://account.cnblogs.com/settings/account">è´¦å·è®¾ç½®</a>
                            <a href="https://vip.cnblogs.com/my">ä¼šå‘˜ä¸­å¿ƒ</a>
                            <a href="javascript:void(0)" id="navbar_lite_mode_toggle" title="ç®€æ´æ¨¡å¼ä¼šä½¿ç”¨ç®€æ´æ¬¾çš®è‚¤æ˜¾ç¤ºæ‰€æœ‰åšå®¢">
    ç®€æ´æ¨¡å¼ <span id="navbar_lite_mode_spinner" class="hide">...</span>
</a>

                            <a href="javascript:void(0)" onclick="account.logout();">é€€å‡ºç™»å½•</a>
                        </div>
                    </div>
                    <a class="navbar-anonymous" href="https://account.cnblogs.com/signup">æ³¨å†Œ</a>
                    <a class="navbar-anonymous" href="javascript:void(0);" onclick="account.login()">ç™»å½•</a>
                </li>
            </ul>
        </nav>
    </div>

    <div id="page_begin_html">
        

    </div>

    <div id="home">
    <div id="header">
        <div id="blogTitle">
            <div class="title"><a id="Header1_HeaderTitle" class="headermaintitle HeaderMainTitle" href="https://www.cnblogs.com/rossiXYZ">ç½—è¥¿çš„æ€è€ƒ</a>
</div>
<div class="subtitle">ä¸€æ‰‹ä¼¸å‘æŠ€æœ¯ï¼Œä¸€æ‰‹ä¼¸å‘ç”Ÿæ´»</div>

        </div>
        <div id="navigator">
            
<ul id="navList">
    <li id="nav_sitehome"><a id="blog_nav_sitehome" class="menu" href="https://www.cnblogs.com/">
åšå®¢å›­</a>
</li>
    <li id="nav_myhome">
<a id="blog_nav_myhome" class="menu" href="https://www.cnblogs.com/rossiXYZ/">
é¦–é¡µ</a>
</li>
    <li id="nav_newpost">

<a id="blog_nav_newpost" class="menu" href="https://i.cnblogs.com/EditPosts.aspx?opt=1">
æ–°éšç¬”</a>
</li>
    <li id="nav_contact">
<a id="blog_nav_contact" class="menu" href="https://msg.cnblogs.com/send/%E7%BD%97%E8%A5%BF%E7%9A%84%E6%80%9D%E8%80%83">
è”ç³»</a></li>
    <li id="nav_rss">
<a id="blog_nav_rss" class="menu" href="javascript:void(0)" data-rss="https://www.cnblogs.com/rossiXYZ/rss/">
è®¢é˜…</a></li>
    <li id="nav_admin">
<a id="blog_nav_admin" class="menu" href="https://i.cnblogs.com/">
ç®¡ç†</a>
</li>
</ul>

            <div class="blogStats">
                <div id="blog_stats_place_holder"><script>loadBlogStats();</script></div>
            </div>
        </div>
    </div>
    <div id="main">
        <div id="mainContent">
            <div class="forFlow">
                <div id="post_detail">
    <div id="topics">
        <div class="post">
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/rossiXYZ/p/18845475" title="å‘å¸ƒäº 2025-04-28 20:42">
    <span role="heading" aria-level="2">æ¢ç§˜Transformerç³»åˆ—ä¹‹ï¼ˆ31ï¼‰--- Medusa</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                    <div id="cnblogs_post_description" style="display: none">
        
        ä»é›¶å¼€å§‹è§£æTransformerï¼Œç›®æ ‡æ˜¯ï¼š(1) è§£æTransformerå¦‚ä½•è¿ä½œï¼Œä»¥åŠä¸ºä½•å¦‚æ­¤è¿ä½œï¼Œè®©æ–°åŒå­¦å¯ä»¥å…¥é—¨ï¼›(2) åŠ›äº‰èå…¥ä¸€äº›æ¯”è¾ƒæ–°çš„æˆ–è€…æœ‰ç‰¹è‰²çš„è®ºæ–‡æˆ–è€…ç†å¿µï¼Œè®©è€é¸Ÿä¹Ÿå¯ä»¥æœ‰æ‰€æ”¶è·ã€‚
    </div>
<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h1 id="æ¢ç§˜transformerç³»åˆ—ä¹‹31----medusa">æ¢ç§˜Transformerç³»åˆ—ä¹‹ï¼ˆ31ï¼‰--- Medusa</h1>
<p></p><div class="toc"><div class="toc-container-header">ç›®å½•</div><ul><li><a href="#æ¢ç§˜transformerç³»åˆ—ä¹‹31----medusa" rel="noopener nofollow">æ¢ç§˜Transformerç³»åˆ—ä¹‹ï¼ˆ31ï¼‰--- Medusa</a><ul><li><a href="#0x00-æ¦‚è¿°" rel="noopener nofollow">0x00 æ¦‚è¿°</a></li><li><a href="#0x01-åŸç†" rel="noopener nofollow">0x01 åŸç†</a><ul><li><a href="#11-åŠ¨æœº" rel="noopener nofollow">1.1 åŠ¨æœº</a></li><li><a href="#12-å€Ÿé‰´" rel="noopener nofollow">1.2 å€Ÿé‰´</a></li><li><a href="#13-æ€è·¯" rel="noopener nofollow">1.3 æ€è·¯</a><ul><li><a href="#131-å•æ¨¡å‹--å¤šå¤´" rel="noopener nofollow">1.3.1 å•æ¨¡å‹ &amp; å¤šå¤´</a></li><li><a href="#132-tree-éªŒè¯" rel="noopener nofollow">1.3.2 Tree éªŒè¯</a></li><li><a href="#133-å°ç»“" rel="noopener nofollow">1.3.3 å°ç»“</a></li></ul></li></ul></li><li><a href="#0x02-è®¾è®¡æ ¸å¿ƒç‚¹" rel="noopener nofollow">0x02 è®¾è®¡æ ¸å¿ƒç‚¹</a><ul><li><a href="#21-æµç¨‹" rel="noopener nofollow">2.1 æµç¨‹</a></li><li><a href="#22-æ¨¡å‹ç»“æ„" rel="noopener nofollow">2.2 æ¨¡å‹ç»“æ„</a></li><li><a href="#23-å¤šå¤´" rel="noopener nofollow">2.3 å¤šå¤´</a><ul><li><a href="#231-headç»“æ„" rel="noopener nofollow">2.3.1 headç»“æ„</a></li><li><a href="#232-ä½ç½®" rel="noopener nofollow">2.3.2 ä½ç½®</a></li></ul></li><li><a href="#24-ç¼ºç‚¹" rel="noopener nofollow">2.4 ç¼ºç‚¹</a></li></ul></li><li><a href="#0x03-tree-verification" rel="noopener nofollow">0x03 Tree Verification</a><ul><li><a href="#31-è§£ç è·¯å¾„" rel="noopener nofollow">3.1 è§£ç è·¯å¾„</a></li><li><a href="#32-æœ€ä½³æ„é€ æ–¹å¼" rel="noopener nofollow">3.2 æœ€ä½³æ„é€ æ–¹å¼</a></li><li><a href="#33-å®ç°" rel="noopener nofollow">3.3 å®ç°</a><ul><li><a href="#331-å…³é”®å˜é‡" rel="noopener nofollow">3.3.1 å…³é”®å˜é‡</a></li><li><a href="#332-ç¤ºä¾‹ä»£ç " rel="noopener nofollow">3.3.2 ç¤ºä¾‹ä»£ç </a></li><li><a href="#333-æ€»ä½“å¯è§†åŒ–" rel="noopener nofollow">3.3.3 æ€»ä½“å¯è§†åŒ–</a></li><li><a href="#334-ä½¿ç”¨" rel="noopener nofollow">3.3.4 ä½¿ç”¨</a><ul><li><a href="#è°ƒç”¨" rel="noopener nofollow">è°ƒç”¨</a></li><li><a href="#åˆå§‹åŒ–" rel="noopener nofollow">åˆå§‹åŒ–</a></li><li><a href="#ç”Ÿæˆå€™é€‰è·¯å¾„" rel="noopener nofollow">ç”Ÿæˆå€™é€‰è·¯å¾„</a></li><li><a href="#éªŒè¯å€™é€‰è·¯å¾„" rel="noopener nofollow">éªŒè¯å€™é€‰è·¯å¾„</a></li><li><a href="#è®¡ç®—æœ€ä¼˜è·¯å¾„" rel="noopener nofollow">è®¡ç®—æœ€ä¼˜è·¯å¾„</a></li></ul></li></ul></li><li><a href="#34-typical-acceptance" rel="noopener nofollow">3.4 Typical Acceptance</a><ul><li><a href="#341-å¸¸è§é‡‡ç”¨æ–¹æ³•" rel="noopener nofollow">3.4.1 å¸¸è§é‡‡ç”¨æ–¹æ³•</a></li><li><a href="#342-æ€è·¯" rel="noopener nofollow">3.4.2 æ€è·¯</a></li><li><a href="#343-typical-acceptance" rel="noopener nofollow">3.4.3 Typical Acceptance</a></li></ul></li></ul></li><li><a href="#0x04-è®­ç»ƒ" rel="noopener nofollow">0x04 è®­ç»ƒ</a><ul><li><a href="#41-medusa-1" rel="noopener nofollow">4.1 MEDUSA-1</a></li><li><a href="#42-medusa-2" rel="noopener nofollow">4.2 MEDUSA-2</a></li><li><a href="#43-ä»£ç " rel="noopener nofollow">4.3 ä»£ç </a></li></ul></li><li><a href="#0x05-decoding" rel="noopener nofollow">0x05 Decoding</a><ul><li><a href="#51-ç¤ºä¾‹" rel="noopener nofollow">5.1 ç¤ºä¾‹</a></li><li><a href="#52-è®¡ç®—å’Œç©ºé—´å¤æ‚åº¦" rel="noopener nofollow">5.2 è®¡ç®—å’Œç©ºé—´å¤æ‚åº¦</a></li></ul></li><li><a href="#0xff-å‚è€ƒ" rel="noopener nofollow">0xFF å‚è€ƒ</a></li></ul></li></ul></div><p></p>
<h2 id="0x00-æ¦‚è¿°">0x00 æ¦‚è¿°</h2>
<p>Medusa æ˜¯è‡ªæŠ•æœºé¢†åŸŸè¾ƒæ—©çš„ä¸€ç¯‡å·¥ä½œï¼Œå¯¹åç»­å·¥ä½œå¯å‘å¾ˆå¤§ï¼Œå…¶ä¸»è¦æ€æƒ³æ˜¯multi-decoding head + tree attention + typical acceptance(threshold)ã€‚Medusa æ²¡æœ‰ä½¿ç”¨ç‹¬ç«‹çš„è‰ç¨¿æ¨¡å‹ï¼Œè€Œæ˜¯åœ¨åŸå§‹æ¨¡å‹çš„åŸºç¡€ä¸Šå¢åŠ å¤šä¸ªè§£ç å¤´ï¼ˆMEDUSA headsï¼‰ï¼Œå¹¶è¡Œé¢„æµ‹å¤šä¸ªåç»­ tokenã€‚</p>
<p>æ­£å¸¸çš„LLMåªæœ‰ä¸€ä¸ªç”¨äºé¢„æµ‹tæ—¶åˆ»tokençš„headã€‚Medusa åœ¨ LLM çš„æœ€åä¸€ä¸ª Transformerå±‚ä¹‹åä¿ç•™åŸå§‹çš„ LM Headï¼Œç„¶åé¢å¤–å¢åŠ å¤šä¸ªï¼ˆå‡è®¾æ˜¯kä¸ªï¼‰ å¯è®­ç»ƒçš„Medusa Headï¼ˆè§£ç å¤´ï¼‰ï¼Œåˆ†åˆ«è´Ÿè´£é¢„æµ‹t+1ï¼Œt+2ï¼Œ...ï¼Œå’Œt+kæ—¶åˆ»çš„ä¸åŒä½ç½®çš„å¤šä¸ª Tokenã€‚Medusa è®©æ¯ä¸ªå¤´ç”Ÿæˆå¤šä¸ªå€™é€‰ tokenï¼Œè€ŒéåƒæŠ•æœºè§£ç é‚£æ ·åªç”Ÿæˆä¸€ä¸ªå€™é€‰ã€‚ç„¶åå°†æ‰€æœ‰çš„å€™é€‰ç»“æœç»„è£…æˆå¤šä¸ªå€™é€‰åºåˆ—ï¼Œå¤šä¸ªå€™é€‰åºåˆ—åˆæ„æˆä¸€æ£µæ ‘ã€‚å†é€šè¿‡æ ‘æ³¨æ„åŠ›æœºåˆ¶å¹¶è¡ŒéªŒè¯è¿™äº›å€™é€‰åºåˆ—ã€‚</p>
<hr>
<p><strong>æ³¨ï¼šå…¨éƒ¨æ–‡ç« åˆ—è¡¨åœ¨è¿™é‡Œï¼Œä¼°è®¡æœ€ç»ˆåœ¨35ç¯‡å·¦å³ï¼Œåç»­æ¯å‘ä¸€ç¯‡æ–‡ç« ï¼Œä¼šä¿®æ”¹æ­¤æ–‡ç« åˆ—è¡¨ã€‚</strong><br>
<strong>cnblogs <a href="https://www.cnblogs.com/rossiXYZ/p/18785601" target="_blank">æ¢ç§˜Transformerç³»åˆ—ä¹‹æ–‡ç« åˆ—è¡¨</a></strong></p>
<hr>
<h2 id="0x01-åŸç†">0x01 åŸç†</h2>
<h3 id="11-åŠ¨æœº">1.1 åŠ¨æœº</h3>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250424214146021-1241116676.jpg" alt="" loading="lazy"></p>
<p>æŠ•æœºé‡‡æ ·çš„æ ¸å¿ƒæ€è·¯å¦‚ä¸Šå›¾ä¸‹æ–¹æ‰€ç¤ºï¼Œé¦–å…ˆä»¥ä½æˆæœ¬çš„æ–¹å¼ï¼ˆä¸€èˆ¬æ¥è¯´æ˜¯ç”¨å°æ¨¡å‹ï¼‰å¿«é€Ÿç”Ÿæˆå¤šä¸ªå€™é€‰ Tokenï¼Œç„¶åé€šè¿‡ä¸€æ¬¡å¹¶è¡ŒéªŒè¯é˜¶æ®µå¿«é€ŸéªŒè¯å¤šä¸ª Tokenï¼Œè¿›è€Œå‡å°‘å¤§æ¨¡å‹çš„ Decoding Stepï¼Œå®ç°åŠ é€Ÿçš„ç›®çš„ã€‚ç„¶è€Œï¼Œé‡‡ç”¨ä¸€ä¸ªç‹¬ç«‹çš„â€œæ¨æµ‹â€æ¨¡å‹ä¹Ÿæœ‰ç¼ºç‚¹ï¼Œå…·ä½“å¦‚ä¸‹ï¼š</p>
<ul>
<li>å¾ˆéš¾æ‰¾åˆ°ä¸€ä¸ªå°è€Œå¼ºçš„æ¨¡å‹æ¥ç”Ÿæˆå¯¹äºåŸå§‹çš„æ¨¡å‹æ¥è¯´æ¯”è¾ƒç®€å•çš„tokenã€‚
<ul>
<li>draftæ¨¡å‹å’Œå¤§æ¨¡å‹å¾ˆéš¾å¯¹é½ï¼Œå­˜åœ¨distribution shiftã€‚</li>
<li>å¹¶ä¸æ˜¯æ‰€æœ‰çš„LLMéƒ½èƒ½æ‰¾åˆ°ç°æˆçš„å°æ¨¡å‹ã€‚é‡æ–°è®­ç»ƒä¸€ä¸ªå°æ¨¡å‹éœ€è¦è¾ƒå¤šçš„é¢å¤–æŠ•å…¥ã€‚</li>
</ul>
</li>
<li>åœ¨ä¸€ä¸ªç³»ç»Ÿä¸­ç»´æŠ¤2ä¸ªä¸åŒçš„æ¨¡å‹ï¼Œå³å¢åŠ äº†æ¨ç†è¿‡ç¨‹çš„è®¡ç®—å¤æ‚åº¦ï¼Œä¹Ÿå¯¼è‡´æ¶æ„ä¸Šçš„å¤æ‚æ€§ï¼Œåœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸Šçš„éƒ¨ç½²éš¾åº¦å¢å¤§ã€‚</li>
<li>ä½¿ç”¨æŠ•æœºé‡‡æ ·çš„æ—¶å€™ï¼Œä¼šå¸¦æ¥é¢å¤–çš„è§£ç å¼€é”€ï¼Œå°¤å…¶æ˜¯å½“ä½¿ç”¨ä¸€ä¸ªæ¯”è¾ƒé«˜çš„é‡‡æ ·æ¸©åº¦å€¼æ—¶ã€‚</li>
</ul>
<h3 id="12-å€Ÿé‰´">1.2 å€Ÿé‰´</h3>
<p>Meduaä¸»è¦å€Ÿé‰´äº†ä¸¤ä¸ªå·¥ä½œï¼šBPDå’ŒSpecInferã€‚</p>
<ul>
<li>
<p>å¤§æ¨¡å‹è‡ªèº«å¸¦æœ‰ä¸€ä¸ªLM headï¼Œç”¨äºæŠŠéšè—å±‚è¾“å‡ºæ˜ å°„åˆ°è¯è¡¨çš„æ¦‚ç‡åˆ†å¸ƒï¼Œä»¥å®ç°å•ä¸ªtokençš„è§£ç ã€‚ä¸ºäº†ç”Ÿæˆå¤šä¸ªtokenï¼Œè®ºæ–‡â€œBlockwise Parallel Decoding for Deep Autoregressive Modelsâ€åœ¨éª¨å¹²æ¨¡å‹ä¸Šä½¿ç”¨å¤šä¸ªè§£ç å¤´æ¥åŠ é€Ÿæ¨ç†ï¼Œé€šè¿‡è®­ç»ƒè¾…åŠ©æ¨¡å‹ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿé¢„æµ‹æœªæ¥ä½ç½®çš„è¾“å‡ºï¼Œç„¶ååˆ©ç”¨è¿™äº›é¢„æµ‹ç»“æœæ¥è·³è¿‡éƒ¨åˆ†è´ªå¿ƒè§£ç æ­¥éª¤ï¼Œä»è€ŒåŠ é€Ÿè§£ç è¿‡ç¨‹ã€‚</p>
</li>
<li>
<p>è®ºæ–‡â€œSpecInfer: Accelerating Generative Large Language Model Serving with Speculative Inference and Token Tree Verificationâ€çš„æ€è·¯æ˜¯ï¼šæ—¢ç„¶å°æ¨¡å‹å¯ä»¥çŒœæµ‹å¤§æ¨¡å‹çš„è¾“å‡ºå¹¶ä¸”æ•ˆç‡éå¸¸é«˜ï¼Œé‚£ä¹ˆä¸€æ ·å¯ä»¥ä½¿ç”¨å¤šä¸ªå°æ¨¡å‹æ¥çŒœæµ‹å¤šä¸ª Token åºåˆ—ï¼Œè¿™æ ·æä¾›çš„å€™é€‰æ›´å¤šï¼ŒçŒœå¯¹çš„æœºä¼šä¹Ÿæ›´å¤§ï¼›ä¸ºäº†æå‡è¿™å¤šä¸ª Token åºåˆ—çš„éªŒè¯æ•ˆç‡ï¼Œä½œè€…æå‡º Token Tree Attention çš„æœºåˆ¶ï¼Œé¦–å…ˆå°†å¤šä¸ªå°æ¨¡å‹ç”Ÿæˆçš„å¤šä¸ª Token åºåˆ—ç»„åˆæˆ Token æ ‘ï¼Œç„¶åå°†å…¶å±•å¼€è¾“å…¥æ¨¡å‹ï¼Œå³å¯å®ç°ä¸€æ¬¡ decoding step å®Œæˆæ•´ä¸ª Token æ ‘çš„éªŒè¯ã€‚</p>
</li>
</ul>
<h3 id="13-æ€è·¯">1.3 æ€è·¯</h3>
<p>åŸºäºè¿™ä¸¤ä¸ªæ€è·¯æ¥æºï¼ŒMedusaå†³å®šè®©target LLMè‡ªå·±è¿›è¡Œé¢„æµ‹ï¼Œå³åœ¨target LLMæœ€åä¸€å±‚decoder layerä¹‹ä¸Šå¼•å…¥äº†å¤šä¸ªé¢å¤–çš„é¢„æµ‹å¤´ï¼Œä½¿å¾—æ¨¡å‹å¯ä»¥åœ¨æ¯ä¸ªè§£ç æ­¥å¹¶è¡Œç”Ÿæˆå¤šä¸ªtokenï¼Œä½œä¸ºâ€œæ¨æµ‹â€ç»“æœã€‚æˆ‘ä»¬è¿›è¡Œå…·ä½“åˆ†æã€‚</p>
<h4 id="131-å•æ¨¡å‹--å¤šå¤´">1.3.1 å•æ¨¡å‹ &amp; å¤šå¤´</h4>
<p>ä¸ºäº†æŠ›å¼ƒç‹¬ç«‹çš„ Draft Modelï¼Œåªä¿ç•™ä¸€ä¸ªæ¨¡å‹ï¼ŒåŒæ—¶ä¿ç•™ Draft-then-Verify èŒƒå¼ï¼ŒMedusa åœ¨ä¸»å¹²æ¨¡å‹çš„æœ€ç»ˆéšè—å±‚ä¹‹åæ·»åŠ äº†è‹¥å¹²ä¸ª Medusa Headsï¼Œæ¯ä¸ªè§£ç å¤´æ˜¯ä¸€ä¸ªå¸¦æ®‹å·®è¿æ¥çš„å•å±‚å‰é¦ˆç½‘ç»œã€‚è¿™äº›Medusa Headsæ˜¯å¯¹BPDä¸­å¤š Head çš„å‡çº§ï¼Œå³ç”±åŸæ¥çš„ä¸€ä¸ª Head ç”Ÿæˆä¸€ä¸ª Token å˜æˆä¸€ä¸ª head ç”Ÿæˆå¤šä¸ªå€™é€‰ Tokenã€‚å› ä¸ºè¿™äº› Heads å…·æœ‰é¢„æµ‹å¯¹åº”ä½ç½® token çš„èƒ½åŠ›ï¼Œå¹¶ä¸”å¯ä»¥å¹¶è¡Œåœ°æ‰§è¡Œï¼Œå› æ­¤å¯ä»¥å®ç°åœ¨ä¸€æ¬¡å‰å‘ä¸­å¾—åˆ°å¤šä¸ª draft tokensã€‚å…·ä½“å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚</p>
<p>å¯èƒ½æœ‰è¯»è€…ä¼šæœ‰ç–‘é—®ï¼Œåé¢å‡ ä¸ªheadè¦è·¨è¯é¢„æµ‹ï¼Œå…¶å‡†ç¡®ç‡åº”è¯¥å¾ˆéš¾ä¿è¯å§ï¼Ÿç¡®å®æ˜¯è¿™æ ·çš„ï¼Œä½†æ˜¯ï¼Œå¦‚æœæˆ‘æ¯ä¸ªé¢„æµ‹æ—¶é—´æ­¥éƒ½å–top3å‡ºæ¥ï¼Œé‚£ä¹ˆæœ€ç»ˆé¢„æµ‹æˆåŠŸçš„æ¦‚ç‡å°±é«˜ä¸å°‘äº†ã€‚è€Œä¸”ï¼ŒMedusa ä½œè€…è§‚å¯Ÿåˆ°ï¼Œè™½ç„¶åœ¨é¢„æµ‹ next next Token çš„æ—¶å€™ top1 çš„å‡†ç¡®ç‡å¯èƒ½åªæœ‰ 60%ï¼Œä½†æ˜¯å¦‚æœé€‰æ‹© top5ï¼Œåˆ™å‡†ç¡®ç‡æœ‰å¯èƒ½è¶…è¿‡ 80%ã€‚è€Œä¸”ï¼Œå› ä¸º MEDUSA è§£ç å¤´ä¸åŸå§‹æ¨¡å‹å…±äº«éšè—å±‚çŠ¶æ€ï¼Œæ‰€ä»¥åˆ†å¸ƒå·®å¼‚è¾ƒå°ã€‚</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250424214201315-1054940840.jpg" alt="" loading="lazy"></p>
<h4 id="132-tree-éªŒè¯">1.3.2 Tree éªŒè¯</h4>
<p>å› ä¸ºè´ªå¿ƒè§£ç çš„æ­£ç¡®ç‡ä¸å¤Ÿé«˜ï¼ŒåŠ é€Ÿæ•ˆæœä¸å¤Ÿæ˜¾è‘—ï¼Œå› æ­¤Medusaè®©æ¯ä¸ªHeadè§£ç top-kä¸ªå€™é€‰ï¼Œä¸åŒheadçš„å€™é€‰é›†åˆç»„æˆä¸€ä¸ªæ ‘çŠ¶ç»“æ„ã€‚ä¸ºäº†æ›´é«˜æ•ˆåœ°éªŒè¯è¿™äº› draft tokensï¼ŒMedusaæ ¹æ®è¿™äº› Head ç”Ÿæˆ Token çš„ç¬›å¡å°”ç§¯æ¥æ„å»ºå‡ºå¤šä¸ª Token åºåˆ—ã€‚ç„¶åä½¿ç”¨Tree Attentionæ–¹æ³•ï¼Œåœ¨æ³¨æ„åŠ›è®¡ç®—ä¸­ï¼Œåªå…è®¸åŒä¸€å»¶ç»­ä¸­çš„ token äº’ç›¸çœ‹åˆ°ï¼ˆattention maskï¼‰ï¼Œå†åŠ ä¸Šä½ç½®ç¼–ç çš„é…åˆï¼Œå°±å¯ä»¥åœ¨ä¸å¢åŠ  batch size çš„æƒ…å†µä¸‹å¹¶è¡Œå¤„ç†å¤šä¸ªå€™é€‰ã€‚</p>
<p>Medusa ä¸­çš„æ ‘å’Œæ³¨æ„åŠ›æ©ç çŸ©é˜µå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚åœ¨æ¯ä¸€è·³ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°å›¾ä¸­Medusaä¿ç•™äº†å¤šä¸ªå¯èƒ½çš„tokenï¼Œä¹Ÿå°±æ˜¯æ¦‚ç‡æœ€é«˜çš„å‡ ä¸ªtokenã€‚è¿™æ ·æ„æˆäº†æ‰€è°“çš„æ ‘ç»“æ„ï¼Œç›´è§‚æ¥è¯´ï¼Œå°±æ˜¯æ¯1è·³çš„æ¯1ä¸ªtokenéƒ½å¯èƒ½å’Œä¸‹1è·³çš„æ‰€æœ‰tokenç»„åˆæˆå¥å­ï¼Œä¹Ÿå¯ä»¥å°±åœ¨è¿™1è·³ç»ˆæ­¢ã€‚ä¾‹å¦‚ï¼Œåœ¨å›¾ä¸­ï¼Œä¸€å…±2ä¸ªheadç”Ÿæˆäº†2è·³çš„tokenï¼Œé‚£ä¹ˆè¿™æ£µæ ‘åŒ…å«äº†6ç§å¯èƒ½çš„å¥å­ï¼šHead 1 åœ¨ä¸‹ä¸€ä¸ªä½ç½®ç”Ÿæˆ 2 ä¸ªå¯èƒ½çš„ Tokenï¼ˆIt å’Œ Iï¼‰ï¼ŒHead 2 åœ¨ä¸‹ä¸‹ä¸€ä¸ªä½ç½®ç”Ÿæˆ 3 ä¸ªå¯èƒ½çš„ Tokenï¼ˆisï¼Œâ€™ å’Œ theï¼‰ï¼Œè¿™æ ·ä¸‹ä¸€ä¸ªä½ç½®å’Œä¸‹ä¸‹ä¸€ä¸ªä½ç½®å°±æœ‰äº† 2 x 3 = 6 ç§å¯èƒ½çš„å€™é€‰åºåˆ—ï¼Œå¦‚ä¸‹å›¾å·¦ä¾§æ‰€ç¤ºã€‚</p>
<p>è€Œå…¶å¯¹åº”çš„ Attention Mask çŸ©é˜µå¦‚å³ä¾§æ‰€ç¤ºã€‚ä¸åŸå§‹æŠ•æœºè§£ç ç•¥æœ‰ä¸åŒçš„åœ°æ–¹æ˜¯ï¼Œæ ‘ä¸­æœ‰å¤šæ¡è§£ç è·¯å¾„ï¼Œä¸åŒè§£ç è·¯å¾„ä¹‹é—´ä¸èƒ½ç›¸äº’è®¿é—®ã€‚æ¯”å¦‚ï¼Œ(1) "It is"å’Œ (2) "I is"æ˜¯ä¸¤æ¡è·¯å¾„ï¼Œé‚£ä¹ˆåœ¨è®¡ç®—(1).isçš„æ¦‚ç‡åˆ†å¸ƒæ—¶ï¼Œåªèƒ½çœ‹åˆ°(1).itï¼Œè€Œä¸èƒ½çœ‹åˆ°(2)ä¸­çš„"I"ã€‚å› æ­¤ï¼ŒMedusaæ–°å»ºäº†åœ¨å¹¶è¡Œè®¡ç®—å¤šæ¡è·¯å¾„æ¦‚ç‡åˆ†å¸ƒæ—¶éœ€è¦çš„attention maskï¼Œç§°ä¸º"Tree attention"ã€‚æœ¬è´¨ä¸Šå°±æ˜¯åŒä¸€æ¡è·¯å¾„å†…éµä»å› æœmaskçš„è§„åˆ™ï¼Œä¸åŒè·¯å¾„ä¹‹é—´ä¸èƒ½ç›¸äº’è®¿é—®ã€‚</p>
<p>Medusaä½œè€…ç§°ï¼ŒSpecInferä¸­æ¯ä¸ªspeculatorç”Ÿæˆç§°çš„åºåˆ—é•¿åº¦ä¸åŒï¼Œæ‰€ä»¥Maskæ˜¯åŠ¨æ€å˜åŒ–çš„ã€‚è€ŒMedusaçš„Tree Attention Maskåœ¨Infrenceè¿‡ç¨‹ä¸­æ˜¯é™æ€ä¸å˜çš„ï¼Œè¿™ä½¿å¾—å¯¹æ ‘æ³¨æ„åŠ›Maskçš„é¢„å¤„ç†è¿›ä¸€æ­¥æé«˜äº†æ•ˆç‡ã€‚</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250424214209973-1434035516.jpg" alt="" loading="lazy"></p>
<h4 id="133-å°ç»“">1.3.3 å°ç»“</h4>
<p>ä¸‹è¡¨ç»™å‡ºäº†BPDï¼ŒSpecInferï¼ŒMedusaä¹‹é—´çš„å·®å¼‚ã€‚</p>
<table>
<thead>
<tr>
<th>é¢†åŸŸ</th>
<th>Blockwise Parallel Decoding</th>
<th>SpecInfer</th>
<th>Medusa</th>
</tr>
</thead>
<tbody>
<tr>
<td>å¤šæ¨¡å‹</td>
<td>æ²¡æœ‰çœŸçš„æ„é€ å‡ºk-1ä¸ªè¾…åŠ©æ¨¡å‹ï¼Œåªå¯¹åŸå§‹æ¨¡å‹ç•¥ä½œæ”¹é€ ï¼Œè®©å…¶å…·å¤‡é¢„æµ‹åkä¸ªtokençš„èƒ½åŠ›</td>
<td>é‡‡ç”¨ä¸€æ‰¹small speculative modelsï¼ˆSSMsï¼‰ï¼Œå¹¶è¡Œé¢„æµ‹å¤šä¸ªå€™é€‰SSMï¼Œå¯ä»¥æ˜¯åŸå§‹LLMçš„è’¸é¦ã€é‡åŒ–ã€å‰ªæç‰ˆæœ¬</td>
<td></td>
</tr>
<tr>
<td>å¤šå¤´</td>
<td>åŠ å…¥kä¸ªproject layerï¼Œè¿™kä¸ªproject layerçš„è¾“å‡ºå°±æ˜¯kä¸ªä¸åŒä½ç½®tokençš„logits</td>
<td></td>
<td>åœ¨ LLM çš„æœ€åä¸€ä¸ª Transformer Layer ä¹‹åä¿ç•™åŸå§‹çš„ LM Headï¼Œç„¶åé¢å¤–å¢åŠ å¤šä¸ª Medusa Headï¼Œè·å¾—å¤šä¸ªå€™é€‰çš„ Token åºåˆ—</td>
</tr>
<tr>
<td>Tree</td>
<td></td>
<td>å°†SSMsé¢„æµ‹çš„å¤šä¸ªå€™é€‰mergeä¸ºä¸€ä¸ªæ–°çš„token treeï¼Œé‡‡ç”¨åŸå§‹LLMåšå¹¶è¡ŒéªŒè¯ã€‚SpecInferä¸­æ¯ä¸ªspeculatorç”Ÿæˆç§°çš„åºåˆ—é•¿åº¦ä¸åŒï¼Œæ‰€ä»¥Maskæ˜¯åŠ¨æ€å˜åŒ–çš„ã€‚</td>
<td>Medusaçš„Tree Attention Maskåœ¨Infrenceè¿‡ç¨‹ä¸­æ˜¯é™æ€ä¸å˜çš„ï¼Œè¿™ä½¿å¾—å¯¹æ ‘æ³¨æ„åŠ›Maskçš„é¢„å¤„ç†è¿›ä¸€æ­¥æé«˜äº†æ•ˆç‡ã€‚</td>
</tr>
<tr>
<td>è®­ç»ƒ</td>
<td>é‡æ–°è®­ç»ƒåŸå§‹æ¨¡å‹</td>
<td>è®­ç»ƒå°æ¨¡å‹</td>
<td>å¹¶ä¸éœ€è¦é‡æ–°è®­ç»ƒæ•´ä¸ªå¤§æ¨¡å‹ï¼Œè€Œæ˜¯å†»ç»“å¤§æ¨¡å‹è€Œåªè®­ç»ƒè§£ç å¤´</td>
</tr>
</tbody>
</table>
<h2 id="0x02-è®¾è®¡æ ¸å¿ƒç‚¹">0x02 è®¾è®¡æ ¸å¿ƒç‚¹</h2>
<h3 id="21-æµç¨‹">2.1 æµç¨‹</h3>
<p>MEDUSAçš„å¤§è‡´æ€è·¯å’ŒæŠ•æœºè§£ç ç±»ä¼¼ï¼Œå…¶ä¸­æ¯ä¸ªè§£ç æ­¥éª¤ä¸»è¦ç”±ä¸‰ä¸ªå­æ­¥éª¤ç»„æˆï¼š</p>
<ul>
<li>ç”Ÿæˆå€™é€‰è€…ã€‚MEDUSAé€šè¿‡æ¥åœ¨åŸæ¨¡å‹çš„å¤šä¸ªMedusaè§£ç å¤´æ¥è·å–å¤šä¸ªä½ç½®çš„å€™é€‰token</li>
<li>å¤„ç†å€™é€‰è€…ã€‚MEDUSAæŠŠå„ä¸ªä½ç½®çš„å€™é€‰tokenè¿›è¡Œå¤„ç†ï¼Œé€‰å‡ºä¸€äº›å€™é€‰åºåˆ—ã€‚ç„¶åé€šè¿‡tree attentionæ¥è¿›è¡ŒéªŒè¯ã€‚ç”±äº MEDUSA å¤´ä½äºåŸå§‹æ¨¡å‹ä¹‹ä¸Šï¼Œå› æ­¤ï¼Œæ­¤å¤„è®¡ç®—çš„ logitså¯ä»¥ç”¨äºä¸‹ä¸€ä¸ªè§£ç æ­¥éª¤ã€‚</li>
<li>æ¥å—å€™é€‰è€…ã€‚é€šè¿‡typical acceptanceï¼ˆå…¸å‹æ¥å—ï¼‰æ¥é€‰æ‹©æœ€ç»ˆè¾“å‡ºçš„ç»“æœã€‚</li>
</ul>
<p>Medusaæ›´å¤§çš„ä¼˜åŠ¿åœ¨äºï¼Œé™¤äº†ç¬¬ä¸€æ¬¡Prefillå¤–ï¼Œåç»­å¯ä»¥è¾¾åˆ°è¾¹verifyè¾¹ç”Ÿæˆçš„æ•ˆæœï¼Œå³ Medusa çš„æ¨ç†æµç¨‹å¯ä»¥ç†è§£ï¼šPrefill + Verify + Verify + ...ã€‚</p>
<h3 id="22-æ¨¡å‹ç»“æ„">2.2 æ¨¡å‹ç»“æ„</h3>
<p>ä¸‹é¢ä»£ç ç»™å‡ºäº†ç¾æœèçš„æ¨¡å‹ç»“æ„ã€‚Medusa æ˜¯åœ¨ LLM çš„æœ€åä¸€ä¸ª Transformer Layer ä¹‹åä¿ç•™åŸå§‹çš„ LM Headï¼Œç„¶åé¢å¤–åŠ å¤šä¸ª Medusa Headï¼Œä¹Ÿå°±æ˜¯å¤šä¸ªä¸åŒåˆ†æ”¯è¾“å‡ºã€‚è¿™æ ·å¯ä»¥é¢„æµ‹å‡ºå¤šä¸ªå€™é€‰çš„ Token åºåˆ—ã€‚<br>
Medusa headçš„è¾“å…¥æ˜¯å¤§æ¨¡å‹çš„éšè—å±‚è¾“å‡ºã€‚è¿™æ˜¯å’Œä½¿ç”¨å¤–æŒ‚å°æ¨¡å‹æŠ•æœºè§£ç çš„å¦ä¸€ä¸ªé‡è¦ä¸åŒã€‚å¤–æŒ‚å°æ¨¡å‹çš„è¾“å…¥æ˜¯æŸ¥è¡¨å¾—åˆ°çš„token embeddingï¼Œæ¯”è¿™é‡Œçš„å¤§æ¨¡å‹æœ€åä¸€å±‚éšè—å±‚è¦å¼±çš„å¤šï¼Œå› æ­¤æ¯”è¾ƒä¾èµ–å°æ¨¡å‹çš„æ€§èƒ½ã€‚æ­£æ˜¯å› ä¸ºå€ŸåŠ©å¤§æ¨¡å‹çš„éšè—å±‚è¾“å‡ºï¼Œè¿™é‡Œçš„Medusa headçš„ç»“æ„éƒ½ååˆ†ç®€å•ã€‚</p>
<pre><code class="language-python">class MedusaLlamaModel(KVLlamaForCausalLM):
    """The Medusa Language Model Head.
    This module creates a series of prediction heads (based on the 'medusa' parameter)
    on top of a given base model. Each head is composed of a sequence of residual blocks
    followed by a linear layer.
    """

    def __init__(
        self,
        config,
    ):
        # Load the base model
        super().__init__(config)
        # For compatibility with the old APIs

        medusa_num_heads = config.medusa_num_heads
        medusa_num_layers = config.medusa_num_layers
        base_model_name_or_path = config._name_or_path
        self.hidden_size = config.hidden_size
        self.vocab_size = config.vocab_size
        self.medusa = medusa_num_heads
        self.medusa_num_layers = medusa_num_layers
        self.base_model_name_or_path = base_model_name_or_path
        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_name_or_path)
        # Create a list of Medusa heads
        self.medusa_head = nn.ModuleList(
            [
                nn.Sequential(
                    *([ResBlock(self.hidden_size)] * medusa_num_layers),
                    nn.Linear(self.hidden_size, self.vocab_size, bias=False),
                )
                for _ in range(medusa_num_heads)
            ]
        )
        

    def forward(
        self,
        input_ids=None,
        attention_mask=None,
        past_key_values=None,
        output_orig=False,
        position_ids=None,
        medusa_forward=False,
        **kwargs,
    ):
        """Forward pass of the MedusaModel.

        Args:
            input_ids (torch.Tensor, optional): Input token IDs.
            attention_mask (torch.Tensor, optional): Attention mask.
            labels (torch.Tensor, optional): Ground truth labels for loss computation.
            past_key_values (tuple, optional): Tuple containing past key and value states for attention.
            output_orig (bool, optional): Whether to also output predictions from the original LM head.
            position_ids (torch.Tensor, optional): Position IDs.

        Returns:
            torch.Tensor: A tensor containing predictions from all Medusa heads.
            (Optional) Original predictions from the base model's LM head.
        """
        if not medusa_forward:
            return super().forward(
                input_ids=input_ids,
                attention_mask=attention_mask,
                past_key_values=past_key_values,
                position_ids=position_ids,
                **kwargs,
            )
        with torch.inference_mode():
            # Pass input through the base model
            outputs = self.base_model.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                past_key_values=past_key_values,
                position_ids=position_ids,
                **kwargs,
            )
            if output_orig:
                # åŸå§‹æ¨¡å‹è¾“å‡º
                orig = self.base_model.lm_head(outputs[0])
        # Clone the output hidden states
        hidden_states = outputs[0].clone()
        medusa_logits = []
        # TODO: Consider parallelizing this loop for efficiency?
        for i in range(self.medusa):
            # ç¾æœèå¤´è¾“å‡º
            medusa_logits.append(self.medusa_head[i](hidden_states))
        if output_orig:
            return torch.stack(medusa_logits, dim=0), outputs, orig
        return torch.stack(medusa_logits, dim=0)        
</code></pre>
<h3 id="23-å¤šå¤´">2.3 å¤šå¤´</h3>
<h4 id="231-headç»“æ„">2.3.1 headç»“æ„</h4>
<p>Medusa é¢å¤–æ–°å¢ medusa_num_heads ä¸ª Medusa Headï¼Œæ¯ä¸ª Medusa Head æ˜¯ä¸€ä¸ªåŠ ä¸Šäº†æ®‹å·®è¿æ¥çš„å•å±‚å‰é¦ˆç½‘ç»œï¼Œå…¶ä¸­çš„ Linear å’Œæ¨¡å‹çš„é»˜è®¤ lm_head ç»´åº¦ä¸€æ ·ï¼Œè¿™æ ·å¯ä»¥é¢„æµ‹åç»­çš„ Tokenã€‚</p>
<pre><code class="language-python">self.medusa_head = nn.ModuleList(
    [
        nn.Sequential(
            *([ResBlock(self.hidden_size)] * medusa_num_layers),
            nn.Linear(self.hidden_size, self.vocab_size, bias=False),
        )
        for _ in range(medusa_num_heads)
    ]
)
</code></pre>
<p>ä¸‹é¢ä»£ç ä¸ºæ‰“å°å‡ºæ¥çš„å®é™…å†…å®¹ã€‚</p>
<pre><code class="language-python">ModuleList(
  (0-3): 4 x Sequential(
    (0): ResBlock(
      (linear): Linear(in_features=4096, out_features=4096, bias=True)
      (act): SiLU()
    )
    (1): Linear(in_features=4096, out_features=32000, bias=False)
  )
)
</code></pre>
<p>æŠŠç¬¬kä¸ªè§£ç å¤´åœ¨è¯è¡¨ä¸Šçš„è¾“å‡ºåˆ†å¸ƒè®°ä½œ <span class="math inline">\(p_t^{(t)}\)</span>ï¼Œå…¶è®¡ç®—æ–¹å¼å¦‚ä¸‹ã€‚dæ˜¯hidden stateçš„è¾“å‡ºç»´åº¦ï¼ŒVæ˜¯è¯è¡¨å¤§å°ï¼ŒåŸå§‹æ¨¡å‹çš„é¢„æµ‹è¡¨ç¤ºä¸º  <span class="math inline">\(p_t^{(0)}\)</span>  ã€‚</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250424214237454-882793487.jpg" alt="" loading="lazy"></p>
<p>ä¸‹é¢æ˜¯æŠŠä»£ç å’Œæ¨¡å‹ç»“æ„ç»“åˆèµ·æ¥çš„ç¤ºæ„å›¾ã€‚</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250424214225377-837213477.jpg" alt="" loading="lazy"></p>
<h4 id="232-ä½ç½®">2.3.2 ä½ç½®</h4>
<p>Medusaæ¯ä¸ªå¤´é¢„æµ‹çš„åç§»é‡æ˜¯ä¸åŒçš„ï¼Œç¬¬kä¸ªå¤´ç”¨æ¥é¢„æµ‹ä½ç½®t+k+1çš„è¾“å‡ºtokenï¼ˆkçš„å–å€¼æ˜¯1~Kï¼‰ã€‚åŸæ¨¡å‹çš„è§£ç å¤´ä¾ç„¶é¢„æµ‹ä½ç½®t+1çš„è¾“å‡ºï¼Œç›¸å½“äºk=0ã€‚å…·ä½“è€Œè¨€ï¼ŒæŠŠåŸå§‹æ¨¡å‹åœ¨ä½ç½®tçš„æœ€åéšè—çŠ¶æ€ <span class="math inline">\(â„_t\)</span>æ¥å…¥åˆ°Kä¸ªè§£ç å¤´ä¸Šï¼Œå¯¹äºè¾“å…¥tokenåºåˆ— <span class="math inline">\(t_0,t_1,..,t_i\)</span>ï¼ŒåŸå§‹çš„headæ ¹æ®è¾“å…¥é¢„æµ‹$ t_{i+1}$ï¼ŒMedusaæ–°å¢çš„ç¬¬ä¸€ä¸ªheadæ ¹æ®è¾“å…¥é¢„æµ‹  <span class="math inline">\(t_{i+2}\)</span>çš„tokenï¼Œä¹Ÿå°±æ˜¯è·³è¿‡token  <span class="math inline">\(t_{i+1}\)</span> é¢„æµ‹ä¸‹ä¸€ä¸ªæœªæ¥çš„tokenã€‚å¹¶ä¸”æ¯ä¸ªå¤´å¯ä»¥æŒ‡å®štopkä¸ªç»“æœã€‚è¿™äº›å¤´çš„é¢„æµ‹ç»“æœæ„æˆäº†å¤šä¸ªå€™é€‰è¯æ±‡åºåˆ—ï¼Œç„¶ååˆ©ç”¨æ ‘å½¢æ³¨æ„åŠ›æœºåˆ¶åŒæ—¶å¤„ç†è¿™äº›å€™é€‰åºåˆ—ã€‚åœ¨æ¯ä¸ªè§£ç æ­¥ï¼Œé€‰æ‹©æœ€é•¿è¢«æ¥å—çš„å€™é€‰åºåˆ—ä½œä¸ºæœ€ç»ˆçš„é¢„æµ‹ç»“æœã€‚è¿™æ ·ï¼Œæ¯æ­¥å¯ä»¥é¢„æµ‹å¤šä¸ªè¯æ±‡ï¼Œä»è€Œå‡å°‘äº†æ€»çš„è§£ç æ­¥æ•°ï¼Œæé«˜äº†æ¨ç†é€Ÿåº¦ã€‚</p>
<p>å¦‚ä¸‹å›¾æ‰€ç¤ºï¼ŒMedusaåœ¨åŸå§‹æ¨¡å‹åŸºç¡€ä¸Šï¼Œå¢åŠ äº†3ä¸ªé¢å¤–çš„Headï¼Œå¯ä»¥å¹¶è¡Œé¢„æµ‹å‡ºå4ä¸ªtokençš„å€™é€‰ã€‚</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250424214250560-2064986584.jpg" alt="" loading="lazy"></p>
<h3 id="24-ç¼ºç‚¹">2.4 ç¼ºç‚¹</h3>
<p>Medusaçš„ç¼ºç‚¹å¦‚ä¸‹ï¼š</p>
<ul>
<li>Medusa æ–°å¢çš„ lm_head å’Œæœ€åä¸€ä¸ª Transformer Block ä¸­é—´åªæœ‰ä¸€ä¸ª MLPï¼Œè¡¨è¾¾èƒ½åŠ›å¯èƒ½æœ‰é™ã€‚</li>
<li>Medusa å¢åŠ äº†æ¨¡å‹å‚æ•°é‡ï¼Œä¼šå¢åŠ æ˜¾å­˜å ç”¨ï¼›</li>
<li>Medusa æ¯ä¸ª head éƒ½æ˜¯ç‹¬ç«‹æ‰§è¡Œçš„ï¼Œä¹Ÿå°±æ˜¯ â€œnext next tokenâ€ é¢„æµ‹å¹¶ä¸ä¼šä¾èµ–ä¸Šä¸€ä¸ª â€œnext tokenâ€ çš„ç»“æœï¼Œå¯¼è‡´ç”Ÿæˆæ•ˆæœä¸ä½³ï¼Œæ¥å—ç‡æ¯”è¾ƒä½ï¼Œåœ¨å¤§ batch size æ—¶ç”šè‡³å¯èƒ½è´Ÿä¼˜åŒ–ã€‚</li>
<li>ç¼ºä¹åºåˆ—ä¾èµ–ä¹Ÿå¯èƒ½å¯¼è‡´ä½æ•ˆçš„æ ‘å‰ªæç®—æ³•ã€‚</li>
<li>è‰ç¨¿è´¨é‡ä»ç„¶ä¸é«˜ï¼ŒåŠ é€Ÿæ•ˆæœæœ‰é™ï¼Œå¹¶ä¸”åœ¨éè´ªå©ªè§£ç  (non-greedy decoding) ä¸‹ä¸èƒ½ä¿è¯è¾“å‡ºåˆ†å¸ƒä¸ç›®æ ‡LLMä¸€è‡´ã€‚</li>
</ul>
<p>å› æ­¤ï¼Œåç»­æœ‰ç ”ç©¶å·¥ä½œå¯¹æ­¤è¿›è¡Œäº†æ”¹è¿›ã€‚æ¯”å¦‚Cloveré‡ç‚¹æ˜¯æä¾›åºåˆ—ä¾èµ–å’ŒåŠ å…¥æ¯”å•ä¸ª MLP å…·æœ‰æ›´å¼ºçš„è¡¨å¾èƒ½åŠ›çš„æ¨¡å—ã€‚Hydra å¢åŠ äº† draft head é¢„æµ‹ä¹‹é—´çš„å…³è”æ€§ã€‚Hydra++ä½¿ç”¨ base model çš„è¾“å‡ºé¢„æµ‹æ¦‚ç‡ä½œä¸ºçŸ¥è¯†è’¸é¦çš„æ•™å¸ˆæ¨¡å‹è¾“å‡ºæ¥è®­ç»ƒ draft headã€‚å¹¶ä¸”ç±»ä¼¼EAGLEï¼ŒHydra++å¢åŠ ä¸€ä¸ªç‹¬ç«‹çš„ decoder layerï¼Œæ¯ä¸ª Hydra head é™¤äº†ä¸Šä¸€ä¸ª token æœ¬èº«ï¼Œè¿˜æ·»åŠ äº†ä¸Šä¸€ä¸ª token åœ¨è¿™ä¸ª decoder layer çš„ representation ä½œä¸ºè¾“å…¥ã€‚</p>
<h2 id="0x03-tree-verification">0x03 Tree Verification</h2>
<p>æ¯ä¸ªMedusa Head ä¼šç”Ÿæˆ top-k ä¸ªé¢„æµ‹æ ‡è®°ï¼Œç„¶åé€šè¿‡è®¡ç®—è¿™äº›é¢„æµ‹çš„ç¬›å¡å°”ç§¯æ¥å½¢æˆå€™é€‰åºåˆ—ã€‚æˆ‘ä»¬å¯ä»¥å¯¹äºæ¯ä¸ªå€™é€‰åºåˆ—éƒ½èµ°ä¸€éæ¨¡å‹æ¥éªŒè¯ï¼Œä½†æ˜¯è¿™æ ·åšå¤ªè€—æ—¶ã€‚å› æ­¤ï¼ŒMedusa ä½œè€…è®¾è®¡äº†ä¸€ç§tree attentionçš„æœºåˆ¶ï¼Œåœ¨å€™é€‰æ ‘å†…è¿›è¡Œæ©ç æ“ä½œï¼Œæ©ç é™åˆ¶æŸä¸ªtokenå¯¹å‰é¢tokençš„æ³¨æ„åŠ›ã€‚åŒæ—¶ï¼Œä¹Ÿè¦ä¸ºç›¸åº”åœ°ä¸ºposition embeddingè®¾ç½®æ­£ç¡®çš„ä½ç½®ç´¢å¼•ã€‚å› ä¸ºæœ‰ tree attention çš„å­˜åœ¨ï¼Œæ‰€ä»¥ Medusa å¯ä»¥å¹¶è¡Œåœ°æ„å»ºã€ç»´æŠ¤å’ŒéªŒè¯å¤šä¸ªå€™é€‰åºåˆ—ã€‚</p>
<h3 id="31-è§£ç è·¯å¾„">3.1 è§£ç è·¯å¾„</h3>
<p>åœ¨Medusaä¸­ï¼ŒåŸºç¡€ç‰ˆæœ¬è§£ç é‡‡ç”¨greedyæ–¹å¼å–Top-1 Tokenã€‚Medusaå¢åŠ é¢å¤–çš„è§£ç å¤´ä¹‹åï¼Œä½¿ç”¨ Top-K Samplingï¼Œæ¯ä¸€ä¸ª Head éƒ½ä¼šè¾“å‡º k ä¸ª tokensã€‚ä¸åŒMedusaå¤´ä¹‹é—´é¢„æµ‹ç»“æœä¸ä¸€è‡´ã€‚<span class="math inline">\(p(t_{t+1}|t_0,...,t_i)\)</span>å’Œ<span class="math inline">\(p(t_{t+2}|t_0,...,t_i)\)</span>å½¢å¼ä¸Šæ˜¯æ¡ä»¶ç‹¬ç«‹çš„ï¼Œä½†æ˜¯å®é™…ä¸Š<span class="math inline">\(p_{t+2}\)</span>ä¾èµ–<span class="math inline">\(p_{t+1}\)</span>ï¼Œä¸èƒ½ç›´æ¥å–<span class="math inline">\(p(t_{t+1}|t_0,...,t_i)\)</span>å’Œ<span class="math inline">\(p(t_{t+2}|t_0,...,t_i)\)</span>æœ€å¤§çš„tokenä½œä¸ºverifyé˜¶æ®µçš„è¾“å…¥ï¼Œè¿™æ ·ç»„æˆçš„å¥å­å¯èƒ½ä¼šåœ¨é€»è¾‘ä¸Šä¸ä¸€è‡´ã€‚å› æ­¤ï¼ŒMedusaè¿˜å¼•å…¥é‡‡æ ·topkç»„åˆä½œä¸ºå€™é€‰åºåˆ—çš„æ–¹å¼å»ç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚æœ€ç»ˆæŠŠ<code>LM_head</code> çš„è¾“å‡ºä½œä¸ºæ ¹èŠ‚ç‚¹æ„å»ºå‡ºæ ‘çŠ¶ç»“æ„ï¼Œæ ‘çš„æ·±åº¦è‡ªé¡¶å‘ä¸‹éå†ç§°ä¸ºè§£ç è·¯å¾„ï¼ˆè®ºæ–‡ä¸­å«åšcandidates pathï¼‰ã€‚æ¯ä¸ªå€™é€‰åºåˆ—å¯ä»¥è¡¨ç¤ºæ‰€æ„å»ºçš„treeä¸Šçš„ä¸€æ¡è·¯å¾„ä¸Šæ‰€æœ‰çš„nodeï¼ˆè€Œä¸åªæ˜¯leaf nodeï¼Œå› ä¸ºtree attentionéªŒè¯çš„æ—¶å€™ä¼šæŠŠè·¯å¾„ä¸Šæ‰€æœ‰tokenéƒ½è¿›è¡ŒéªŒè¯ï¼‰ã€‚</p>
<p>ç”±äºæœ‰Kä¸ªheadï¼Œæ¯ä¸ªheadé€‰å–<span class="math inline">\(\text{top-}s_k\)</span>ä¸ªé¢„æµ‹è¾“å‡ºï¼Œåˆ™æ‰€æœ‰è·¯å¾„å¯èƒ½ç»„åˆä¸ºæ ‘ä¸Šæ‰€æœ‰èŠ‚ç‚¹çš„æ€»å’Œï¼Œå³<span class="math inline">\(\sum_{k=1}^K \prod_{i=1}^k s_i\)</span>ã€‚åœ¨æ„å»ºæ ‘å½¢ç»“æ„æ—¶ï¼Œæœ€ç®€å•çš„æ–¹æ³•æ˜¯é€šè¿‡ç¬›å¡å°”ç§¯æ¥è·å–å¤šä¸ªè§£ç å¤´ç»„æˆçš„æ‰€æœ‰å¯èƒ½çš„å€™é€‰åºåˆ—ã€‚ä¸‹å›¾ä¾‹å­ä½¿ç”¨äº†Cartesian productå¯¹ä¸¤ä¸ªè§£ç å¤´çš„ç»“æœè¿›è¡Œå¤„ç†ï¼Œè·å¾—æ‰€æœ‰å€™é€‰åºåˆ—ã€‚å…·ä½“æ¥è¯´å°±æ˜¯å°†æ¯ä¸ªå¤´çš„top-kä¸ªè¯ä½œä¸ºèŠ‚ç‚¹ï¼Œæ¯ä¸ªå¤´ä½œä¸ºæ ‘çš„ä¸€å±‚ã€‚å›¾ä¸Šä¸€å…±å­˜åœ¨6æ¡è§£ç è·¯å¾„ï¼Œç›¸å½“äº Head 1 åœ¨ä¸‹ä¸€ä¸ªä½ç½®ç”Ÿæˆ 2 ä¸ªå¯èƒ½çš„ Tokenï¼ˆIt å’Œ Iï¼‰ï¼ŒHead 2 åœ¨ä¸‹ä¸‹ä¸€ä¸ªä½ç½®ç”Ÿæˆ 3 ä¸ªå¯èƒ½çš„ Tokenï¼ˆisï¼Œâ€™ å’Œ theï¼‰ï¼Œè¿™æ ·ä¸‹ä¸€ä¸ªä½ç½®å’Œä¸‹ä¸‹ä¸€ä¸ªä½ç½®å°±æœ‰äº† 2 x 3 = 6 ç§å¯èƒ½çš„å€™é€‰åºåˆ—ã€‚ä¸ºäº†åŒºåˆ†ä¸åŒçš„ prefixï¼ŒMedusa è®¾ç½®äº†ä¸€äº›å†—ä½™ï¼Œä¾‹å¦‚ Head 2 çš„ä¸‰ä¸ªé¢„æµ‹ token å‡å‡ºç°äº†ä¸¤æ¬¡ï¼Œè¿™æ˜¯ä¸ºäº†åˆ†åˆ«å¯¹åº” It å’Œ I è¿™ä¸¤ä¸ªä¸åŒçš„ prefixã€‚æ¯ä¸ª token åœ¨ tree mask çš„ä½œç”¨ä¸‹åªå¯ä»¥çœ‹è§è‡ªå·±çš„ prefixã€‚</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250424214302437-1328376257.jpg" alt="" loading="lazy"></p>
<h3 id="32-æœ€ä½³æ„é€ æ–¹å¼">3.2 æœ€ä½³æ„é€ æ–¹å¼</h3>
<p>ä¸Šå›¾é‡‡ç”¨top-3ï¼Œä¸¤ä¸ªå¤´ä¸€å…±æœ‰6æ¡å€™é€‰è·¯å¾„ã€‚å¦‚æœè§£ç å¤´æ•°é‡æ•°é‡æ¯”è¾ƒå¤šï¼Œæ¯ä¸ªå¤´ç»™å‡ºçš„å€™é€‰tokenä¹Ÿæ¯”è¾ƒå¤šã€‚è§£ç è·¯å¾„ä¼šéšç€Top-k å’Œå¤´æ•°å¢å¤šæ€¥å‰§å¢åŠ ï¼Œä¼šäº§ç”Ÿå¤§é‡çš„å€™é€‰è·¯å¾„ï¼Œå…·æœ‰åºå¤§çš„æœç´¢ç©ºé—´ã€‚è™½ç„¶å¢åŠ å€™é€‰åºåˆ—çš„æ•°é‡ï¼Œæœ€ç»ˆæ¥å—tokençš„å‘½ä¸­ç‡å°±ä¼šæå‡ï¼Œä½†æ˜¯éªŒè¯æ›´å¤šå€™é€‰åºåˆ—ä¹Ÿä¼šå¸¦æ¥é¢å¤–çš„è®¡ç®—æ¶ˆè€—ã€‚é‚£ä¹ˆæ–°çš„é—®é¢˜æ˜¯ï¼š</p>
<ul>
<li>å¦‚ä½•èƒ½å‡å°‘å€™é€‰è§£ç è·¯å¾„ï¼Ÿ</li>
<li>å¦‚ä½•èƒ½åœ¨å€™é€‰è§£ç è·¯å¾„ä¸­ï¼Œå¾—åˆ°æœ€ä¼˜è§£ç è·¯å¾„ï¼Ÿ</li>
</ul>
<p>ç›´è§‚ä¸Šæ¥è¯´ï¼Œé‚£äº›ç”±ä¸åŒå¤´éƒ¨çš„topké¢„æµ‹ç»„æˆçš„å€™é€‰ç»“æœå¯èƒ½å…·æœ‰ä¸åŒçš„å‡†ç¡®ç‡ã€‚åº”è¯¥ä¼˜å…ˆé€‰æ‹©é‚£äº›æ›´å‡†ç¡®çš„é¢„æµ‹ï¼Œä»è€Œæ„å»ºä¸€ä¸ªæ›´æœ‰æ•ˆçš„æ ‘ï¼Œè€Œä¸éœ€è¦ä½¿ç”¨æ‰€æœ‰å¯èƒ½çš„æ’åˆ—ç»„åˆã€‚Medusa é€šè¿‡åœ¨æ ¡å‡†æ•°æ®é›†ä¸Šç»Ÿè®¡æ¯ä¸ªè§£ç å¤´çš„ top-k é¢„æµ‹çš„å‡†ç¡®ç‡ï¼Œç„¶åè´ªå©ªåœ°é€‰æ‹©é‚£äº›èƒ½å¤Ÿæœ€å¤§åŒ–æœŸæœ›æ¥å—é•¿åº¦çš„èŠ‚ç‚¹ï¼Œå°†å®ƒä»¬åŠ å…¥åˆ°æ ‘ä¸­ã€‚è¿™æ ·å¯ä»¥åœ¨ç›¸åŒçš„èŠ‚ç‚¹æ€»æ•°ä¸‹ï¼Œè·å¾—æ›´é«˜çš„åŠ é€Ÿæ¯”ã€‚å…¶å®ï¼Œæ­¤æ–¹æ³•æœ¬è´¨ä¸Šå°±æ˜¯ç”¨å‰ªææ¥åŠ é€Ÿï¼Œå‰ªå»æ¯ä¸ªheadä¸­<span class="math inline">\(\text{top-}s_k\)</span>çš„è‹¥å¹²ä¸ªã€‚</p>
<p>å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ä¸ªcalibration datasetï¼ˆæ¯”å¦‚Alpaca-eval datasetï¼‰æ¥è·å–ä¸åŒè§£ç å¤´ç»™å‡ºçš„å„ä¸ªtokençš„å‡†ç¡®ç‡ï¼šæŠŠç¬¬ k ä¸ªè§£ç å¤´ç»™å‡ºçš„ç¬¬ i ä¸ªtokençš„å‡†ç¡®ç‡è®°ä¸º <span class="math inline">\(a_k^{(i)}\)</span>ã€‚å‡è®¾å„ä¸ªtokençš„å‡†ç¡®ç‡ä¹‹é—´æ˜¯ç‹¬ç«‹çš„ï¼Œé‚£ä¹ˆä¸€ä¸ªç”±$[i_1,i_2,\cdots,i_k] $æ„æˆçš„å€™é€‰åºåˆ—çš„å‡†ç¡®ç‡å¯ä»¥å†™ä½œ <span class="math inline">\(\prod_{j=1}^ka_j^{(i_j)}\)</span>ã€‚æˆ‘ä»¬ç”¨ I è¡¨ç¤ºå€™é€‰åºåˆ—çš„é›†åˆï¼Œé‚£ä¹ˆé›†åˆé‡Œçš„å€™é€‰åºåˆ—çš„expectation of acceptance lengthå°±è¡¨ç¤ºä¸ºï¼š</p>
<p></p><div class="math display">\[\sum_{[i_1,i_2,\cdots,i_k]\in I}\prod_{j=1}^ka_j^{(i_j)}
\]</div><p></p><p>åœ¨æ„å»ºtreeçš„æ—¶å€™ï¼ŒMedusa ç”¨è´ªå¿ƒç®—æ³•ä¼˜å…ˆåŠ å…¥å½“å‰æœ‰æœ€å¤§å‡†ç¡®ç‡çš„å€™é€‰åºåˆ—ï¼Œç›´åˆ°treeçš„èŠ‚ç‚¹æ•°é‡è¾¾åˆ°æ¥å—é•¿åº¦çš„æœŸæœ›å€¼ä¸Šé™ï¼Œè¿™æ ·èƒ½æœ€å¤§åŒ–expectation of acceptance lengthï¼Œä¹Ÿå°±èƒ½æœ€å¤§åŒ–acceleration rateã€‚è¿™æ˜¯ä¸€ç§æ‰‹å·¥è®¾è®¡çš„ç¨€ç–æ ‘ç»“æ„ï¼Œè¶Šé å‰çš„èŠ‚ç‚¹ï¼Œæœ‰æ›´å¤šçš„å­èŠ‚ç‚¹è·¯å¾„ã€‚å³ï¼ŒæŠŠæ‰€æœ‰ä½ç½®ç»„åˆç©·ä¸¾ï¼Œç„¶åé€‰å–å‰ğ‘ä¸ªç»„åˆä½œä¸ºå›ºå®šè€ƒå¯Ÿçš„å¯èƒ½ï¼Œå‰©ä½™çš„å‰ªæå»æ‰ã€‚</p>
<p>MEDUSA-2 Vicuna-7Bæ¨¡å‹çš„ä¸€ä¸ªç¨€ç–æ ‘ç¤ºä¾‹å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚è¿™ä¸ªæ ‘ç»“æ„å»¶ä¼¸äº†å››ä¸ªå±‚æ¬¡ï¼Œè¡¨æ˜æœ‰å››ä¸ªMEDUSAå¤´å‚ä¸äº†è®¡ç®—ã€‚è¯¥æ ‘æœ€åˆé€šè¿‡ç¬›å¡å°”ç§¯æ–¹æ³•ç”Ÿæˆï¼Œéšåæ ¹æ®æ¯ä¸ªMEDUSAå¤´åœ¨Alpaca-evalæ•°æ®é›†ä¸Šæµ‹é‡çš„å‰ k ä¸ªé¢„æµ‹çš„ç»Ÿè®¡æœŸæœ›å€¼è¿›è¡Œä¿®å‰ªã€‚æ ‘å‘å·¦å€¾æ–œåœ¨è§†è§‰ä¸Šä»£è¡¨äº†ç®—æ³•å€¾å‘äºä½¿ç”¨æ›´é«˜å‡†ç¡®ç‡çš„tokenï¼Œæ¯ä¸ªèŠ‚ç‚¹è¡¨ç¤ºMEDUSAå¤´éƒ¨çš„top-ké¢„æµ‹ä¸­çš„ä¸€ä¸ªtokenï¼Œè¾¹æ˜¾ç¤ºäº†å®ƒä»¬ä¹‹é—´çš„è¿æ¥ï¼Œçº¢çº¿çªå‡ºæ˜¾ç¤ºäº†æ­£ç¡®é¢„æµ‹æœªæ¥tokençš„è·¯å¾„ã€‚è¿™æ ·å°±å°†1000ä¸ªè·¯å¾„çš„æ ‘ä¼˜åŒ–åˆ°åªæœ‰42æ¡è·¯å¾„ï¼Œè€Œä¸”ï¼Œè¿™é‡Œçš„è·¯å¾„å¯ä»¥æå‰ç»“æŸï¼Œä¸è¦æ±‚ä¸€å®šè¦éå†åˆ°æœ€åä¸€å±‚ã€‚</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250424214312348-1076649173.jpg" alt="" loading="lazy"></p>
<h3 id="33-å®ç°">3.3 å®ç°</h3>
<h4 id="331-å…³é”®å˜é‡">3.3.1 å…³é”®å˜é‡</h4>
<p>æˆ‘ä»¬é¦–å…ˆçœ‹çœ‹æ³¨æ„åŠ›æ ‘æ‰€æ¶‰åŠçš„å…³é”®å˜é‡ã€‚</p>
<p><strong>demo_tensor</strong></p>
<p>demo_tensoræ˜¯è¾“å…¥å¼ é‡ï¼Œä¾‹å­å¦‚ä¸‹ï¼š</p>
<pre><code class="language-python">[2, 3, 0, 0, 0, 0, 0, 0 ...] # 1st depth we choose top 2
[4, 5, 6, 0, 0, 0, 0, 0 ...] # 2nd depth we choose top 3
</code></pre>
<p>å¯¹åº”ä¸‹å›¾ã€‚</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250424214323951-1171552244.jpg" alt="" loading="lazy"></p>
<p><strong>medusa_choices</strong></p>
<p>medusa_choicesæ˜¯ä¸€ä¸ªåµŒå¥—åˆ—è¡¨ï¼Œè¡¨ç¤ºmedusaæ ‘ç»“æ„ï¼Œå†³å®šè§£ç è·¯å¾„ã€‚å¤–éƒ¨åˆ—è¡¨å¯¹åº”äºæ ‘ä¸­çš„èŠ‚ç‚¹ï¼Œæ¯ä¸ªå†…éƒ¨åˆ—è¡¨ç»™å‡ºè¯¥èŠ‚ç‚¹åœ¨æ ‘ä¸­çš„ç¥–å…ˆåŠå…¶ä½ç½®ã€‚æ ¹æ®<code>Medusa choies</code> æˆ‘ä»¬å¯ä»¥æ„å»ºç¨€ç–æ ‘çš„æ‰€æœ‰æ•°æ®æˆå‘˜ï¼Œæºç ä¸­çš„ä¾‹å­å¦‚ä¸‹ã€‚</p>
<pre><code class="language-python">vicuna_7b_stage2 = [(0,), (0, 0), (1,), (0, 1), (0, 0, 0), (1, 0), (2,), (0, 2), (0, 0, 1), (0, 3), (3,), (0, 1, 0), (2, 0), (4,), (0, 0, 2), (0, 4), (1, 1), (1, 0, 0), (0, 0, 0, 0), (5,), (0, 0, 3), (0, 5), (0, 2, 0), (3, 0), (0, 1, 1), (0, 6), (6,), (0, 7), (0, 0, 4), (4, 0), (1, 2), (0, 8), (7,), (0, 3, 0), (0, 0, 0, 1), (0, 0, 5), (2, 1), (0, 0, 6), (1, 0, 1), (0, 0, 1, 0), (2, 0, 0), (5, 0), (0, 9), (0, 1, 2), (8,), (0, 4, 0), (0, 2, 1), (1, 3), (0, 0, 7), (0, 0, 0, 2), (0, 0, 8), (1, 1, 0), (0, 1, 0, 0), (6, 0), (9,), (0, 1, 3), (0, 0, 0, 3), (1, 0, 2), (0, 5, 0), (3, 1), (0, 0, 2, 0), (7, 0), (1, 4)]
vicuna_7b_stage1_ablation = [(0,), (0, 0), (1,), (0, 0, 0), (0, 1), (1, 0), (2,), (0, 2), (0, 0, 1), (3,), (0, 3), (0, 1, 0), (2, 0), (0, 0, 2), (0, 4), (4,), (0, 0, 0, 0), (1, 0, 0), (1, 1), (0, 0, 3), (0, 2, 0), (0, 5), (5,), (3, 0), (0, 1, 1), (0, 6), (6,), (0, 0, 4), (1, 2), (0, 0, 0, 1), (4, 0), (0, 0, 5), (0, 7), (0, 8), (0, 3, 0), (0, 0, 1, 0), (1, 0, 1), (7,), (2, 0, 0), (0, 0, 6), (2, 1), (0, 1, 2), (5, 0), (0, 2, 1), (0, 9), (0, 0, 0, 2), (0, 4, 0), (8,), (1, 3), (0, 0, 7), (0, 1, 0, 0), (1, 1, 0), (6, 0), (9,), (0, 0, 8), (0, 0, 9), (0, 5, 0), (0, 0, 2, 0), (1, 0, 2), (0, 1, 3), (0, 0, 0, 3), (3, 0, 0), (3, 1)]
vicuna_7b_stage1 = [(0,), (0, 0), (1,), (2,), (0, 1), (1, 0), (3,), (0, 2), (4,), (0, 0, 0), (0, 3), (5,), (2, 0), (0, 4), (6,), (0, 5), (1, 1), (0, 0, 1), (7,), (3, 0), (0, 6), (8,), (9,), (0, 1, 0), (0, 7), (0, 8), (4, 0), (0, 0, 2), (1, 2), (0, 9), (2, 1), (5, 0), (1, 0, 0), (0, 0, 3), (1, 3), (0, 2, 0), (0, 1, 1), (0, 0, 4), (6, 0), (1, 4), (0, 0, 5), (2, 2), (0, 3, 0), (3, 1), (0, 0, 6), (7, 0), (1, 5), (1, 0, 1), (2, 0, 0), (0, 0, 7), (8, 0), (0, 0, 0, 0), (4, 1), (0, 1, 2), (0, 4, 0), (9, 0), (0, 2, 1), (2, 3), (1, 6), (0, 0, 8), (0, 5, 0), (3, 2), (5, 1)]
</code></pre>
<p>æˆ‘ä»¬æ­¤å¤„ä¾‹å­ä¸ºï¼š<code>[[0], [0, 0], [0, 1], [0, 2], [1], [1, 0], [1, 1], [1, 2]]</code>ï¼Œè¿™é‡Œ<code>[1]</code>ä¸ºæ ¹èŠ‚ç‚¹ï¼Œåˆ™å¯è§†åŒ–å¦‚ä¸‹ã€‚</p>
<pre><code class="language-python">[1]
[2, 3]
[4, 5, 6]
</code></pre>
<p><strong>medusa_buffers</strong></p>
<p>medusa_buffersæ•°æ®ç»“æ„ä¿¡æ¯å¦‚ä¸‹ã€‚</p>
<pre><code class="language-python">medusa_buffers = generate_medusa_buffers(medusa_choices, device='cpu')

medusa_buffers = {
    "medusa_attn_mask": medusa_attn_mask.unsqueeze(0).unsqueeze(0),
    "tree_indices": medusa_tree_indices,
    "medusa_position_ids": medusa_position_ids,
    "retrieve_indices": retrieve_indices,
    }
</code></pre>
<p>å…¶ä¸­æˆå‘˜å˜é‡ä½œç”¨å¦‚ä¸‹ï¼š</p>
<ul>
<li>medusa_attn_maskï¼šå°±æ˜¯æ ‘æ³¨æ„åŠ›ç”¨åˆ°çš„æ©ç ã€‚</li>
<li>tree_indicesï¼šdemo_tensorä¸­å…ƒç´ åœ¨æ ‘çš„å“ªä¸ªä½ç½®ï¼Œåœ¨ generate_candidates()å‡½æ•°ä¸­ä¼šç”¨åˆ°ã€‚</li>
<li>medusa_position_idsï¼šä¿è¯åŒä¸€æ·±åº¦çš„èŠ‚ç‚¹å…·æœ‰åŒæ ·çš„position IDï¼ŒåŠ åˆ°ä½ç½®ç¼–ç ä¸Šï¼Œåç»­åœ¨è®­ç»ƒæ—¶åŠ å…¥è¿™äº›ä¿¡æ¯ï¼Œå¯ä»¥å¾—åˆ°æ›´å¥½çš„medusaå¤´ã€‚åœ¨tree_decoding()å‡½æ•°ä¸­ç”¨åˆ°ã€‚</li>
<li>retrieve_indicesï¼šä»æ ‘æ˜ å°„åˆ°ç¬›å¡å°”ç§¯ï¼Œä»£è¡¨æ¯ä¸ªç¬›å¡å°”ç§¯åœ¨logitsä¸­çš„ä½ç½®ã€‚ä¾æ®è¿™äº›ä¿¡æ¯ï¼Œå¯ä»¥ä»logitsé‡Œé¢æå–æ¯ä¸ªç¬›å¡å°”ç§¯å¯¹åº”çš„logitsã€‚åœ¨tree_decoding()å‡½æ•°å’Œgenerate_candidates()å‡½æ•°ä¸­ç”¨åˆ°ã€‚</li>
</ul>
<p><strong>tree_indices</strong></p>
<p>tree_indicesä»£è¡¨demo_tensorä¸­å…ƒç´ åœ¨æ ‘çš„å“ªä¸ªä½ç½®ã€‚å¯¹äºç»™å®šçš„è¾“å…¥å¼ é‡ï¼Œå¯¹åº”çš„tree_indiceså¦‚ä¸‹ã€‚</p>
<pre><code class="language-python">[0, 1, 2, 3, 4, 5, 3, 4, 5]
</code></pre>
<p>é•¿æˆçš„æ ‘å¦‚ä¸‹ã€‚</p>
<pre><code class="language-python">1
|-- 2
|  |-- 4
|  |-- 5
|  |-- 6
|-- 3
|  |-- 4
|  |-- 5
|  |-- 6

</code></pre>
<p>ä»demo_tensor æ‹¿åˆ°å±•å¹³çš„æ ‘èŠ‚ç‚¹å¦‚ä¸‹ã€‚</p>
<pre><code class="language-python">[1, 2, 3, 4, 5, 6, 4, 5, 6]

</code></pre>
<p>å‚è§ä¸‹å›¾ã€‚</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250424214526158-1052329458.jpg" alt="" loading="lazy"></p>
<p><strong>medusa_position_ids</strong></p>
<p>medusa_position_idsï¼šä¿è¯åŒä¸€æ·±åº¦çš„èŠ‚ç‚¹å…·æœ‰åŒæ ·çš„position IDã€‚åŠ å…¥è¿™äº›ä¿¡æ¯ä¹‹åï¼Œæ¯ä¸ªtokenå¯¹åº”çš„ä½ç½®ç¼–ç æ˜¯ï¼šåºåˆ—ä¸­çš„ä½ç½® + æ ‘ä¸­çš„æ·±åº¦ã€‚è¿™æ ·åœ¨åç»­è®­ç»ƒmedusaå¤´æ—¶å°±çŸ¥é“æ·±åº¦ä¿¡æ¯ï¼Œå¯ä»¥è®­ç»ƒå‡ºæ›´å¥½çš„medusaå¤´ã€‚åœ¨tree_decoding()å‡½æ•°ä¸­ç”¨åˆ°æ­¤å˜é‡ã€‚</p>
<p>è¾“å…¥å¼ é‡å¯¹åº”çš„ä½ç½®idå¦‚ä¸‹ã€‚</p>
<pre><code class="language-python">[0, 1, 1, 2, 2, 2, 2, 2, 2] # Medusa position IDs
 |  |  |  |  |  |  |  |  |
[1, 2, 3, 4, 5, 6, 4, 5, 6] # Flatten tree representation of the tensor
</code></pre>
<p>å¯è§†åŒ–å¦‚ä¸‹ã€‚</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250424214534612-2094281850.jpg" alt="" loading="lazy"></p>
<p><strong>retrieve_indices</strong></p>
<p>retrieve_indicesæ˜¯ä»æ ‘æ˜ å°„åˆ°ç¬›å¡å°”ç§¯ï¼Œä»£è¡¨æ¯ä¸ªç¬›å¡å°”ç§¯åœ¨logitsä¸­çš„ä½ç½®ã€‚ä¾æ®è¿™äº›ä¿¡æ¯ï¼Œå¯ä»¥ä»logitsé‡Œé¢æå–æ¯ä¸ªç¬›å¡å°”ç§¯å¯¹åº”çš„logitsã€‚</p>
<p>æœ¬ä¾‹çš„retrieve_indiceså¦‚ä¸‹ã€‚</p>
<pre><code class="language-python">[0, 2, 8]
[0, 2, 7]
[0, 2, 6]
[0, 1, 5]
[0, 1, 4]
[0, 1, 3]
</code></pre>
<p>æŠŠæ ‘æ˜ å°„åˆ°ç¬›å¡å°”ç§¯ä¹‹åå¦‚ä¸‹ã€‚</p>
<pre><code class="language-python">[1, 3, 6]
[1, 3, 5]
[1, 3, 4]
[1, 2, 6]
[1, 2, 5]
[1, 2, 4]
</code></pre>
<p>å…·ä½“å¯è§†åŒ–å¦‚ä¸‹ã€‚</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250424214544065-1586733585.jpg" alt="" loading="lazy"></p>
<p><strong>medusa_attn_mask</strong></p>
<p>å› ä¸ºæœ€ç»ˆç»„æˆçš„æ ‘æ˜¯å°†æ¯ä¸ªå¤´çš„top-kä¸ªè¯ä½œä¸ºèŠ‚ç‚¹ï¼Œæ¯ä¸ªå¤´ä½œä¸ºæ ‘çš„ä¸€å±‚ï¼Œæ¯æ¡ç›´åˆ°å¶å­èŠ‚ç‚¹çš„è·¯å¾„æ„æˆä¸€ç»„å¾…éªŒè¯çš„é¢„æµ‹ã€‚åœ¨è¿™æ£µæ ‘å†…ï¼ŒAttention Maskéœ€è¦æ–°çš„è®¾è®¡ï¼Œè¯¥Maskåªé™åˆ¶ä¸€ä¸ªtokenå¯¹å‰é¢tokençš„æ³¨æ„åŠ›ã€‚åŒæ—¶ï¼Œè¦ä¸ºç›¸åº”åœ°ä¸ºposition embeddingè®¾ç½®æ­£ç¡®çš„ä½ç½®ç´¢å¼•ã€‚æ©ç çŸ©é˜µçš„ç»†èŠ‚å¦‚ä¸‹ï¼š</p>
<ul>
<li><code>Mask</code>çŸ©é˜µçš„æ¯è¡Œéƒ½å¯ä»¥ä»£è¡¨ä¸€ä¸ªtokené¢„æµ‹ä»»åŠ¡</li>
<li>åœ¨<code>Tree Mask</code>çŸ©é˜µä¸­ï¼Œéœ€è¦å¯¹ä½ç½®ç¼–ç è¿›è¡Œé”™ä½ç¼–ç </li>
</ul>
<p>è®ºæ–‡ä¸­ä¾‹å­å¦‚ä¸‹ã€‚</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250424214603304-759920735.jpg" alt="" loading="lazy"></p>
<p>å¯¹äºæœ¬ä¾‹çš„æ©ç å¦‚ä¸‹ã€‚</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250424214610946-1747775261.jpg" alt="" loading="lazy"></p>
<h4 id="332-ç¤ºä¾‹ä»£ç ">3.3.2 ç¤ºä¾‹ä»£ç </h4>
<p>ç¤ºä¾‹ä»£ç å¦‚ä¸‹</p>
<pre><code class="language-python">demo_tensor = torch.zeros(2,10).long()
demo_tensor[0,0] = 2
demo_tensor[0,1] = 3
demo_tensor[1,0] = 4
demo_tensor[1,1] = 5
demo_tensor[1,2] = 6
print('Demo tensor: \n', demo_tensor)
demo_tensor = demo_tensor.flatten()
demo_tensor = torch.cat([torch.ones(1).long(), demo_tensor])
print('='*50)
medusa_choices = [[0], [0, 0], [0, 1], [0, 2], [1], [1, 0], [1, 1], [1, 2]]
medusa_buffers = generate_medusa_buffers(medusa_choices, device='cpu')
tree_indices = medusa_buffers['tree_indices']
medusa_position_ids = medusa_buffers['medusa_position_ids']
retrieve_indices = medusa_buffers['retrieve_indices']
print('Tree indices: \n', tree_indices.tolist())
print('Tree reprentation of the tensor: \n', demo_tensor[tree_indices].tolist())
print('='*50)
print('Medusa position ids: \n', medusa_position_ids.tolist())
print('='*50)
print('Retrieve indices: \n', retrieve_indices.tolist())
demo_tensor_tree = demo_tensor[tree_indices]
demo_tensor_tree_ext = torch.cat([demo_tensor_tree, torch.ones(1).long().mul(-1)])
print('Retrieve reprentation of the tensor: \n', demo_tensor_tree_ext[retrieve_indices].tolist())
print('='*50)
demo_tensor_tree_ext[retrieve_indices].tolist()
print('='*50)
print(medusa_buffers['medusa_attn_mask'][0,0,:,:].int())
print('='*50)
print(medusa_buffers['medusa_attn_mask'][0,0,:,:].int())
</code></pre>
<p>æ‰“å°ç»“æœï¼š</p>
<pre><code class="language-python">Demo tensor: 
 tensor([[2, 3, 0, 0, 0, 0, 0, 0, 0, 0],
        [4, 5, 6, 0, 0, 0, 0, 0, 0, 0]])
==================================================
Tree indices: 
 [0, 1, 2, 11, 12, 13, 11, 12, 13]
Tree reprentation of the tensor: 
 [1, 2, 3, 4, 5, 6, 4, 5, 6]
==================================================
Medusa position ids: 
 [0, 1, 1, 2, 2, 2, 2, 2, 2]
==================================================
Retrieve indices: 
 [[0, 2, 8], [0, 2, 7], [0, 2, 6], [0, 1, 5], [0, 1, 4], [0, 1, 3]]
Retrieve reprentation of the tensor: 
 [[1, 3, 6], [1, 3, 5], [1, 3, 4], [1, 2, 6], [1, 2, 5], [1, 2, 4]]
==================================================
tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 1, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 1, 0, 0, 0],
        [1, 0, 1, 0, 0, 0, 1, 0, 0],
        [1, 0, 1, 0, 0, 0, 0, 1, 0],
        [1, 0, 1, 0, 0, 0, 0, 0, 1]], dtype=torch.int32)

</code></pre>
<h4 id="333-æ€»ä½“å¯è§†åŒ–">3.3.3 æ€»ä½“å¯è§†åŒ–</h4>
<p>å…·ä½“å¯è§†åŒ–å‚è§ä¸‹å›¾ã€‚</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250424214646451-1377660664.jpg" alt="" loading="lazy"></p>
<h4 id="334-ä½¿ç”¨">3.3.4 ä½¿ç”¨</h4>
<h5 id="è°ƒç”¨">è°ƒç”¨</h5>
<p>æ•´ä½“è°ƒç”¨ä»£ç å¦‚ä¸‹ã€‚åŸºæœ¬é€»è¾‘æ˜¯ï¼š</p>
<ul>
<li>æ ¹æ®è®¾å®šçš„medusa choiceså¾—åˆ°ç¨€ç–çš„æ ‘ç»“æ„è¡¨è¾¾ï¼Œå…·ä½“æ¶‰åŠgenerate_medusa_buffers()å‡½æ•°ã€‚</li>
<li>åˆå§‹åŒ–keyå’Œvalueã€‚</li>
<li>æ„å»ºæ ‘æ³¨æ„åŠ›æ©ç ï¼Œæ ¹æ®è¾“å…¥çš„ Prompt è¿›è¡Œé¢„æµ‹ï¼Œè¾“å‡º logits å’Œ medusa_logitsã€‚å…·ä½“æ¶‰åŠinitialize_medusa()å‡½æ•°ã€‚logitså¯¹åº” lm_head çš„è¾“å‡ºï¼Œmedusa_logitså¯¹åº”medusa_head çš„è¾“å‡ºã€‚</li>
<li>ä»æ ‘ä¸­æå–ç”¨ç¾æœèå¤´å¾—åˆ°çš„topké¢„æµ‹ã€‚è¿™äº›é¢„æµ‹æ„æˆäº†å€™é€‰è·¯å¾„ã€‚å…·ä½“æ¶‰åŠgenerate_candidates()å‡½æ•°ã€‚</li>
<li>ç”¨æ ‘æ³¨æ„åŠ›éªŒè¯å€™é€‰è·¯å¾„ï¼Œå¾—åˆ°æœ€ä½³è·¯å¾„ã€‚å…·ä½“æ¶‰åŠtree_decoding()å‡½æ•°å’Œevaluate_posterior()å‡½æ•°ã€‚tree_decoding()å‡½æ•°æ‰§è¡ŒåŸºäºæ ‘æ³¨æ„åŠ›ï¼ˆtree-attention-basedï¼‰çš„æ¨ç†ã€‚evaluate_posterior()å‡½æ•°æ‰§è¡Œå¯¹æ ‘çš„éªŒè¯ã€‚</li>
<li>æ ¹æ®å€™é€‰ Token åºåˆ—é€‰å‡ºå¯¹åº”çš„ logitsï¼Œmedusa_logitsï¼Œå¹¶æ›´æ–°è¾“å…¥ï¼Œkeyã€value cache ç­‰ã€‚å…·ä½“æ¶‰åŠupdate_inference_inputs()å‡½æ•°ã€‚</li>
</ul>
<pre><code class="language-python">def medusa_forward(input_ids, model, tokenizer, medusa_choices, temperature, posterior_threshold, posterior_alpha, top_p=0.8, sampling = 'typical', fast = True, max_steps = 512):

    # Avoid modifying the input_ids in-place
    input_ids = input_ids.clone()

    # Cache medusa buffers (the fixed patterns for tree attention)
    if hasattr(model, "medusa_choices") and model.medusa_choices == medusa_choices:
        # Load the cached medusa buffer
        medusa_buffers = model.medusa_buffers
    else:
        # Initialize the medusa buffer
        # 1. æ ¹æ®è®¾å®šçš„medusa choiceså¾—åˆ°ç¨€ç–çš„æ ‘ç»“æ„è¡¨è¾¾
        medusa_buffers = generate_medusa_buffers(
            medusa_choices, device=model.base_model.device
        )
    model.medusa_buffers = medusa_buffers
    model.medusa_choices = medusa_choices

    # Initialize the past key and value states
    if hasattr(model, "past_key_values"):
        past_key_values = model.past_key_values
        past_key_values_data = model.past_key_values_data
        current_length_data = model.current_length_data
        # Reset the past key and value states
        current_length_data.zero_()
    else:
        (
            past_key_values,
            past_key_values_data,
            current_length_data,
        ) = initialize_past_key_values(model.base_model)
        model.past_key_values = past_key_values
        model.past_key_values_data = past_key_values_data
        model.current_length_data = current_length_data

    input_len = input_ids.shape[1]
    reset_medusa_mode(model)
    
    # Initialize tree attention mask and process prefill tokens
    medusa_logits, logits = initialize_medusa(
            input_ids, model, medusa_buffers["medusa_attn_mask"], past_key_values
    )
    new_token = 0
    
    for idx in range(max_steps): 
        # Generate candidates with topk predictions from Medusa heads
        # ç”¨ç¾æœèå¤´å¾—åˆ°çš„topké¢„æµ‹æ¥ç”Ÿæˆå€™é€‰è·¯å¾„ã€‚candidatesæ˜¯å¤šä¸ªå€™é€‰ Token åºåˆ—ã€‚tree_candidatesæ˜¯Token æ ‘
        candidates, tree_candidates = generate_candidates(
                medusa_logits,
                logits,
                medusa_buffers["tree_indices"],
                medusa_buffers["retrieve_indices"],
                temperature, posterior_threshold, posterior_alpha, top_p, sampling, fast
            )
        # Use tree attention to verify the candidates and get predictions
        # ç”¨æ ‘æ³¨æ„åŠ›éªŒè¯å€™é€‰è·¯å¾„ã€‚ä½¿ç”¨ Tree Attention æœºåˆ¶å¯¹ tree_candidates è¿›è¡ŒéªŒè¯æ¨ç†ï¼Œè·å¾—æ–°çš„ logits å’Œ medusa_logits è¾“å‡ºã€‚
        medusa_logits, logits, outputs = tree_decoding(
                model,
                tree_candidates,
                past_key_values,
                medusa_buffers["medusa_position_ids"],
                input_ids,
                medusa_buffers["retrieve_indices"],
            )
        # è¯„ä¼°æ¯æ¡è·¯å¾„åˆç†æ€§ï¼Œå¾—åˆ°æœ€ä½³è·¯å¾„ã€‚å¦‚æœæ‰€æœ‰åºåˆ—éƒ½æ²¡æœ‰é€šè¿‡ï¼Œåˆ™åªä½¿ç”¨ç¬¬ä¸€ä¸ª Tokenï¼Œå¯¹åº” accept_length ä¸º 0ï¼Œå¦‚æœæŸä¸ªåºåˆ—é€šè¿‡ï¼Œåˆ™ä½¿ç”¨è¯¥åºåˆ—ä¸­çš„å·²æ¥å—çš„ Token
        best_candidate, accept_length = evaluate_posterior(
                logits, candidates, temperature, posterior_threshold, posterior_alpha , top_p, sampling, fast
            )
        # æ ¹æ®å€™é€‰ Token åºåˆ—é€‰å‡ºå¯¹åº”çš„ logitsï¼Œmedusa_logitsï¼Œå¹¶æ›´æ–°è¾“å…¥ï¼Œkeyã€value cache ç­‰
        input_ids, logits, medusa_logits, new_token = update_inference_inputs(
                input_ids,
                candidates,
                best_candidate,
                accept_length,
                medusa_buffers["retrieve_indices"],
                outputs,
                logits,
                medusa_logits,
                new_token,
                past_key_values_data,
                current_length_data,
            )
        if tokenizer.eos_token_id in input_ids[0, input_len:].tolist():
            break
        if new_token &gt; 1024:
            break
    return input_ids, new_token, idx
</code></pre>
<h5 id="åˆå§‹åŒ–">åˆå§‹åŒ–</h5>
<p>initialize_medusa()å‡½æ•°ä¼šè¿›è¡Œåˆå§‹åŒ–æ“ä½œï¼Œå¾—åˆ°logitså’Œmaskã€‚</p>
<pre><code class="language-python">def initialize_medusa(input_ids, model, medusa_attn_mask, past_key_values):
    """
    Initializes the Medusa structure for a given model.

    This function performs the following operations:
    1. Forward pass through the model to obtain the Medusa logits, original model outputs, and logits.
    2. Sets the Medusa attention mask within the base model.

    Args:
    - input_ids (torch.Tensor): The input tensor containing token ids.
    - model (MedusaLMHead): The model containing the Medusa layers and base model.
    - medusa_attn_mask (torch.Tensor): The attention mask designed specifically for the Medusa structure.
    - past_key_values (list of torch.Tensor): Contains past hidden states and past attention values.

    Returns:
    - medusa_logits (torch.Tensor): Logits from the Medusa heads.
    - logits (torch.Tensor): Original logits from the base model.
    """
    medusa_logits, outputs, logits = model(
        input_ids, past_key_values=past_key_values, output_orig=True, medusa_forward=True
    )
    model.base_model.model.medusa_mask = medusa_attn_mask
    return medusa_logits, logits
</code></pre>
<p>åœ¨å…·ä½“æ¨¡å‹ä¸­ï¼Œä¼šæŠŠmedusa_maskå’Œcausal maskç»„åˆåœ¨ä¸€èµ·ï¼Œå½¢æˆä¸€ä¸ªæ–°çš„maskã€‚æœ€ç»ˆåœ¨å‰å‘ä¼ æ’­æ—¶å€™ï¼Œä¼ é€’çš„å°±æ˜¯è¿™ä¸ªæœ€ç»ˆç»„åˆmaskã€‚</p>
<pre><code class="language-python">class LlamaModel(LlamaPreTrainedModel):
    # Copied from transformers.models.bart.modeling_bart.BartDecoder._prepare_decoder_attention_mask
    def _prepare_decoder_attention_mask(
        self, attention_mask, input_shape, inputs_embeds, past_key_values_length
    ):
        # create causal mask
        # [bsz, seq_len] -&gt; [bsz, 1, tgt_seq_len, src_seq_len]
        combined_attention_mask = None
        if input_shape[-1] &gt; 1:
            combined_attention_mask = _make_causal_mask(
                input_shape,
                # inputs_embeds.dtype,
                torch.float32,  # [MODIFIED] force to cast to float32
                device=inputs_embeds.device,
                past_key_values_length=past_key_values_length,
            )

        if attention_mask is not None:
            # [bsz, seq_len] -&gt; [bsz, 1, tgt_seq_len, src_seq_len]
            expanded_attn_mask = _expand_mask(
                attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]
            ).to(inputs_embeds.device)
            combined_attention_mask = (
                expanded_attn_mask
                if combined_attention_mask is None
                else expanded_attn_mask + combined_attention_mask
            )

        # [MODIFIED] add medusa mask
        if hasattr(self, "medusa_mask") and self.medusa_mask is not None:
            medusa_mask = self.medusa_mask
            medusa_len = medusa_mask.size(-1)
            combined_attention_mask[:, :, -medusa_len:, -medusa_len:][
                medusa_mask == 0
            ] = combined_attention_mask.min()
            if hasattr(self, "medusa_mode"):
                # debug mode
                if self.medusa_mode == "debug":
                    torch.save(combined_attention_mask, "medusa_mask.pt")

        return combined_attention_mask

    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)
    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values=None,  # [MODIFIED] past_key_value is KVCache class
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -&gt; Union[Tuple, BaseModelOutputWithPast]:

        # ......

        # embed positions
        if attention_mask is None:
            attention_mask = torch.ones(
                (batch_size, seq_length_with_past),
                dtype=torch.bool,
                device=inputs_embeds.device,
            )
        attention_mask = self._prepare_decoder_attention_mask(
            attention_mask,
            (batch_size, seq_length),
            inputs_embeds,
            past_key_values_length,
        )
        
        # ......

        # decoder layers
        for idx, decoder_layer in enumerate(self.layers):
            if self.gradient_checkpointing and self.training:
				# ......
            else:
                layer_outputs = decoder_layer(
                    hidden_states,
                    attention_mask=attention_mask,
                    position_ids=position_ids,
                    past_key_value=past_key_value,
                    output_attentions=output_attentions,
                    use_cache=use_cache,
                )

            hidden_states = layer_outputs[0]

        hidden_states = self.norm(hidden_states)
      
        # ......
</code></pre>
<h5 id="ç”Ÿæˆå€™é€‰è·¯å¾„">ç”Ÿæˆå€™é€‰è·¯å¾„</h5>
<p>generate_candidates()å‡½æ•°çš„ç»†èŠ‚å¦‚ä¸‹ï¼Œä¸»è¦æ˜¯é¢„æµ‹æ¯ä¸ªå¤´çš„topkçš„tokenï¼Œå¹¶ä¸”ç”¨ç¬›å¡å°”ç§¯ç»„è£…æˆå¯ä»¥è§£ææˆtreeçš„å€™é€‰åºåˆ—ã€‚</p>
<pre><code class="language-python">def generate_candidates(medusa_logits, logits, tree_indices, retrieve_indices, temperature = 0, posterior_threshold=0.3, posterior_alpha = 0.09, top_p=0.8, sampling = 'typical', fast = False):
    """
    Generate candidates based on provided logits and indices.
    
    Parameters:
    - medusa_logits (torch.Tensor): Logits from a specialized Medusa structure, aiding in candidate selection.
    - logits (torch.Tensor): Standard logits from a language model.
    - tree_indices (list or torch.Tensor): Indices representing a tree structure, used for mapping candidates.
    - retrieve_indices (list or torch.Tensor): Indices for extracting specific candidate tokens.
    - temperature (float, optional): Controls the diversity of the sampling process. Defaults to 0.
    - posterior_threshold (float, optional): Threshold for typical sampling. Defaults to 0.3.
    - posterior_alpha (float, optional): Scaling factor for the entropy-based threshold in typical sampling. Defaults to 0.09.
    - top_p (float, optional): Cumulative probability threshold for nucleus sampling. Defaults to 0.8.
    - sampling (str, optional): Defines the sampling strategy ('typical' or 'nucleus'). Defaults to 'typical'.
    - fast (bool, optional): If True, enables faster, deterministic decoding for typical sampling. Defaults to False.

    Returns:
    - tuple (torch.Tensor, torch.Tensor): A tuple containing two sets of candidates:
        1. Cartesian candidates derived from the combined original and Medusa logits.
        2. Tree candidates mapped from the Cartesian candidates using tree indices.
    """
    # Greedy decoding: Select the most probable candidate from the original logits.
    if temperature == 0 or fast:
        candidates_logit = torch.argmax(logits[:, -1]).unsqueeze(0)
    else:
        if sampling == 'typical':
            candidates_logit = get_typical_one_token(logits[:, -1], temperature, posterior_threshold, posterior_alpha).squeeze(0)
        elif sampling == 'nucleus':
            candidates_logit = get_nucleus_one_token(logits[:, -1], temperature, top_p).squeeze(0)
        else:
            raise NotImplementedError
    # Extract the TOPK candidates from the medusa logits.
    candidates_medusa_logits = torch.topk(medusa_logits[:, 0, -1], TOPK, dim = -1).indices

    # Combine the selected candidate from the original logits with the topk medusa logits.
    # æŠŠlm headå’Œmedusa headsçš„logitsæ‹¼æ¥åœ¨ä¸€èµ·
    candidates = torch.cat([candidates_logit, candidates_medusa_logits.view(-1)], dim=-1)

    # Map the combined candidates to the tree indices to get tree candidates.
    # ä»candidatesä¸­æ‹¿åˆ°æ ‘å¯¹åº”çš„èŠ‚ç‚¹
    tree_candidates = candidates[tree_indices]

    # Extend the tree candidates by appending a zero.
    tree_candidates_ext = torch.cat([tree_candidates, torch.zeros((1), dtype=torch.long, device=tree_candidates.device)], dim=0)

    # ä»æ ‘èŠ‚ç‚¹ä¸­æ‹¿åˆ°ç¬›å¡å°”ç§¯
    # Retrieve the cartesian candidates using the retrieve indices.
    cart_candidates = tree_candidates_ext[retrieve_indices]

    # Unsqueeze the tree candidates for dimension consistency.
    tree_candidates = tree_candidates.unsqueeze(0)
    return cart_candidates, tree_candidates
</code></pre>
<h5 id="éªŒè¯å€™é€‰è·¯å¾„">éªŒè¯å€™é€‰è·¯å¾„</h5>
<p>tree_decoding()å‡½æ•°ç»†èŠ‚å¦‚ä¸‹ã€‚å¯¹ä¸Šé¢çš„å¾—åˆ°çš„æ‹‰å¹³çš„åºåˆ—ï¼Œç”¨åŸºç¡€çš„LLMæ¨¡å‹é¢„æµ‹æ¯ä¸€æ¡è·¯å¾„çš„æ¦‚ç‡ï¼Œæœ€åæ ¹æ®retrieve_indicesè¿˜åŸåˆ°åŸå§‹çš„ç¬›å¡å°”ç§¯çš„è·¯å¾„ï¼Œå¯ä»¥å¾—åˆ°è·¯å¾„ä¸Šæ¯ä¸ªä½ç½®çš„æ¦‚ç‡ã€‚</p>
<pre><code class="language-python">def tree_decoding(
    model,
    tree_candidates,
    past_key_values,
    medusa_position_ids,
    input_ids,
    retrieve_indices,
):
    """
    Decode the tree candidates using the provided model and reorganize the logits.
    
    Parameters:
    - model (nn.Module): Model to be used for decoding the tree candidates.
    - tree_candidates (torch.Tensor): Input candidates based on a tree structure.
    - past_key_values (torch.Tensor): Past states, such as key and value pairs, used in attention layers.
    - medusa_position_ids (torch.Tensor): Positional IDs associated with the Medusa structure.
    - input_ids (torch.Tensor): Input sequence IDs.
    - retrieve_indices (list or torch.Tensor): Indices for reordering the logits.
    
    Returns:
    - tuple: Returns medusa logits, regular logits, and other outputs from the model.
    """

    # Compute new position IDs by adding the Medusa position IDs to the length of the input sequence.
    position_ids = medusa_position_ids + input_ids.shape[1]

    # Use the model to decode the tree candidates. 
    # The model is expected to return logits for the Medusa structure, original logits, and possibly other outputs.
    tree_medusa_logits, outputs, tree_logits = model(
        tree_candidates,
        output_orig=True,
        past_key_values=past_key_values,
        position_ids=position_ids,
        medusa_forward=True,
    )
    
    # Reorder the obtained logits based on the retrieve_indices to ensure consistency with some reference ordering.
    logits = tree_logits[0, retrieve_indices] # ä»logitsé‡Œé¢æ ¹æ®retrieve_indicesè·å–ç¬›å¡å°”ç§¯
    medusa_logits = tree_medusa_logits[:, 0, retrieve_indices]
    return medusa_logits, logits, outputs

</code></pre>
<h5 id="è®¡ç®—æœ€ä¼˜è·¯å¾„">è®¡ç®—æœ€ä¼˜è·¯å¾„</h5>
<p>evaluate_posterior()å‡½æ•°ä¼šè®¡ç®—æœ€ä¼˜è·¯å¾„ã€‚</p>
<pre><code class="language-python">def evaluate_posterior(
    logits, candidates, temperature, posterior_threshold=0.3, posterior_alpha = 0.09, top_p=0.8, sampling = 'typical', fast = True
):
    """
    Evaluate the posterior probabilities of the candidates based on the provided logits and choose the best candidate.

    Depending on the temperature value, the function either uses greedy decoding or evaluates posterior
    probabilities to select the best candidate.

    Args:
    - logits (torch.Tensor): Predicted logits of shape (batch_size, sequence_length, vocab_size).
    - candidates (torch.Tensor): Candidate token sequences.
    - temperature (float): Softmax temperature for probability scaling. A value of 0 indicates greedy decoding.
    - posterior_threshold (float): Threshold for posterior probability.
    - posterior_alpha (float): Scaling factor for the threshold.
    - top_p (float, optional): Cumulative probability threshold for nucleus sampling. Defaults to 0.8.
    - sampling (str, optional): Defines the sampling strategy ('typical' or 'nucleus'). Defaults to 'typical'.
    - fast (bool, optional): If True, enables faster, deterministic decoding for typical sampling. Defaults to False.
    Returns:
    - best_candidate (torch.Tensor): Index of the chosen best candidate.
    - accept_length (int): Length of the accepted candidate sequence.
    """
    # Greedy decoding based on temperature value
    if temperature == 0:
        # Find the tokens that match the maximum logits for each position in the sequence
        posterior_mask = (
            candidates[:, 1:] == torch.argmax(logits[:, :-1], dim=-1)
        ).int()
        candidates_accept_length = (torch.cumprod(posterior_mask, dim=1)).sum(dim=1)
        accept_length = candidates_accept_length.max()
        # Choose the best candidate
        if accept_length == 0:
            # Default to the first candidate if none are accepted
            best_candidate = torch.tensor(0, dtype=torch.long, device=candidates.device)
        else:
            best_candidate = torch.argmax(candidates_accept_length).to(torch.long)
        return best_candidate, accept_length
        
    if sampling == 'typical':
        if fast:
            posterior_prob = torch.softmax(logits[:, :-1] / temperature, dim=-1)
            candidates_prob = torch.gather(
                posterior_prob, dim=-1, index=candidates[:, 1:].unsqueeze(-1)
            ).squeeze(-1)
            posterior_entropy = -torch.sum(
                posterior_prob * torch.log(posterior_prob + 1e-5), dim=-1
            )  # torch.sum(torch.log(*)) is faster than torch.prod
            threshold = torch.minimum(
                torch.ones_like(posterior_entropy) * posterior_threshold,
                torch.exp(-posterior_entropy) * posterior_alpha,
            )
            posterior_mask = candidates_prob &gt; threshold
            candidates_accept_length = (torch.cumprod(posterior_mask, dim=1)).sum(dim=1)

            # Choose the best candidate based on the evaluated posterior probabilities
            accept_length = candidates_accept_length.max()
            if accept_length == 0:
                # If no candidates are accepted, just choose the first one
                best_candidate = torch.tensor(0, dtype=torch.long, device=candidates.device)
            else:
                best_candidates = torch.where(candidates_accept_length == accept_length)[0]
                # Accept the best one according to likelihood
                likelihood = torch.sum(
                    torch.log(candidates_prob[best_candidates, :accept_length]), dim=-1
                )
                best_candidate = best_candidates[torch.argmax(likelihood)]
            return best_candidate, accept_length
        # Calculate posterior probabilities and thresholds for candidate selection
        posterior_mask = get_typical_posterior_mask(logits, candidates, temperature, posterior_threshold, posterior_alpha, fast)
        candidates_accept_length = (torch.cumprod(posterior_mask, dim=1)).sum(dim=1)
        # Choose the best candidate based on the evaluated posterior probabilities
        accept_length = candidates_accept_length.max()
        
        if accept_length == 0:
            # If no candidates are accepted, just choose the first one
            best_candidate = torch.tensor(0, dtype=torch.long, device=candidates.device)
        else:
            best_candidate = torch.argmax(candidates_accept_length).to(torch.long)
            # Accept the best one according to likelihood
        return best_candidate, accept_length
    
    if sampling == 'nucleus':
        assert top_p &lt; 1.0 + 1e-6, "top_p should between 0 and 1"
        posterior_mask = get_nucleus_posterior_mask(logits, candidates, temperature, top_p)
        candidates_accept_length = (torch.cumprod(posterior_mask, dim=1)).sum(dim=1)
        accept_length = candidates_accept_length.max()
        # Choose the best candidate
        if accept_length == 0:
            # Default to the first candidate if none are accepted
            best_candidate = torch.tensor(0, dtype=torch.long, device=candidates.device)
        else:
            best_candidate = torch.argmax(candidates_accept_length).to(torch.long)
        return best_candidate, accept_length
    else:
        raise NotImplementedError
</code></pre>
<h3 id="34-typical-acceptance">3.4 Typical Acceptance</h3>
<p>åœ¨æŠ•æœºè§£ç ä¸­ï¼Œæ‹’ç»é‡‡æ ·æ˜¯æŒ‡ä»è‰ç¨¿æ¨¡å‹çš„è¾“å‡ºä¸­éšæœºé‡‡æ ·ä¸€ä¸ª token åºåˆ—ï¼Œç„¶åä½¿ç”¨åŸå§‹æ¨¡å‹æ¥éªŒè¯æ˜¯å¦æ¥å—ã€‚å¦‚æœéªŒè¯å¤±è´¥ï¼Œå°±é‡æ–°é‡‡æ ·ï¼Œç›´è‡³æ‰¾åˆ°ä¸€ä¸ªåˆé€‚çš„ token åºåˆ—ã€‚è€Œåœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¾€å¾€ä¸éœ€è¦å®Œå…¨åŒ¹é…åŸå§‹æ¨¡å‹çš„åˆ†å¸ƒï¼Œåªè¦ä¿è¯è¾“å‡ºçš„è´¨é‡å’Œå¤šæ ·æ€§å³å¯ï¼Œè¿™æ ·å¯ä»¥è·å–æ›´åŠ åˆç†çš„å€™é€‰tokenï¼Œä¹Ÿå¯ä»¥åŠ é€Ÿè§£ç è¿‡ç¨‹ã€‚å› æ­¤ Medusa ä½¿ç”¨äº†å…¸å‹æ¥å—æ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆæ˜¯åŸºäºåŸå§‹æ¨¡å‹é¢„æµ‹çš„æ¦‚ç‡ï¼Œä½¿ç”¨æ¸©åº¦æ¥è®¾å®šä¸€ä¸ªé˜ˆå€¼ï¼Œæ ¹æ®è¿™ä¸ªé˜ˆå€¼æ¥å†³å®šæ˜¯å¦æ¥å—å€™é€‰çš„ tokenã€‚å¦‚æœå€™é€‰ token çš„æ¦‚ç‡è¶…è¿‡äº†é˜ˆå€¼ï¼Œå°±è®¤ä¸ºè¿™ä¸ª token æ˜¯ã€Œå…¸å‹ã€çš„ï¼Œåº”è¯¥æ¥å—ã€‚</p>
<h4 id="341-å¸¸è§é‡‡ç”¨æ–¹æ³•">3.4.1 å¸¸è§é‡‡ç”¨æ–¹æ³•</h4>
<p>LLMæ¨¡å‹çš„è¾“å‡ºæ˜¯åœ¨è¯è¡¨ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒï¼Œé‡‡æ ·ç­–ç•¥ç›´æ¥å†³å®šäº†æˆ‘ä»¬å¾—åˆ°æ€ä¹ˆæ ·çš„è¾“å‡ºæ•ˆæœã€‚æœ‰æ—¶å€™æˆ‘ä»¬å¸Œæœ›å¾—åˆ°å®Œå…¨ç¡®å®šçš„ç»“æœï¼Œæœ‰æ—¶å€™å¸Œæœ›å¾—åˆ°æ›´åŠ ä¸°å¯Œæœ‰è¶£çš„ç»“æœã€‚</p>
<p>ç¡®å®šæ€§é‡‡æ ·çš„è¾“å‡ºç»“æœæ˜¯ç¡®å®šæ€§çš„ï¼Œæœ¬è´¨ä¸Šæ˜¯æœç´¢è¿‡ç¨‹ï¼Œå…¸å‹ä¸¤ç§æ–¹æ³•å¦‚ä¸‹ã€‚</p>
<ul>
<li>Greedy Searchã€‚æ¯æ¬¡é€‰å–æ¦‚ç‡æœ€é«˜çš„tokenè¾“å‡ºã€‚</li>
<li>Beam Searchã€‚ç»´æŠ¤beamçš„å¤§å°ä¸ºkï¼Œå¯¹å½“å‰beamä¸­çš„æ‰€æœ‰pathåšä¸‹ä¸ªtokençš„å±•å¼€ï¼Œé€‰å–ç´¯ç§¯æ¦‚ç‡æœ€é«˜çš„å‰kä¸ªpathï¼Œä½œä¸ºæ–°çš„beamï¼Œä»¥æ­¤ç±»æ¨ã€‚</li>
</ul>
<p>æ¦‚ç‡æ€§é‡‡æ ·ä¼šåŸºäºæ¦‚ç‡åˆ†å¸ƒåšé‡‡æ ·ï¼Œå¸¸è§çš„æœ‰ä»¥ä¸‹3ç§</p>
<ul>
<li>Multinomialé‡‡æ ·ã€‚ç›´æ¥åŸºäºæ¦‚ç‡åˆ†å¸ƒåšçº¯éšæœºé‡‡æ ·ï¼Œå®¹æ˜“é‡‡åˆ°æä½æ¦‚ç‡çš„è¯ã€‚</li>
<li>Top-ké‡‡æ ·ã€‚åœ¨æ¦‚ç‡æ’åå‰kçš„å€™é€‰é›†ä¸­åšéšæœºé‡‡æ ·ï¼Œæ³¨æ„é‡‡æ ·å‰åšé‡æ–°å½’ä¸€åŒ–ã€‚</li>
<li>Top-pé‡‡æ ·ã€‚ä¹Ÿå«Nucleusé‡‡æ ·ï¼Œå…ˆå¯¹è¾“å‡ºæ¦‚ç‡åšä»å¤§åˆ°å°çš„æ’åºï¼Œç„¶ååœ¨ç´¯ç§¯æ¦‚ç‡è¾¾åˆ°pçš„è¿™äº›å€™é€‰é›†ä¸­åšéšæœºé‡‡æ ·ï¼ŒåŒæ ·éœ€è¦åšé‡æ–°å½’ä¸€åŒ–ã€‚</li>
</ul>
<p>åŸºäºé‡‡æ ·çš„æ–¹æ³•ä¸­å¾€å¾€æœ‰ä¸€ä¸ªæ¸©åº¦å‚æ•°ï¼Œæ¸©åº¦è¶Šé«˜é‡‡æ ·çš„å¤šæ ·æ€§è¶Šé«˜ï¼Œé€‚ç”¨äºåˆ›æ„ç”Ÿæˆçš„åœºæ™¯ï¼Œæ¯”å¦‚å†™ä½œæ–‡ã€‚</p>
<h4 id="342-æ€è·¯">3.4.2 æ€è·¯</h4>
<p>æ¨æµ‹è§£ç ä¸­ï¼Œä½œè€…é‡‡ç”¨æ‹’ç»é‡‡æ ·æ¥äº§ç”Ÿä¸åŸå§‹æ¨¡å‹çš„åˆ†å¸ƒä¸€è‡´çš„ä¸åŒè¾“å‡ºã€‚ç„¶è€Œï¼Œåç»­çš„ç ”ç©¶å·¥ä½œå‘ç°ï¼Œéšç€é‡‡æ ·æ¸©åº¦çš„å‡é«˜ï¼Œè¿™ç§é‡‡æ ·ç­–ç•¥ä¼šå¯¼è‡´æ•ˆç‡é™ä½ã€‚æ¯”å¦‚ï¼Œdraftæ¨¡å‹ä¸targetæ¨¡å‹ä¸€æ ·å¥½ï¼Œä»–ä»¬çš„åˆ†å¸ƒå®Œç¾åœ°å¯¹é½ã€‚åœ¨è¿™ç§çŠ¶æ€ä¸‹ï¼Œæˆ‘ä»¬åº”è¯¥æ¥å—draftæ¨¡å‹æ‰€æœ‰è¾“å‡ºã€‚ç„¶è€Œï¼Œå› ä¸ºè‰ç¨¿æ¨¡å‹ä¸åŸå§‹æ¨¡å‹è¿›è¡Œç‹¬ç«‹é‡‡æ ·ï¼Œtemperatureæå‡ä¸€èˆ¬å¯¹åº”æ›´å¼ºçš„creativityç‰¹æ€§ï¼Œdraft modelæ‰€é€‰æ‹©çš„å€™é€‰tokençš„å¤šæ ·æ€§å°±å¢å¤§ï¼Œä¹Ÿå°±é™ä½äº†å‘½ä¸­åŸæ¨¡å‹tokenè¢«æ¥å—çš„æ¦‚ç‡ï¼Œä»è€Œå¯¼è‡´å¹¶è¡Œè§£ç é•¿åº¦å¾ˆçŸ­ã€‚è€Œæ­¤æ—¶ï¼Œè´ªå©ªè§£ç ä¼šæ¥å—è‰ç¨¿æ¨¡å‹çš„æ‰€æœ‰è¾“å‡ºï¼Œåè€Œä¼šæœ€å¤§åŒ–æ•ˆç‡ã€‚</p>
<p>ä½†æ˜¯è¿™ç§ç‰¹æ€§å¹¶ä¸åˆç†ã€‚å› ä¸ºåœ¨ç°å®åœºæ™¯ä¸­ï¼Œè¯­è¨€æ¨¡å‹çš„é‡‡æ ·é€šå¸¸ç”¨äºç”Ÿæˆä¸åŒçš„å“åº”ï¼Œè€Œæ¸©åº¦å‚æ•°ä»…ç”¨äºè°ƒèŠ‚å“åº”çš„"åˆ›é€ åŠ›"ã€‚å› æ­¤ï¼Œè¾ƒé«˜çš„æ¸©åº¦åº”è¯¥ä¼šå¯¼è‡´åŸå§‹æ¨¡å‹æœ‰æ›´å¤šæœºä¼šæ¥å—è‰ç¨¿æ¨¡å‹çš„è¾“å‡ºï¼Œä½†ä¸ä¸€å®šè¦åŒ¹é…åŸå§‹æ¨¡å‹çš„åˆ†å¸ƒã€‚é‚£ä¹ˆï¼Œä¸ºä»€ä¹ˆä¸åªæ˜¯ä¸“æ³¨äºæ¥å—ä¼¼ä¹åˆç†ï¼ˆplausibleï¼‰çš„å€™é€‰tokenå‘¢ï¼Ÿ</p>
<h4 id="343-typical-acceptance">3.4.3 Typical Acceptance</h4>
<p>MEDUSAè®¤ä¸ºæ—¢ç„¶é‡‡æ ·å°±æ˜¯è¿½æ±‚åˆ›é€ æ€§ï¼Œå€™é€‰åºåˆ—çš„åˆ†å¸ƒæ²¡æœ‰å¿…è¦å®Œå…¨åŒ¹é…åŸæ¨¡å‹çš„åˆ†å¸ƒã€‚æˆ‘ä»¬è¦åšçš„åº”è¯¥æ˜¯é€‰å‡ºtypicalçš„å€™é€‰ï¼Œä¹Ÿå°±æ˜¯ï¼Œåªè¦å€™é€‰åºåˆ—ä¸æ˜¯æä¸å¯èƒ½çš„ç»“æœï¼Œå°±å¯ä»¥è¢«æ¥å—ã€‚ç›´è§‚ç†è§£æ˜¯æˆ‘ä»¬åœ¨LLMè§£ç è¿‡ç¨‹ï¼Œä¸éœ€è¦å¤ªç¡®å®šçš„è¯ï¼Œä¹Ÿä¸èƒ½æœ‰å¤ªè¶…å‡ºé¢„æœŸçš„è¯ï¼Œè¿™æ ·å°±èƒ½ä¿è¯æˆ‘ä»¬èƒ½å¾—åˆ°ä¸°å¯Œä¸”é¿å…é‡å¤ç”Ÿæˆçš„è¯æ±‡ã€‚</p>
<p>äºæ˜¯ï¼ŒMedusaä»æˆªæ–­é‡‡æ ·ï¼ˆTruncation Samplingï¼‰å·¥ä½œä¸­æ±²å–çµæ„Ÿï¼Œæ—¨åœ¨æ‰©å¤§é€‰æ‹©åŸå§‹æ¨¡å‹å¯èƒ½æ¥å—çš„å€™é€‰é¡¹ã€‚Medusa æ ¹æ®åŸå§‹æ¨¡å‹çš„é¢„æµ‹æ¦‚ç‡è®¾å®šä¸€ä¸ªé˜ˆå€¼ï¼Œå¦‚æœå€™é€‰tokenè¶…è¿‡äº†è¿™ä¸ªé˜ˆå€¼ï¼Œå°±ä¼šè¢«æ¥å—è¯¥token åŠå…¶ prefixï¼Œå¹¶åœ¨è¿™äº›tokenä¸­åšGreedyé‡‡æ ·é€‰æ‹©top-kã€‚è€Œè¿™ä¸ªé˜ˆå€¼ç”±åŸå§‹æ¨¡å‹çš„é¢„æµ‹æ¦‚ç‡ç›¸å…³ã€‚</p>
<p>å…·ä½“æ¥è¯´ï¼Œä½œè€…é‡‡å–hard thresholdå’Œentropy-dependent thresholdçš„æœ€å°å€¼æ¥å†³å®šæ˜¯å¦åƒåœ¨truncation samplingä¸­é‚£æ ·æ¥å—ä¸€ä¸ªå€™é€‰tokenã€‚è¿™ç¡®ä¿äº†åœ¨è§£ç è¿‡ç¨‹ä¸­é€‰æ‹©æœ‰æ„ä¹‰çš„tokenå’Œåˆç†çš„å»¶ç»­ã€‚ä½œè€…æ€»æ˜¯ä½¿ç”¨Greedy Decodingæ¥å—ç¬¬ä¸€ä¸ªtokenï¼Œç¡®ä¿æ¯ä¸€æ­¥è‡³å°‘ç”Ÿæˆä¸€ä¸ªtokenã€‚æœ€åé€‰æ‹©è¢«æ¥å—çš„è§£ç é•¿åº¦æœ€é•¿çš„å€™é€‰åºåˆ—ä½œä¸ºæœ€ç»ˆç»“æœã€‚è¿™ç§æ–¹æ³•çš„å¥½å¤„æ˜¯å…¶é€‚åº”æ€§ï¼šå¦‚æœä½ å°†é‡‡æ ·æ¸©åº¦è®¾ä¸ºé›¶ï¼Œå®ƒå°±ç®€å•åœ°å›å½’åˆ°æœ€é«˜æ•ˆçš„å½¢å¼Greedy Searchã€‚å½“ä½ æé«˜æ¸©åº¦æ—¶ï¼Œæ­¤æ–¹æ³•å˜å¾—æ›´åŠ é«˜æ•ˆï¼Œå…è®¸æ›´é•¿çš„æ¥å—åºåˆ—ã€‚</p>
<ul>
<li>å½“æ¦‚ç‡åˆ†å¸ƒä¸­æœ‰ä¸ªåˆ«tokençš„æ¦‚ç‡å¾ˆé«˜ï¼Œè¿™æ—¶ç†µå°ï¼Œ expâ¡(âˆ’ğ»(â‹…)) å¤§ï¼Œtokenæ¥å—çš„æ¡ä»¶æ›´ä¸¥æ ¼ã€‚</li>
<li>å½“æ¦‚ç‡åˆ†å¸ƒä¸­æ¯ä¸ªtokençš„æ¦‚ç‡æ¯”è¾ƒå¹³å‡æ—¶ï¼Œç†µå¤§ï¼Œ expâ¡(âˆ’ğ»(â‹…)) å°ï¼Œtokenæ¥å—çš„æ¡ä»¶å®½æ¾ä¸€äº›ã€‚</li>
</ul>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250424214713432-794547550.jpg" alt="" loading="lazy"></p>
<p>å…·ä½“å®ç°ä½äºevaluate_posterior()å‡½æ•°ä¸­ï¼Œè¿™é‡Œä¸å†èµ˜è¿°ã€‚</p>
<h2 id="0x04-è®­ç»ƒ">0x04 è®­ç»ƒ</h2>
<p>MEDUSAçš„è¿™äº›åˆ†ç±»å¤´éœ€è¦ç»è¿‡è®­ç»ƒæ‰èƒ½æœ‰æ¯”è¾ƒå¥½çš„é¢„æµ‹æ•ˆæœã€‚é’ˆå¯¹ä¸åŒçš„æ¡ä»¶ï¼Œå¯ä»¥é€‰æ‹©ä¸åŒçš„è®­ç»ƒæ–¹å¼ï¼š</p>
<ul>
<li>MEDUSA-1ï¼šå†»ç»“åŸæ¨¡å‹çš„backboneï¼ˆåŒ…æ‹¬åŸæ¨¡å‹çš„è§£ç å¤´ï¼‰ï¼Œåªè®­ç»ƒå¢åŠ çš„è§£ç å¤´ã€‚è¿™ç§æ–¹æ¡ˆé€‚ç”¨äºè®¡ç®—èµ„æºæ¯”è¾ƒå°‘ï¼Œæˆ–è€…ä¸æƒ³å½±å“åŸæ¨¡å‹çš„æ•ˆæœçš„æƒ…å†µã€‚è¿˜å¯ä»¥ä½¿ç”¨QLoRAå¯¹è§£ç å¤´è¿›è¡Œè®­ç»ƒï¼Œè¿›ä¸€æ­¥èŠ‚çœå†…å­˜å’Œè®¡ç®—èµ„æºã€‚</li>
<li>MEDUSA-2ï¼šåŸæ¨¡å‹å’ŒMEDUSAçš„è§£ç å¤´ä¸€èµ·è®­ç»ƒã€‚MEDUSA-1è¿™æ ·çš„è®­ç»ƒæ–¹æ³•è™½ç„¶å¯ä»¥èŠ‚çœèµ„æºï¼Œä½†æ˜¯å¹¶ä¸èƒ½æœ€å¤§ç¨‹åº¦å‘æŒ¥å¤šä¸ªè§£ç å¤´çš„åŠ é€Ÿæ•ˆæœï¼Œè€ŒMEDUSA-2åˆ™å¯ä»¥è¿›ä¸€æ­¥å‘æŒ¥MEDUSAè§£ç å¤´çš„æé€Ÿèƒ½åŠ›ã€‚è€Œä¸”ï¼Œç”±äºæ˜¯åŸºå¹²æ¨¡å‹ä¸Medusa Headsä¸€èµ·è¿›è¡Œè®­ç»ƒï¼Œç¡®ä¿äº†MEDUSA headsçš„åˆ†å¸ƒä¸åŸå§‹æ¨¡å‹çš„åˆ†å¸ƒä¿æŒä¸€è‡´ï¼Œä»è€Œå‡è½»äº†åˆ†å¸ƒæ¼‚ç§»é—®é¢˜ï¼Œæ˜¾è‘—æé«˜Headsçš„å‡†ç¡®æ€§ã€‚MEDUSA-2é€‚ç”¨äºè®¡ç®—èµ„æºå……è¶³ï¼Œæˆ–è€…ä»Baseæ¨¡å‹è¿›è¡ŒSFTçš„åœºæ™¯ã€‚</li>
</ul>
<p>å¦å¤–ï¼Œå¦‚æœåŸæ¨¡å‹çš„SFTæ•°æ®é›†æ˜¯availableçš„ï¼Œé‚£å¯ä»¥ç›´æ¥è¿›è¡Œè®­ç»ƒã€‚å¦‚æœä¸èƒ½è·å¾—åŸæ¨¡å‹çš„SFTæ•°æ®ï¼Œæˆ–è€…åŸæ¨¡å‹æ˜¯ç»è¿‡RLHFè®­ç»ƒçš„ï¼Œåˆ™å¯ä»¥é€šè¿‡self-distillationæ¥è·å–MEDUSA headçš„è®­ç»ƒæ•°æ®ã€‚</p>
<h3 id="41-medusa-1">4.1 MEDUSA-1</h3>
<p>MEDUSA-1å†»ç»“äº†åŸæ¨¡å‹çš„å‚æ•°ï¼Œè€Œåªå¯¹æ–°å¢çš„è§£ç å¤´è¿›è¡Œè®­ç»ƒã€‚ä½¿ç”¨Medusa-1è®­ç»ƒHeadsï¼Œä¸»è¦è®¡ç®—Medusa Headsé¢„æµ‹çš„ç»“æœä¸Ground Truthä¹‹é—´çš„äº¤å‰ç†µæŸå¤±ã€‚å…·ä½“è®¡ç®—ä¸ºï¼Œç»™å®šä½ç½® t+k+1 å¤„çš„Ground Truth <span class="math inline">\(y_{t+k+1}\)</span> ,åˆ™ç¬¬ k ä¸ªHeadçš„è®­ç»ƒlosså¯ä»¥å†™ä½œï¼š</p>
<p></p><div class="math display">\[\mathcal{L}_k=-\log p_t^{(k)}(y_{t+k+1})
\]</div><p></p><p>å¹¶ä¸”å½“k è¾ƒå¤§æ—¶ï¼Œ <span class="math inline">\(\mathcal{L}_k\)</span> ä¹Ÿä¼šéšä¹‹å˜å¤§ï¼Œå› ä¸ºå½“ k å˜å¤§æ—¶ï¼Œé åçš„Headçš„é¢„æµ‹å°†æ›´åŠ ä¸ç¡®å®šã€‚ä¸ºäº†å¹³è¡¡å„ä¸ª Head ä¸Š loss çš„å¤§å°ï¼Œå› æ­¤åœ¨ <span class="math inline">\(\mathcal{L}_k\)</span>  ä¸Šå¢åŠ æŒ‡æ•°è¡°å‡çš„æƒé‡å‚æ•° <span class="math inline">\(\lambda_k\)</span> æ¥å¹³è¡¡ä¸åŒheadçš„æŸå¤±ã€‚æœ€ç»ˆMedusaçš„æŸå¤±è®¡ç®—å¦‚ä¸‹ï¼š</p>
<p></p><div class="math display">\[\mathcal{L}_{\text{MEDUSA-l}}=\sum_{k=1}^K-\lambda_k\log p_t^{(k)}(y_{t+k+1})
\]</div><p></p><p>è¿™é‡Œçš„  <span class="math inline">\(\lambda_{k}\)</span> æ˜¯æ¯ä¸ªè§£ç å¤´çš„ç¼©æ”¾ç³»æ•°ï¼Œæ˜¯ä¸€ç³»åˆ—è¶…å‚ã€‚å› ä¸º k è¶Šå¤§ï¼Œå¯¹åº”è§£ç å¤´çš„é¢„æµ‹éš¾åº¦è¶Šå¤§ï¼Œlossä¹Ÿå°±è¶Šå¤§ï¼Œä¸ºäº†é˜²æ­¢é åçš„è§£ç å¤´è¿‡åˆ†ä¸»å¯¼è®­ç»ƒï¼Œå› æ­¤ä½¿ç”¨ä¸€ä¸ªç¼©æ”¾ç³»æ•°è¿›è¡Œè°ƒæ•´ã€‚å®é™…ä½¿ç”¨ä¸­ï¼Œ<span class="math inline">\(\lambda_{k}=0.8^{k}\)</span>ã€‚</p>
<h3 id="42-medusa-2">4.2 MEDUSA-2</h3>
<p>ä¸ºäº†è¿›ä¸€æ­¥æé«˜Medusa Headsçš„å‡†ç¡®æ€§ï¼ŒMEDUSA-2æŠŠåŸæ¨¡å‹å’Œå¤šä¸ªè§£ç å¤´ä¸€èµ·è®­ç»ƒï¼Œå› æ­¤å„ä¸ªè§£ç å¤´çš„å‡†ç¡®ç‡èƒ½è¾¾åˆ°æ›´é«˜çš„æ°´å¹³ï¼Œacceleration rateä¹Ÿæ›´é«˜ã€‚ä½†æ˜¯ä¸ºäº†ä¿æŒåŸæ¨¡å‹çš„è¾“å‡ºè´¨é‡ï¼Œéœ€è¦ä¸€äº›ç‰¹æ®Šçš„è®­ç»ƒæŠ€å·§ã€‚Medusa-2ä½¿ç”¨ä»¥ä¸‹ä¸‰ä¸ªç­–ç•¥æ¥å®ç°è¿™ä¸ªç›®æ ‡ã€‚</p>
<p><strong>Combined loss</strong></p>
<p>ä¸ºäº†ä¿æŒbackboneæ¨¡å‹ next tokené¢„æµ‹çš„èƒ½åŠ›ï¼Œéœ€è¦å°†backboneæ¨¡å‹çš„äº¤å‰ç†µæŸå¤± <span class="math inline">\(L_{LM}\)</span>æ·»åŠ åˆ°MedusaæŸå¤±ä¸­ï¼Œå³æŠŠåŸæ¨¡å‹è§£ç å¤´çš„lossä¹ŸåŠ ä¸Šã€‚åŒæ—¶è¿˜éœ€è¦æ·»åŠ ä¸€ä¸ªæƒé‡å› å­ <span class="math inline">\(\lambda_0\)</span> æ¥å¹³è¡¡backboneå’ŒMedusa Headsä¹‹é—´çš„æŸå¤±ã€‚å…·ä½“å¦‚ä¸‹å¼</p>
<p></p><div class="math display">\[\mathcal{L}_{\text{MEDUSA-}2}=\mathcal{L}_{\text{LM}}+\lambda_0\mathcal{L}_{\text{MEDUSA-}1} \\\mathcal{L}_{\text{LM}}=-\log p_t^{(0)}(y_{t+1})
\]</div><p></p><p>å®é™…ä½¿ç”¨ä¸­ï¼Œç›´æ¥è®­ç»ƒæ—¶ <span class="math inline">\(\lambda_0=0.2\)</span>ï¼Œä½¿ç”¨self-distillationæ—¶<span class="math inline">\(\lambda_0=0.01\)</span>ã€‚</p>
<p><strong>Differential learning rates</strong></p>
<p>åŸæ¨¡å‹å·²ç»æ˜¯è®­ç»ƒå¥½äº†çš„ï¼Œï¼Œè€Œ MEDUSA headséœ€è¦æ›´å¤šè®­ç»ƒï¼Œå› æ­¤åŸæ¨¡å‹å’Œæ–°åŠ å…¥çš„è§£ç å¤´ä½¿ç”¨ç›¸åŒçš„å­¦ä¹ ç‡å¹¶ä¸åˆé€‚ã€‚æˆ‘ä»¬å¯ä»¥è®©æ–°çš„è§£ç å¤´ä½¿ç”¨æ›´å¤§çš„å­¦ä¹ ç‡ï¼Œè€ŒåŸæ¨¡å‹å‚æ•°ä½¿ç”¨ç›¸å¯¹å°çš„å­¦ä¹ ç‡ï¼Œä»¥å®ç° MEDUSA headsæ›´å¿«çš„æ”¶æ•›ï¼ŒåŒæ—¶ä¿ç•™backboneæ¨¡å‹çš„èƒ½åŠ›ã€‚å®è·µä¸­æŠŠå­¦ä¹ ç‡å·®è·è®¾ä¸º4å€ï¼Œæ¯”å¦‚åˆ†åˆ«ä½¿ç”¨2e-3å’Œ5e-4ã€‚</p>
<p><strong>Heads warmup</strong></p>
<p>æ–°åŠ å…¥çš„è§£ç å¤´åœ¨ä¸€å¼€å§‹è®­ç»ƒä¼šæœ‰æ¯”è¾ƒå¤§çš„lossï¼Œä»è€Œå¯¼è‡´æ›´å¤§çš„æ¢¯åº¦ï¼Œæœ‰å¯èƒ½æŸå®³åŸæ¨¡å‹çš„èƒ½åŠ›ã€‚é’ˆå¯¹è¿™ä¸ªé—®é¢˜ï¼Œå¯ä»¥ä½¿ç”¨ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹gçš„æ–¹å¼ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œå…ˆåœ¨MEDUSA-1çš„ç­–ç•¥ä¸‹ä»…è®­ç»ƒè§£ç å¤´ï¼Œåœ¨ç¬¬äºŒé˜¶æ®µï¼Œå†è¿›è¡ŒMEDUSA-2çš„è®­ç»ƒã€‚è¿™å…¶å®ç›¸å½“äºæŠŠ  <span class="math inline">\(\lambda_0\)</span> åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ¸å¢å¤§ã€‚</p>
<h3 id="43-ä»£ç ">4.3 ä»£ç </h3>
<p>æˆ‘ä»¬å†æ¥çœ‹çœ‹ä¸€ä¸ªå·²ç»è®­ç»ƒå¥½çš„LLMå¦‚ä½•é€‚é…MEDUSAï¼Œå…·ä½“åˆ†ä¸ºå¦‚ä¸‹å‡ æ­¥ï¼š</p>
<ul>
<li>æ·»åŠ è§£ç å¤´ï¼šåœ¨ LLM æœ€åä¸€ä¸ªéšè—å±‚åæ·»åŠ è‹¥å¹²ä¸ª MEDUSA è§£ç å¤´ã€‚</li>
<li>åˆå§‹åŒ–è§£ç å¤´ï¼šå¯ä½¿ç”¨éšæœºåˆå§‹åŒ–ï¼Œä¹Ÿå¯ä½¿ç”¨åŸå§‹æ¨¡å‹è§£ç å¤´çš„å‚æ•°è¿›è¡Œåˆå§‹åŒ–ï¼Œè¿™æ ·å¯ä»¥åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚</li>
<li>é€‰æ‹©è®­ç»ƒç­–ç•¥ ï¼šæ ¹æ®å®é™…æƒ…å†µé€‰æ‹© MEDUSA-1 æˆ– MEDUSA-2 ç­–ç•¥ã€‚</li>
<li>å‡†å¤‡è®­ç»ƒæ•°æ® ï¼šå¯ä»¥å¤ç”¨åŸå§‹æ¨¡å‹çš„è®­ç»ƒæ•°æ®ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨è‡ªè’¸é¦æ–¹æ³•ç”Ÿæˆè®­ç»ƒæ•°æ®ã€‚</li>
<li>è®­ç»ƒ ï¼šæ ¹æ®é€‰æ‹©çš„ç­–ç•¥å’Œæ•°æ®ï¼Œè®­ç»ƒ MEDUSA è§£ç å¤´æˆ–åŒæ—¶å¾®è°ƒ LLMã€‚</li>
</ul>
<p>è®­ç»ƒå…·ä½“ä»£ç å¦‚ä¸‹ã€‚é¦–å…ˆéœ€è¦è®­ç»ƒå‡ ä¸ªæ–°å¢çš„å¤´ï¼Œä¸åŒçš„å¤´é¢„æµ‹çš„labelçš„åç§»é‡ä¸åŒï¼Œæ‰€ä»¥å¯ä»¥ç»„è£…æ¯ä¸ªå¤´çš„topkä½œä¸ºå€™é€‰ã€‚</p>
<pre><code class="language-python"># Customized for training Medusa heads
class CustomizedTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        """
        Compute the training loss for the model.

        Args:
            model (torch.nn.Module): The model for which to compute the loss.
            inputs (dict): The input data, including input IDs, attention mask, and labels.
            return_outputs (bool): Whether to return model outputs along with the loss.

        Returns:
            Union[float, Tuple[float, torch.Tensor]]: The computed loss, optionally with model outputs.
        """
        # DDP will give us model.module
        if hasattr(model, "module"):
            medusa = model.module.medusa
        else:
            medusa = model.medusa

        logits = model(
            input_ids=inputs["input_ids"], attention_mask=inputs["attention_mask"]
        )
        labels = inputs["labels"]
        # Shift so that tokens &lt; n predict n
        loss = 0
        loss_fct = CrossEntropyLoss()
        log = {}
        for i in range(medusa):
            medusa_logits = logits[i, :, : -(2 + i)].contiguous()
            # å¸¸è§„çš„æ ‡ç­¾éœ€è¦åç§»1ä¸ªä½ç½®, ç”±äºä¸è®­ç»ƒLM Headï¼Œæ‰€ä»¥åç§»2ä¸ªä½ç½®.
            medusa_labels = labels[..., 2 + i :].contiguous()
            medusa_logits = medusa_logits.view(-1, logits.shape[-1])
            medusa_labels = medusa_labels.view(-1)
            medusa_labels = medusa_labels.to(medusa_logits.device)
            loss_i = loss_fct(medusa_logits, medusa_labels)
            loss += loss_i
            not_ignore = medusa_labels.ne(IGNORE_TOKEN_ID)
            medusa_labels = medusa_labels[not_ignore]

            # Add top-k accuracy
            for k in range(1, 2):
                _, topk = medusa_logits.topk(k, dim=-1)
                topk = topk[not_ignore]
                correct = topk.eq(medusa_labels.unsqueeze(-1)).any(-1)

        return (loss, logits) if return_outputs else loss
</code></pre>
<h2 id="0x05-decoding">0x05 Decoding</h2>
<h3 id="51-ç¤ºä¾‹">5.1 ç¤ºä¾‹</h3>
<p>å®˜æ–¹githubæºç ç»™å‡ºäº†å‰å‘ä¼ æ’­ä»£ç å¦‚ä¸‹ã€‚</p>
<pre><code class="language-python">@contextmanager
def timed(wall_times, key):
    start = time.time()
    torch.cuda.synchronize()
    yield
    torch.cuda.synchronize()
    end = time.time()
    elapsed_time = end - start
    wall_times[key].append(elapsed_time)

def medusa_forward(input_ids, model, tokenizer, medusa_choices, temperature, posterior_threshold, posterior_alpha, max_steps = 512):
    wall_times = {'medusa': [], 'tree': [], 'posterior': [], 'update': [], 'init': []}
    
    with timed(wall_times, 'init'):
        if hasattr(model, "medusa_choices") and model.medusa_choices == medusa_choices:
            # Load the cached medusa buffer
            medusa_buffers = model.medusa_buffers
        else:
            # Initialize the medusa buffer
            medusa_buffers = generate_medusa_buffers(
                medusa_choices, device=model.base_model.device
            )
        model.medusa_buffers = medusa_buffers
        model.medusa_choices = medusa_choices

        # Initialize the past key and value states
        if hasattr(model, "past_key_values"):
            past_key_values = model.past_key_values
            past_key_values_data = model.past_key_values_data
            current_length_data = model.current_length_data
            # Reset the past key and value states
            current_length_data.zero_()
        else:
            (
                past_key_values,
                past_key_values_data,
                current_length_data,
            ) = initialize_past_key_values(model.base_model)
            model.past_key_values = past_key_values
            model.past_key_values_data = past_key_values_data
            model.current_length_data = current_length_data

        input_len = input_ids.shape[1]
        reset_medusa_mode(model)
        medusa_logits, logits = initialize_medusa(
                input_ids, model, medusa_buffers["medusa_attn_mask"], past_key_values
        )
    new_token = 0

    for idx in range(max_steps): 
        with timed(wall_times, 'medusa'):
            candidates, tree_candidates = generate_candidates(
                    medusa_logits,
                    logits,
                    medusa_buffers["tree_indices"],
                    medusa_buffers["retrieve_indices"],
                )

        with timed(wall_times, 'tree'):
            medusa_logits, logits, outputs = tree_decoding(
                    model,
                    tree_candidates,
                    past_key_values,
                    medusa_buffers["medusa_position_ids"],
                    input_ids,
                    medusa_buffers["retrieve_indices"],
                )

        with timed(wall_times, 'posterior'):
            best_candidate, accept_length = evaluate_posterior(
                    logits, candidates, temperature, posterior_threshold, posterior_alpha
                )
        
        with timed(wall_times, 'update'):
            input_ids, logits, medusa_logits, new_token = update_inference_inputs(
                    input_ids,
                    candidates,
                    best_candidate,
                    accept_length,
                    medusa_buffers["retrieve_indices"],
                    outputs,
                    logits,
                    medusa_logits,
                    new_token,
                    past_key_values_data,
                    current_length_data,
                )

        if tokenizer.eos_token_id in input_ids[0, input_len:].tolist():
            break

    return input_ids, new_token, idx, wall_times
</code></pre>
<p>è°ƒç”¨æ–¹æ³•æ ·ä¾‹å¦‚ä¸‹ã€‚</p>
<pre><code class="language-python">import os
os.environ["CUDA_VISIBLE_DEVICES"] = "3" # define GPU id, remove if you want to use all GPUs available
import torch
from tqdm import tqdm
import time
from contextlib import contextmanager
import numpy as np
from medusa.model.modeling_llama_kv import LlamaForCausalLM as KVLlamaForCausalLM
from medusa.model.medusa_model import MedusaModel
from medusa.model.kv_cache import *
from medusa.model.utils import *
from medusa.model.medusa_choices import *
import transformers
from huggingface_hub import hf_hub_download

# åŠ è½½æ¨¡å‹
model_name = 'FasterDecoding/medusa-vicuna-7b-v1.3'
model = MedusaModel.from_pretrained(
    model_name,
    medusa_num_heads = 4,
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True,
    device_map="auto"
)
tokenizer = model.get_tokenizer()

medusa_choices = mc_sim_7b_63

# è®¾ç½®æ¨ç†å‚æ•°
temperature = 0.
posterior_threshold = 0.09
posterior_alpha = 0.3

# è®¾ç½®prompt
prompt = "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: Hi, could you share a tale about a charming llama that grows Medusa-like hair and starts its own coffee shop? ASSISTANT:"

# æ‰§è¡Œæ¨ç†
with torch.inference_mode():
    input_ids = tokenizer([prompt]).input_ids
    output_ids, new_token, idx, wall_time = medusa_forward(
                    torch.as_tensor(input_ids).cuda(),
                    model,
                    tokenizer,
                    medusa_choices,
                    temperature,
                    posterior_threshold,
                    posterior_alpha,
                )
    output_ids = output_ids[0][len(input_ids[0]) :]
    print("Output length:", output_ids.size(-1))
    print("Compression ratio:", new_token / idx)
    
# è§£ç 
output = tokenizer.decode(
                    output_ids,
                    spaces_between_special_tokens=False,
                )
print(output)
</code></pre>
<h3 id="52-è®¡ç®—å’Œç©ºé—´å¤æ‚åº¦">5.2 è®¡ç®—å’Œç©ºé—´å¤æ‚åº¦</h3>
<p>ä¸‹å›¾ç»™å‡ºäº†prefillï¼Œdecodingã€MEDUSA decodingé˜¶æ®µçš„è®¡ç®—å’Œç©ºé—´å¤æ‚åº¦ã€‚</p>
<ul>
<li>bæ˜¯batch sizeã€‚</li>
<li>sæ˜¯åºåˆ—é•¿åº¦ã€‚</li>
<li>hæ˜¯hidden dimensionã€‚</li>
<li>iæ˜¯intermediate dimensionã€‚</li>
<li>næ˜¯æ³¨æ„åŠ›å¤´ä¸ªæ•°ã€‚</li>
<li>dæ˜¯å¤´ç»´åº¦ã€‚</li>
<li>qæ˜¯MEDUSAçš„å€™é€‰é•¿åº¦ã€‚</li>
</ul>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250424214805005-1513046543.jpg" alt="" loading="lazy"></p>
<p>å¦å¤–ï¼Œä¸‹å›¾ç»™å‡ºäº†Medusa çš„æ“ä½œæµç¨‹ã€‚å½“æ²¡æœ‰ç®—å­èåˆæˆ–è€…Tilingç­–ç•¥æ—¶ï¼Œ<span class="math inline">\(QK^âŠ¤\)</span>ï¼ŒDCM(Dense Causal Mask)ï¼ŒSoftmaxéƒ½ä¼šå¯¼è‡´æ˜¾å­˜å’Œç‰‡ä¸Šç¼“å­˜ä¹‹é—´å¤§é‡çš„IOæ“ä½œã€‚</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250424214750668-507947274.jpg" alt="" loading="lazy"></p>
<h2 id="0xff-å‚è€ƒ">0xFF å‚è€ƒ</h2>
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2305.09781" target="_blank" rel="noopener nofollow">SpecInfer: Accelerating Generative Large Language Model Serving with Speculative Inference and Token Tree Verification</a></p>
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2401.10774" target="_blank" rel="noopener nofollow">Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads</a></p>
<p><a href="https://mp.weixin.qq.com/s?__biz=Mzk0ODU3MjcxNA==&amp;mid=2247483892&amp;idx=1&amp;sn=8ba8df6e8a80405e5e3e61b24dcbb79f&amp;chksm=c364c6b1f4134fa7acbf96c1fd4d952572c5c2887957b545ac0bbd30ee26f4dac2518412e271&amp;scene=178&amp;cur_album_id=3098338677899952131#rd" target="_blank" rel="noopener nofollow">LLM æŠ•æœºè§£ç  &amp; ç¾æœè(Medusa)å®ç°</a>  AIé—²è°ˆ</p>
<p><a href="https://zhuanlan.zhihu.com/p/686000524" target="_blank" rel="noopener nofollow">ã€æ‰‹æ’•LLM-Medusaã€‘å¹¶è¡Œè§£ç èŒƒå¼: ç¾æœèé©¾åˆ°, é€šé€šé—ªå¼€ï¼ï¼</a>  <a href="https://www.zhihu.com/people/aigc-69" target="_blank" rel="noopener nofollow">å°å†¬ç“œAIGC</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/651359908" target="_blank" rel="noopener nofollow">æ–¹ä½³ç‘ï¼šå¤§æ¨¡å‹æ¨ç†å¦™æ‹›â€”æŠ•æœºé‡‡æ ·ï¼ˆSpeculative Decodingï¼‰</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/701465989" target="_blank" rel="noopener nofollow">[Transformer 101ç³»åˆ—] æ·±å…¥LLMæŠ•æœºé‡‡æ ·(ä¸Š)</a>  <a href="https://www.zhihu.com/people/aaronxic" target="_blank" rel="noopener nofollow">aaronxic</a></p>
<p><a href="https://github.com/FasterDecoding/Medusa/blob/main/notebooks/medusa_introduction.ipynb" target="_blank" rel="noopener nofollow">https://github.com/FasterDecoding/Medusa/blob/main/notebooks/medusa_introduction.ipynb</a></p>
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2401.10774" target="_blank" rel="noopener nofollow">Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads</a>, Jan 2024, Princeton University. Proceedings of the ICML 2024.</p>
<p>[2401.10774] Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads</p>
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2401.10774" target="_blank" rel="noopener nofollow">Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads</a></p>
<p><a href="https://github.com/FasterDecoding/Medusa" target="_blank" rel="noopener nofollow">GitHub - FasterDecoding/Medusa: Medusa: Simple Framework for Accelerating LLM Generation with Multiple Decoding Heads</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/658544279" target="_blank" rel="noopener nofollow">LLMæ¨ç†åŠ é€Ÿä¹‹Medusaï¼šBlockwise Parallel Decodingçš„ç»§æ‰¿ä¸å‘å±•</a>  <a href="https://www.zhihu.com/people/feifeibear" target="_blank" rel="noopener nofollow">æ–¹ä½³ç‘</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/658298728" target="_blank" rel="noopener nofollow">æ–¹ä½³ç‘ï¼šLLMæ¨ç†åŠ é€Ÿçš„æ–‡è‰ºå¤å…´ï¼šNoam Shazeerå’ŒBlockwise Parallel Decoding</a>?</p>
<p><a href="https://mp.weixin.qq.com/s/PyAKiFzbQNq6w7HmaTnSEw" target="_blank" rel="noopener nofollow">ä¸‡å­—ç»¼è¿° 10+ ç§ LLM æŠ•æœºé‡‡æ ·æ¨ç†åŠ é€Ÿæ–¹æ¡ˆ</a>  AIé—²è°ˆ</p>
<p>[2401.07851] Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding</p>
<p><a href="https://zhuanlan.zhihu.com/p/675406771" target="_blank" rel="noopener nofollow">é€Ÿè§ˆMedusaä¸LookaheadæŠ•æœºæ¨ç†</a>  <a href="https://www.zhihu.com/people/zhanzy178" target="_blank" rel="noopener nofollow">æ˜¯é˜¿æ²…å•Š</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/682033950" target="_blank" rel="noopener nofollow">å¼€æºè¿›å±• | Medusa: ä½¿ç”¨å¤šå¤´è§£ç ï¼Œå°†å¤§æ¨¡å‹æ¨ç†é€Ÿåº¦æå‡2å€ä»¥ä¸Š</a>  <a href="https://www.zhihu.com/people/august-53-55" target="_blank" rel="noopener nofollow">æ´ªæ´—è±¡</a></p>
<p>arXiv:1811.03115: Berkey, Google Brain, Blockwise Parallel Decoding for Deep Autoregressive Models.</p>
<p>arXiv:2211.17192: Google Research, Fast Inference from Transformers via Speculative Decoding</p>
<p>arXiv:2202.00666: ETH ZuÌˆrichã€University of Cambridgeï¼ŒLocally Typical Sampling</p>
<p>[4] arXiv:2106.05234: Dalian University of Technologyã€Princeton Universityã€Peking Universityã€Microsoft Research Asiaï¼ŒDo Transformers Really Perform Bad for Graph Representation?</p>
<p><a href="https://zhuanlan.zhihu.com/p/701417546" target="_blank" rel="noopener nofollow">3ä¸‡å­—è¯¦ç»†è§£ææ¸…åå¤§å­¦æœ€æ–°ç»¼è¿°å·¥ä½œï¼šå¤§æ¨¡å‹é«˜æ•ˆæ¨ç†ç»¼è¿°</a>  <a href="https://www.zhihu.com/people/zenRRan" target="_blank" rel="noopener nofollow">zenRRan</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/703461293" target="_blank" rel="noopener nofollow">å¤§æ¨¡å‹æ¨ç†åŠ é€Ÿ-MEDUSA</a>  <a href="https://www.zhihu.com/people/us4ever" target="_blank" rel="noopener nofollow">Linsight</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/655809033" target="_blank" rel="noopener nofollow">LLMæ¨ç†åŠ é€Ÿ-Medusa</a>  uuuuu</p>
<p><a href="https://zhuanlan.zhihu.com/p/686000524" target="_blank" rel="noopener nofollow">ã€æ‰‹æ’•LLM-Medusaã€‘å¹¶è¡Œè§£ç èŒƒå¼: ç¾æœèé©¾åˆ°, é€šé€šé—ªå¼€ï¼ï¼</a>   <a href="https://www.zhihu.com/people/aigc-69" target="_blank" rel="noopener nofollow">å°å†¬ç“œAIGC</a></p>
<p><a href="https://mp.weixin.qq.com/s?__biz=Mzk0ODU3MjcxNA==&amp;mid=2247483742&amp;idx=1&amp;sn=47e560f7807d9a62075abb6d15183593&amp;chksm=c364c61bf4134f0d13ae807f9d44969e2421e391bfa75280b9a6331bf5190ae8cbda9b898708&amp;scene=21#wechat_redirect" target="_blank" rel="noopener nofollow">Blockwise Parallel Decoding è®ºæ–‡è§£è¯»</a> AIé—²è°ˆ</p>
<p><a href="https://mp.weixin.qq.com/s?__biz=Mzk0ODU3MjcxNA==&amp;mid=2247483892&amp;idx=1&amp;sn=8ba8df6e8a80405e5e3e61b24dcbb79f&amp;chksm=c364c6b1f4134fa7acbf96c1fd4d952572c5c2887957b545ac0bbd30ee26f4dac2518412e271&amp;scene=21#wechat_redirect" target="_blank" rel="noopener nofollow">LLM æŠ•æœºè§£ç  &amp; ç¾æœè(Medusa)å®ç°</a>  AIé—²è°ˆ</p>
<p><a href="https://sites.google.com/view/medusa-llm" target="_blank" rel="noopener nofollow">https://sites.google.com/view/medusa-llm</a></p>
<p><a href="https://github.com/FasterDecoding/Medusa" target="_blank" rel="noopener nofollow">https://github.com/FasterDecoding/Medusa</a></p>
<p><a href="https://mp.weixin.qq.com/s?__biz=Mzk0ODU3MjcxNA==&amp;mid=2247486819&amp;idx=1&amp;sn=5b5ec1009fbd32ada8cc61cda9a6abdc&amp;chksm=c364ca26f4134330c5f283d2ebd029257900c9f1bd680181420ee44a05f0aa0fcb39147fccb9&amp;scene=178&amp;cur_album_id=3209901770408230914#rd" target="_blank" rel="noopener nofollow">ç™¾å· Cloverï¼šä¼˜äº Medusa çš„æŠ•æœºé‡‡æ ·</a>   AIé—²è°ˆ</p>
<p>[2405.00263] Clover: Regressive Lightweight Speculative Decoding with Sequential Knowledge</p>
<p><a href="https://zhuanlan.zhihu.com/p/691883733" target="_blank" rel="noopener nofollow">Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding</a>  <a href="https://www.zhihu.com/people/detached_sextant" target="_blank" rel="noopener nofollow">ç°ç³å…­åˆ†ä»ª</a></p>
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2402.05109" target="_blank" rel="noopener nofollow">Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/15978788714" target="_blank" rel="noopener nofollow">ã€è®ºæ–‡è§£è¯»ã€‘Medusaï¼šä½¿ç”¨å¤šä¸ªè§£ç å¤´å¹¶è¡Œé¢„æµ‹åç»­å¤šä¸ªtoken</a>  tomsheep</p>
<p><a href="https://zhuanlan.zhihu.com/p/1894866252989711570" target="_blank" rel="noopener nofollow">LLMæ¨ç†åŠ é€Ÿ(ä¸‰): MedusaæŠ•æœºé‡‡æ ·</a>  <a href="https://www.zhihu.com/people/yue-da-84-55" target="_blank" rel="noopener nofollow">æ‚¦å¤§</a></p>

</div>
<div class="clear"></div>
<div id="blog_post_info_block" role="contentinfo">
    <div id="blog_post_info"></div>
    <div class="clear"></div>
    <div id="post_next_prev"></div>
</div>
            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="7.720979505303241" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-05-01 23:41">2025-04-28 20:42</span>&nbsp;
<a href="https://www.cnblogs.com/rossiXYZ">ç½—è¥¿çš„æ€è€ƒ</a>&nbsp;
é˜…è¯»(<span id="post_view_count">219</span>)&nbsp;
è¯„è®º(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18845475);return false;">æ”¶è—</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18845475', targetLink: 'https://www.cnblogs.com/rossiXYZ/p/18845475', title: 'æ¢ç§˜Transformerç³»åˆ—ä¹‹ï¼ˆ31ï¼‰--- Medusa' })">ä¸¾æŠ¥</a>
</div>
        </div>
        <script>
    var cb_entryId = 18845475, cb_entryCreatedDate = '2025-04-28 20:42', cb_postType = 1, cb_postTitle = 'æ¢ç§˜Transformerç³»åˆ—ä¹‹ï¼ˆ31ï¼‰--- Medusa';
    var allowComments = true, cb_blogId = 556264, cb_blogApp = 'rossiXYZ', cb_blogUserGuid = '3d1961d5-3b13-4975-9d25-08d753a9a8fd';
    mermaidRender.render()
    markdown_highlight()
    zoomManager.apply("#cnblogs_post_body img:not(.code_img_closed):not(.code_img_opened)");    
</script>
        <a id="!comments"></a>
<div id="blog-comments-placeholder"></div>
<div id="comment_form" class="commentform">
    <a name="commentform"></a>
    <div id="divCommentShow"></div>
    <div id="comment_nav"> 
        <div class="comment-nav-right">
            <span id="span_refresh_tips"></span><a href="#" onclick="return RefreshPage();">åˆ·æ–°é¡µé¢</a><a href="#top">è¿”å›é¡¶éƒ¨</a>
        </div>
    </div>
    <div id="comment_form_container"></div>
    <div class="ad_text_commentbox" id="ad_text_under_commentbox"></div>
        <div id="cnblogs_ch"></div>
    <div id="opt_under_post"></div>
        <div id="blog_c1" class="under-post-card">
            <a href="https://www.trae.com.cn/?utm_source=advertising&amp;utm_medium=cnblogs_ug_cpa&amp;utm_term=hw_trae_cnblogs" rel="nofollow" target="_blank" onclick="countCreativeClicks('C1-å­—èŠ‚-trae')">
                <img src="https://img2024.cnblogs.com/blog/35695/202504/35695-20250422130943631-261509646.jpg" onload="countCreativeImpressions('C1-å­—èŠ‚-trae')" alt="" />
                <span id="c1_impression" style="display:none"></span>
            </a>
        </div>
    <div id="under_post_card1"></div>
    <div id="under_post_card2"></div>
    <div id="HistoryToday" class="under-post-card"></div>
    <script type="text/javascript">
        var commentManager = new blogCommentManager();
        commentManager.renderComments(0);
        fixPostBody();
        window.footnoteTipManager.generateFootnoteTips();

            window.tocManager.displayDisableTocTips = false;
            window.tocManager.generateToc();
            
                setTimeout(function() { countViews(cb_blogId, cb_entryId); }, 50);
            
            deliverT2();
            deliverC1C2();
            loadNewsAndKb();
            
                LoadPostCategoriesTags(cb_blogId, cb_entryId);
            
            LoadPostInfoBlock(cb_blogId, cb_entryId, cb_blogApp, cb_blogUserGuid);
            GetPrevNextPost(cb_entryId, cb_blogId, cb_entryCreatedDate, cb_postType);
            loadOptUnderPost();
            GetHistoryToday(cb_blogId, cb_blogApp, cb_entryCreatedDate);
                </script>
</div>

    </div>
</div>
            </div>
        </div>

        <div id="sideBar">
            <div id="sideBarMain">
                <div id="sidebar_news" class="newsItem">
    
<h3 class="catListTitle">å…¬å‘Š</h3>
<div id="blog-news" class="sidebar-news">
    <div id="sidebar_news_container">
    </div>
</div>
<script>loadBlogNews();</script> 
</div>
<div id="sidebar_c3"></div>
                <div id="calendar"><div id="blog-calendar" style="display:none"></div></div>                
                <script>loadBlogDefaultCalendar();</script>
                <div id="leftcontentcontainer">
                    <!-- begin:SingleColumn -->
                    <div id="blog-sidecolumn"></div>
                    <script>loadBlogSideColumn();</script>
                    <!-- end:  SingleColumn -->
                </div>
            </div>
        </div>
        <div class="clear"></div>
    </div>
    <div class="clear"></div>
    <div id="footer">
        <!--done-->
Copyright &copy; 2025 ç½—è¥¿çš„æ€è€ƒ
<br /><span id="poweredby">Powered by .NET 9.0 on Kubernetes</span>

    </div>
</div>


    

    <input type="hidden" id="antiforgery_token" value="CfDJ8Ct_7-Gh-gZNte6RB_khjDqDCN-FiEy-CUQ5qwdXy-QmV_p7U8bdF8w9l_GCJNPq61KxHMcnkYVuaR5uHPaiTcNc1hp1bcC62emjFQyupm_Txh0TyO5f9W-sJromfSI4hBnNo1IJNR8W-k9qILVACXc" />
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-M95P3TTWJZ"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-M95P3TTWJZ');
</script>
<script defer src="https://hm.baidu.com/hm.js?866c9be12d4a814454792b1fd0fed295"></script>
</body>
</html>
