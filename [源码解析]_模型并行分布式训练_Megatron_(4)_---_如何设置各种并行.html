<!DOCTYPE html>
<html lang="zh-cn">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="referrer" content="origin-when-cross-origin" />
    <meta name="keywords" content="001_æœºå™¨å­¦ä¹ ,006_æ·±åº¦å­¦ä¹ ,011_åˆ†å¸ƒå¼æœºå™¨å­¦ä¹ " />
    <meta name="description" content="NVIDIA Megatron æ˜¯ä¸€ä¸ªåŸºäº PyTorch çš„åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶ï¼Œç”¨æ¥è®­ç»ƒè¶…å¤§Transformerè¯­è¨€æ¨¡å‹ï¼Œå…¶é€šè¿‡ç»¼åˆåº”ç”¨äº†æ•°æ®å¹¶è¡Œï¼ŒTensorå¹¶è¡Œå’ŒPipelineå¹¶è¡Œæ¥å¤ç° GPT3ï¼Œå€¼å¾—æˆ‘ä»¬æ·±å…¥åˆ†æå…¶èƒŒåæœºç†ã€‚" />
    <meta property="og:description" content="NVIDIA Megatron æ˜¯ä¸€ä¸ªåŸºäº PyTorch çš„åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶ï¼Œç”¨æ¥è®­ç»ƒè¶…å¤§Transformerè¯­è¨€æ¨¡å‹ï¼Œå…¶é€šè¿‡ç»¼åˆåº”ç”¨äº†æ•°æ®å¹¶è¡Œï¼ŒTensorå¹¶è¡Œå’ŒPipelineå¹¶è¡Œæ¥å¤ç° GPT3ï¼Œå€¼å¾—æˆ‘ä»¬æ·±å…¥åˆ†æå…¶èƒŒåæœºç†ã€‚" />
    <meta http-equiv="Cache-Control" content="no-transform" />
    <meta http-equiv="Cache-Control" content="no-siteapp" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <title>[æºç è§£æ] æ¨¡å‹å¹¶è¡Œåˆ†å¸ƒå¼è®­ç»ƒ Megatron (4) --- å¦‚ä½•è®¾ç½®å„ç§å¹¶è¡Œ - ç½—è¥¿çš„æ€è€ƒ - åšå®¢å›­</title>
    <link rel="icon" id="favicon" href="https://assets.cnblogs.com/favicon_v3_2.ico" type="image/x-icon" />
    <link rel="canonical" href="https://www.cnblogs.com/rossiXYZ/p/15876714.html" />
    
    <link rel="stylesheet" href="/css/blog-common.min.css?v=3DArmf-Or-4qxFZkl3OdynS2Am4I6_pcIbQbRZRdGaM" />
    

    <link id="MainCss" rel="stylesheet" href="/skins/lessismoreright/bundle-lessismoreright.min.css?v=O5zHESxCF0tzyVg01nX06fLeohvC5JYxsLWE4NmQOMg" />
        <link id="highlighter-theme-cnblogs" type="text/css" rel="stylesheet" href="/css/hljs/cnblogs.css?v=5J1NDtbnnIr2Rc2SdhEMlMxD4l9Eydj88B31E7_NhS4" />
    
    
    <link id="mobile-style" media="only screen and (max-width: 767px)" type="text/css" rel="stylesheet" href="/skins/lessismoreright/bundle-lessismoreright-mobile.min.css?v=Uw1Hg7i9RFPazLAd0cWltL-cniUkUgHHPLh7ZV9ZL9o" />
    
    <link type="application/rss+xml" rel="alternate" href="https://www.cnblogs.com/rossiXYZ/rss" />
    <link type="application/rsd+xml" rel="EditURI" href="https://www.cnblogs.com/rossiXYZ/rsd.xml" />
    <link type="application/wlwmanifest+xml" rel="wlwmanifest" href="https://www.cnblogs.com/rossiXYZ/wlwmanifest.xml" />
    
    <script type="application/ld&#x2B;json">
    {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "@id": "https://www.cnblogs.com/rossiXYZ/p/15876714.html",
      "headline": "[æºç è§£æ] æ¨¡å‹å¹¶è¡Œåˆ†å¸ƒå¼è®­ç»ƒ Megatron (4) --- å¦‚ä½•è®¾ç½®å„ç§å¹¶è¡Œ",
      "description": "[æºç è§£æ] æ¨¡å‹å¹¶è¡Œåˆ†å¸ƒå¼è®­ç»ƒ Megatron (4) å¦‚ä½•è®¾ç½®å„ç§å¹¶è¡Œ 0x00 æ‘˜è¦ NVIDIA Megatron æ˜¯ä¸€ä¸ªåŸºäº PyTorch çš„åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶ï¼Œç”¨æ¥è®­ç»ƒè¶…å¤§Transformerè¯­è¨€æ¨¡å‹ï¼Œå…¶é€šè¿‡ç»¼åˆåº”ç”¨äº†æ•°æ®å¹¶è¡Œï¼ŒTensorå¹¶è¡Œå’ŒPipelineå¹¶è¡Œæ¥å¤ç° GPT3ï¼Œå€¼",
      "image": [
        
      ],
      "author": {
        "@type": "Person",
        "@id": "https://www.cnblogs.com/rossiXYZ/",
        "name": "ç½—è¥¿çš„æ€è€ƒ",
        "url": "https://www.cnblogs.com/rossiXYZ/"
      },
      "publisher": {
        "@type": "Organization",
        "@id": "https://www.cnblogs.com/",
        "name": "åšå®¢å›­",
        "url": "https://www.cnblogs.com/"
      },
      "datePublished": "2022-02-10T18:42:00.0000000&#x2B;08:00",
      "dateModified": "2022-02-10T18:42:00.0000000&#x2B;08:00",
      "wordCount": "37422",
      "isPartOf": {
        "@type": "Blog",
        "@id": "https://www.cnblogs.com/rossiXYZ/",
        "name": "ç½—è¥¿çš„æ€è€ƒ",
        "publisher": {
          "@type": "Organization",
          "@id": "https://www.cnblogs.com/",
          "name": "åšå®¢å›­"
        }
      }
    }
    </script>

    <script>
        var currentBlogId = 556264;
        var currentBlogApp = 'rossiXYZ';
        var isLogined = false;
        var isBlogOwner = false;
        var skinName = 'LessIsMoreRight';
        var visitorUserId = '';
        var hasCustomScript = false;
        window.cb_enable_mathjax = true;
        window.mathEngine = 0;
        window.codeHighlightEngine = 1;
        window.enableCodeLineNumber = false;
        window.codeHighlightTheme = 'cnblogs';
        window.darkModeCodeHighlightTheme = 'vs2015';
        window.isDarkCodeHighlightTheme = false;
        window.isDarkModeCodeHighlightThemeDark = true;
        window.isDisableCodeHighlighter = false;
        window.enableCodeThemeTypeFollowSystem = false;
        window.enableMacStyleCodeBlock = false;
    </script>
        <script>
            window.currentPostId = 15876714;
            window.currentPostDateAdded = '2022-02-10 18:42';
        </script>
    <script src="https://assets.cnblogs.com/scripts/jquery-3.3.1.min.js"></script>
    <script src="https://cdn-www.cnblogs.com/js/blog-common.min.js?v=wZ-j9lgqsnaTqSE7AdWd3J3j9ENiZHPW0sel6vKY_Mo"></script>
    
</head>
<body class="skin-lessismoreright has-navbar mathjax2">
    <a name="top"></a>
        <div id="imagebar" class="imagebar-mobile imagebar-text-mobile formobile">
                <a href="https://www.doubao.com?channel=cnblogs&amp;source=hw_db_cnblogs&amp;type=lunt&amp;theme=bianc" onclick="countCreativeClicks('M2-å­—èŠ‚-è±†åŒ…')" rel="nofollow">
                    <img src="https://img2024.cnblogs.com/blog/35695/202412/35695-20241201073014811-1847930772.jpg" alt="" onload="countCreativeImpressionsOnMobile('M2-å­—èŠ‚-è±†åŒ…')" />
                    <span id="m2_impression" style="display:none"></span>
                </a>
        </div>
    <div id="top_nav" class="navbar forpc">
        <nav id="nav_main" class="navbar-main">
            <ul id="nav_left" class="navbar-list navbar-left">
                <li class="navbar-branding">                    
                    <a href="https://www.cnblogs.com/" title="å¼€å‘è€…çš„ç½‘ä¸Šå®¶å›­" role="banner">
                        <img src="//assets.cnblogs.com/logo.svg" alt="åšå®¢å›­logo" />
                    </a>
                </li>               
                <li><a href="https://cnblogs.vip/">ä¼šå‘˜</a></li>
                <li><a href="https://cnblogs.vip/store">å‘¨è¾¹</a></li>
                <li><a href="https://news.cnblogs.com/" onclick="countClicks('nav', 'skin-navbar-news')">æ–°é—»</a></li>
                <li><a href="https://q.cnblogs.com/" onclick="countClicks('nav', 'skin-navbar-q')">åšé—®</a></li>
                <li><a href="https://ing.cnblogs.com/" onclick="countClicks('nav', 'skin-navbar-ing')">é—ªå­˜</a></li>
                <li><a href="https://www.cnblogs.com/cmt/p/18341478">èµåŠ©å•†</a></li>
                <li><a href="https://chat2db-ai.com/" target="_blank" onclick="countClicks('nav', 'skin-navbar-chat2db')">Chat2DB</a></li>
            </ul>
            <ul id="nav_right" class="navbar-list navbar-right">
                <li>
                    <form id="zzk_search" class="navbar-search dropdown" action="https://zzk.cnblogs.com/s" method="get" role="search">
                        <input name="w" id="zzk_search_input" placeholder="ä»£ç æ”¹å˜ä¸–ç•Œ" type="search" tabindex="3" autocomplete="off" />
                        <button id="zzk_search_button" onclick="window.navbarSearchManager.triggerActiveOption()">
                            <img id="search_icon" class="focus-hidden" src="//assets.cnblogs.com/icons/search.svg" alt="æœç´¢" />
                            <img class="hidden focus-visible" src="//assets.cnblogs.com/icons/enter.svg" alt="æœç´¢" />
                        </button>
                        <ul id="navbar_search_options" class="dropdown-menu quick-search-menu">
                            <li tabindex="0" class="active" onclick="zzkSearch(event, document.getElementById('zzk_search_input').value)">
                                <div class="keyword-wrapper">
                                    <img src="//assets.cnblogs.com/icons/search.svg" alt="æœç´¢" />
                                    <div class="keyword"></div>
                                </div>
                                <span class="search-area">æ‰€æœ‰åšå®¢</span>
                            </li>
                                    <li tabindex="1" onclick="zzkBlogSearch(event, 'rossiXYZ', document.getElementById('zzk_search_input').value)">
                                        <div class="keyword-wrapper">
                                            <img src="//assets.cnblogs.com/icons/search.svg" alt="æœç´¢" />
                                            <div class="keyword"></div>
                                        </div>
                                        <span class="search-area">å½“å‰åšå®¢</span>
                                    </li>
                        </ul>
                    </form>
                </li>
                <li id="navbar_login_status" class="navbar-list">
                    <a class="navbar-user-info navbar-blog" href="https://i.cnblogs.com/EditPosts.aspx?opt=1" alt="å†™éšç¬”" title="å†™éšç¬”">
                        <img id="new_post_icon" class="navbar-icon" src="//assets.cnblogs.com/icons/newpost.svg" alt="å†™éšç¬”" />
                    </a>
                    <a id="navblog-myblog-icon" class="navbar-user-info navbar-blog" href="https://passport.cnblogs.com/GetBlogApplyStatus.aspx" alt="æˆ‘çš„åšå®¢" title="æˆ‘çš„åšå®¢">
                        <img id="myblog_icon" class="navbar-icon" src="//assets.cnblogs.com/icons/myblog.svg" alt="æˆ‘çš„åšå®¢" />
                    </a>
                    <a class="navbar-user-info navbar-message navbar-icon-wrapper" href="https://msg.cnblogs.com/" alt="çŸ­æ¶ˆæ¯" title="çŸ­æ¶ˆæ¯">
                        <img id="msg_icon" class="navbar-icon" src="//assets.cnblogs.com/icons/message.svg" alt="çŸ­æ¶ˆæ¯" />
                        <span id="msg_count" style="display: none"></span>
                    </a>
                    <a id="navbar_lite_mode_indicator" data-current-page="blog" style="display: none" href="javascript:void(0)" alt="ç®€æ´æ¨¡å¼" title="ç®€æ´æ¨¡å¼å¯ç”¨ï¼Œæ‚¨åœ¨è®¿é—®ä»–äººåšå®¢æ—¶ä¼šä½¿ç”¨ç®€æ´æ¬¾çš®è‚¤å±•ç¤º">
                        <img class="navbar-icon" src="//assets.cnblogs.com/icons/lite-mode-on.svg" alt="ç®€æ´æ¨¡å¼" />
                    </a>
                    <div id="user_info" class="navbar-user-info dropdown">
                        <a class="dropdown-button" href="https://home.cnblogs.com/">
                            <img id="user_icon" class="navbar-avatar" src="//assets.cnblogs.com/icons/avatar-default.svg" alt="ç”¨æˆ·å¤´åƒ" />
                        </a>
                        <div class="dropdown-menu">
                            <a id="navblog-myblog-text" href="https://passport.cnblogs.com/GetBlogApplyStatus.aspx">æˆ‘çš„åšå®¢</a>
                            <a href="https://home.cnblogs.com/">æˆ‘çš„å›­å­</a>
                            <a href="https://account.cnblogs.com/settings/account">è´¦å·è®¾ç½®</a>
                            <a href="https://vip.cnblogs.com/my">ä¼šå‘˜ä¸­å¿ƒ</a>
                            <a href="javascript:void(0)" id="navbar_lite_mode_toggle" title="ç®€æ´æ¨¡å¼ä¼šä½¿ç”¨ç®€æ´æ¬¾çš®è‚¤æ˜¾ç¤ºæ‰€æœ‰åšå®¢">
    ç®€æ´æ¨¡å¼ <span id="navbar_lite_mode_spinner" class="hide">...</span>
</a>

                            <a href="javascript:void(0)" onclick="account.logout();">é€€å‡ºç™»å½•</a>
                        </div>
                    </div>
                    <a class="navbar-anonymous" href="https://account.cnblogs.com/signup">æ³¨å†Œ</a>
                    <a class="navbar-anonymous" href="javascript:void(0);" onclick="account.login()">ç™»å½•</a>
                </li>
            </ul>
        </nav>
    </div>

    <div id="page_begin_html">
        

    </div>

    <div id="home">
    <div id="header">
        <div id="blogTitle">
            <div class="title"><a id="Header1_HeaderTitle" class="headermaintitle HeaderMainTitle" href="https://www.cnblogs.com/rossiXYZ">ç½—è¥¿çš„æ€è€ƒ</a>
</div>
<div class="subtitle">ä¸€æ‰‹ä¼¸å‘æŠ€æœ¯ï¼Œä¸€æ‰‹ä¼¸å‘ç”Ÿæ´»</div>

        </div>
        <div id="navigator">
            
<ul id="navList">
    <li id="nav_sitehome"><a id="blog_nav_sitehome" class="menu" href="https://www.cnblogs.com/">
åšå®¢å›­</a>
</li>
    <li id="nav_myhome">
<a id="blog_nav_myhome" class="menu" href="https://www.cnblogs.com/rossiXYZ/">
é¦–é¡µ</a>
</li>
    <li id="nav_newpost">

<a id="blog_nav_newpost" class="menu" href="https://i.cnblogs.com/EditPosts.aspx?opt=1">
æ–°éšç¬”</a>
</li>
    <li id="nav_contact">
<a id="blog_nav_contact" class="menu" href="https://msg.cnblogs.com/send/%E7%BD%97%E8%A5%BF%E7%9A%84%E6%80%9D%E8%80%83">
è”ç³»</a></li>
    <li id="nav_rss">
<a id="blog_nav_rss" class="menu" href="javascript:void(0)" data-rss="https://www.cnblogs.com/rossiXYZ/rss/">
è®¢é˜…</a></li>
    <li id="nav_admin">
<a id="blog_nav_admin" class="menu" href="https://i.cnblogs.com/">
ç®¡ç†</a>
</li>
</ul>

            <div class="blogStats">
                <div id="blog_stats_place_holder"><script>loadBlogStats();</script></div>
            </div>
        </div>
    </div>
    <div id="main">
        <div id="mainContent">
            <div class="forFlow">
                <div id="post_detail">
    <div id="topics">
        <div class="post">
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/rossiXYZ/p/15876714.html" title="å‘å¸ƒäº 2022-02-10 18:42">
    <span role="heading" aria-level="2">[æºç è§£æ] æ¨¡å‹å¹¶è¡Œåˆ†å¸ƒå¼è®­ç»ƒ Megatron (4) --- å¦‚ä½•è®¾ç½®å„ç§å¹¶è¡Œ</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                    <div id="cnblogs_post_description" style="display: none">
        
        NVIDIA Megatron æ˜¯ä¸€ä¸ªåŸºäº PyTorch çš„åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶ï¼Œç”¨æ¥è®­ç»ƒè¶…å¤§Transformerè¯­è¨€æ¨¡å‹ï¼Œå…¶é€šè¿‡ç»¼åˆåº”ç”¨äº†æ•°æ®å¹¶è¡Œï¼ŒTensorå¹¶è¡Œå’ŒPipelineå¹¶è¡Œæ¥å¤ç° GPT3ï¼Œå€¼å¾—æˆ‘ä»¬æ·±å…¥åˆ†æå…¶èƒŒåæœºç†ã€‚
    </div>
<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h1 id="æºç è§£æ-æ¨¡å‹å¹¶è¡Œåˆ†å¸ƒå¼è®­ç»ƒ-megatron-4-----å¦‚ä½•è®¾ç½®å„ç§å¹¶è¡Œ">[æºç è§£æ] æ¨¡å‹å¹¶è¡Œåˆ†å¸ƒå¼è®­ç»ƒ Megatron (4) --- å¦‚ä½•è®¾ç½®å„ç§å¹¶è¡Œ</h1>
<p></p><div class="toc"><div class="toc-container-header">ç›®å½•</div><ul><li><a href="#æºç è§£æ-æ¨¡å‹å¹¶è¡Œåˆ†å¸ƒå¼è®­ç»ƒ-megatron-4-----å¦‚ä½•è®¾ç½®å„ç§å¹¶è¡Œ" rel="noopener nofollow">[æºç è§£æ] æ¨¡å‹å¹¶è¡Œåˆ†å¸ƒå¼è®­ç»ƒ Megatron (4) --- å¦‚ä½•è®¾ç½®å„ç§å¹¶è¡Œ</a><ul><li><a href="#0x00-æ‘˜è¦" rel="noopener nofollow">0x00 æ‘˜è¦</a></li><li><a href="#0x01-å‰æ–‡å›é¡¾" rel="noopener nofollow">0x01 å‰æ–‡å›é¡¾</a></li><li><a href="#0x02-åˆå§‹åŒ–" rel="noopener nofollow">0x02 åˆå§‹åŒ–</a><ul><li><a href="#21-å…¨å±€å˜é‡" rel="noopener nofollow">2.1 å…¨å±€å˜é‡</a></li><li><a href="#22-åˆå§‹åŒ–ä»£ç " rel="noopener nofollow">2.2 åˆå§‹åŒ–ä»£ç </a></li></ul></li><li><a href="#0x03-åˆ‡åˆ†æ ·ä¾‹" rel="noopener nofollow">0x03 åˆ‡åˆ†æ ·ä¾‹</a><ul><li><a href="#31-æ³¨é‡Š" rel="noopener nofollow">3.1 æ³¨é‡Š</a></li><li><a href="#32-åˆ‡åˆ†æƒ…å†µ" rel="noopener nofollow">3.2 åˆ‡åˆ†æƒ…å†µ</a></li><li><a href="#33-åˆ‡åˆ†ç­–ç•¥" rel="noopener nofollow">3.3 åˆ‡åˆ†ç­–ç•¥</a></li><li><a href="#34-å®éªŒ" rel="noopener nofollow">3.4 å®éªŒ</a></li></ul></li><li><a href="#0x04-èµ·å§‹çŠ¶æ€" rel="noopener nofollow">0x04 èµ·å§‹çŠ¶æ€</a><ul><li><a href="#41-gpu-çŠ¶å†µ" rel="noopener nofollow">4.1 GPU çŠ¶å†µ</a></li><li><a href="#42-ç¬¦å·è¯´æ˜" rel="noopener nofollow">4.2 ç¬¦å·è¯´æ˜</a></li><li><a href="#43-åˆå§‹åˆ†ç»„" rel="noopener nofollow">4.3 åˆå§‹åˆ†ç»„</a></li></ul></li><li><a href="#0x05-tensor-model-parallel" rel="noopener nofollow">0x05 Tensor model-parallel</a><ul><li><a href="#51-åˆ†ç»„" rel="noopener nofollow">5.1 åˆ†ç»„</a></li><li><a href="#52-ä½¿ç”¨" rel="noopener nofollow">5.2 ä½¿ç”¨</a></li></ul></li><li><a href="#0x06-pipe-parallel" rel="noopener nofollow">0x06 Pipe-parallel</a><ul><li><a href="#61-åˆ†ç»„" rel="noopener nofollow">6.1 åˆ†ç»„</a></li><li><a href="#62-ä½¿ç”¨" rel="noopener nofollow">6.2 ä½¿ç”¨</a><ul><li><a href="#621-ä¸Šä¸‹æ¸¸rank" rel="noopener nofollow">6.2.1 ä¸Šä¸‹æ¸¸rank</a></li><li><a href="#622-world-size" rel="noopener nofollow">6.2.2 world size</a></li></ul></li></ul></li><li><a href="#0x07-data-parallel" rel="noopener nofollow">0x07 Data-parallel</a><ul><li><a href="#71-åˆ†ç»„" rel="noopener nofollow">7.1 åˆ†ç»„</a></li><li><a href="#72-å¦‚ä½•ä½¿ç”¨" rel="noopener nofollow">7.2 å¦‚ä½•ä½¿ç”¨</a></li></ul></li><li><a href="#0x08-æ¨¡å‹ç»„" rel="noopener nofollow">0x08 æ¨¡å‹ç»„</a></li><li><a href="#0x09-å¦‚ä½•æŠŠæ¨¡å‹åˆ†åˆ°gpu" rel="noopener nofollow">0x09 å¦‚ä½•æŠŠæ¨¡å‹åˆ†åˆ°GPU</a></li><li><a href="#0xff-å‚è€ƒ" rel="noopener nofollow">0xFF å‚è€ƒ</a></li></ul></li></ul></div><p></p>
<h2 id="0x00-æ‘˜è¦">0x00 æ‘˜è¦</h2>
<p>NVIDIA Megatron æ˜¯ä¸€ä¸ªåŸºäº PyTorch çš„åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶ï¼Œç”¨æ¥è®­ç»ƒè¶…å¤§Transformerè¯­è¨€æ¨¡å‹ï¼Œå…¶é€šè¿‡ç»¼åˆåº”ç”¨äº†æ•°æ®å¹¶è¡Œï¼ŒTensorå¹¶è¡Œå’ŒPipelineå¹¶è¡Œæ¥å¤ç° GPT3ï¼Œå€¼å¾—æˆ‘ä»¬æ·±å…¥åˆ†æå…¶èƒŒåæœºç†ã€‚</p>
<p>æœ¬ç³»åˆ—å¤§æ¦‚æœ‰ 5 ç¯‡æ–‡ç« ï¼Œé€šè¿‡è®ºæ–‡å’Œæºç å’Œå¤§å®¶ä¸€èµ·å­¦ä¹ ç ”ç©¶ã€‚æœ¬æ–‡å°†çœ‹çœ‹ Megatron å¦‚ä½•å¤„ç†è®¾ç½®å¹¶è¡Œã€‚</p>
<p>æœ¬ç³»åˆ—å…¶ä»–æ–‡ç« ä¸ºï¼š</p>
<p>[<a href="https://www.cnblogs.com/rossiXYZ/p/15840803.html" target="_blank">æºç è§£æ] æ¨¡å‹å¹¶è¡Œåˆ†å¸ƒå¼è®­ç»ƒMegatron (1) --- è®ºæ–‡ &amp; åŸºç¡€</a></p>
<p>[<a href="https://www.cnblogs.com/rossiXYZ/p/15868988.html" target="_blank">æºç è§£æ] æ¨¡å‹å¹¶è¡Œåˆ†å¸ƒå¼è®­ç»ƒMegatron (2) --- æ•´ä½“æ¶æ„</a></p>
<p>[<a href="https://www.cnblogs.com/rossiXYZ/p/15871062.html" target="_blank">æºç è§£æ] æ¨¡å‹å¹¶è¡Œåˆ†å¸ƒå¼è®­ç»ƒ Megatron (3) ---æ¨¡å‹å¹¶è¡Œå®ç°</a></p>
<h2 id="0x01-å‰æ–‡å›é¡¾">0x01 å‰æ–‡å›é¡¾</h2>
<p>å‰æ–‡æˆ‘ä»¬å¯¹æ¨¡å‹å¹¶è¡Œçš„åŸç†å’Œä»£ç è¿›è¡Œäº†åˆ†æï¼Œå¯¹äºç»™å®šçš„æ¨¡å‹ï¼Œç°åœ¨è¿˜éœ€è¦è§£å†³å‡ ä¸ªé—®é¢˜ï¼š</p>
<ul>
<li>å¦‚ä½•æŠŠæ¨¡å‹åˆ‡åˆ†ç»™èŠ‚ç‚¹ï¼Œæ¯”å¦‚å“ªä¸ªèŠ‚ç‚¹è´Ÿè´£å“ªäº›å±‚ã€‚</li>
<li>æ•°æ®å¹¶è¡Œï¼Œæ¨¡å‹å¹¶è¡Œï¼Œæµæ°´çº¿å¹¶è¡Œè¿™å‡ ç§å¹¶è¡Œä¹‹ä¸­ï¼Œæ¯ä¸ªèŠ‚ç‚¹åˆ†åˆ«å±äºå“ªä¸ªéƒ¨åˆ†ï¼Ÿ</li>
<li>å¦‚ä½•é¿å…æµæ°´çº¿å¸¦æ¥çš„é—®é¢˜ã€‚</li>
</ul>
<p>æˆ‘ä»¬æ¥ä¸‹æ¥å°±ä»”ç»†åˆ†æä¸€ä¸‹ã€‚</p>
<h2 id="0x02-åˆå§‹åŒ–">0x02 åˆå§‹åŒ–</h2>
<p>initialize_model_parallel  æ–¹æ³•ç”¨æ¥è®¾ç½®æ¨¡å‹å¹¶è¡Œï¼Œæ‰€ä»¥æˆ‘ä»¬æ¥ä¸‹æ¥å°±å…·ä½“åˆ†æã€‚</p>
<h3 id="21-å…¨å±€å˜é‡">2.1 å…¨å±€å˜é‡</h3>
<p>å› ä¸ºå‰æ–‡_initialize_distributedä¹‹ä¸­è°ƒç”¨äº†torch.distributed.init_process_group åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒï¼Œæ‰€ä»¥æˆ‘ä»¬çŸ¥é“ï¼Œæ¯ä¸ªè¿›ç¨‹éƒ½æœ‰è‡ªå·±çš„ gloabl rank å’Œ local rankï¼Œéƒ½æœ‰è‡ªå·±çš„å…¨å±€å˜é‡ã€‚</p>
<p>ä¸»è¦å˜é‡å¦‚ä¸‹ï¼ˆå…·ä½“ä¾‹å­å¯ä»¥ç»“åˆ initialize_model_parallel ä¹‹ä¸­çš„æ³¨é‡Šæ¥çœ‹ï¼‰ï¼š</p>
<ul>
<li>_TENSOR_MODEL_PARALLEL_GROUP ï¼šå½“å‰ rank æ‰€å±äºçš„Intra-layer model parallel groupï¼Œå°±æ˜¯tensor å¹¶è¡Œè¿›ç¨‹ç»„ã€‚
<ul>
<li>å‡å¦‚æ¯ä¸€å±‚åˆ†ä¸ºä¸¤ä¸ªtensorï¼Œåˆ™ _TENSOR_MODEL_PARALLEL_GROUP ä¾‹å­ä¸ºï¼š[g0, g1], [g2, g3], [g4, g5], [g6, g7], [g8, g9], [g10, g11], [g12, g13], [g14, g15]ã€‚</li>
</ul>
</li>
<li>_PIPELINE_MODEL_PARALLEL_GROUP ï¼šå½“å‰ rank æ‰€å±äºçš„Intra-layer model parallel groupï¼Œå°±æ˜¯æµæ°´çº¿è¿›ç¨‹ç»„ã€‚
<ul>
<li>å‡å¦‚æµæ°´çº¿æ·±åº¦ä¸º4ï¼Œåˆ™ä¾‹å­ä¸º [g0, g4, g8, g12], [g1, g5, g9, g13], [g2, g6, g10, g14], [g3, g7, g11, g15]ã€‚</li>
</ul>
</li>
<li>_MODEL_PARALLEL_GROUP ï¼šå½“å‰ rank æ‰€å±äºçš„æ¨¡å‹å¹¶è¡Œè¿›ç¨‹ç»„ï¼ŒåŒ…æ‹¬äº†ä»¥ä¸Šä¸¤ç»„ã€‚
<ul>
<li>é’ˆå¯¹æˆ‘ä»¬ä¾‹å­ï¼Œå°±æ˜¯å®Œæ•´æ¨¡å‹è¢«å¤åˆ¶äº†ä¸¤ä»½ï¼Œå…¶ GPU èŠ‚ç‚¹å…·ä½“æ˜¯[0, 1, 4, 5, 8, 9, 12, 13]ï¼Œ[2, 3, 6, 7, 10, 11, 14, 15]</li>
</ul>
</li>
<li>_EMBEDDING_GROUP ï¼š åµŒå…¥å¯¹åº”çš„è¿›ç¨‹ç»„ã€‚</li>
<li>_DATA_PARALLEL_GROUP ï¼šå½“å‰ rank æ‰€å±äºçš„Data parallel groupã€‚
<ul>
<li>å‡å¦‚æ•°æ®å¹¶è¡Œåº¦æ•°ä¸º2ï¼Œåˆ™ä¾‹å­ä¸º[g0, g2], [g1, g3], [g4, g6], [g5, g7], [g8, g10], [g9, g11], [g12, g14], [g13, g15]ã€‚</li>
</ul>
</li>
</ul>
<p>å…·ä½“å¦‚ä¸‹ï¼š</p>
<pre><code class="language-python"># Intra-layer model parallel group that the current rank belongs to.
_TENSOR_MODEL_PARALLEL_GROUP = None
# Inter-layer model parallel group that the current rank belongs to.
_PIPELINE_MODEL_PARALLEL_GROUP = None
# Model parallel group (both intra- and pipeline) that the current rank belongs to.
_MODEL_PARALLEL_GROUP = None
# Embedding group.
_EMBEDDING_GROUP = None
# Data parallel group that the current rank belongs to.
_DATA_PARALLEL_GROUP = None

_VIRTUAL_PIPELINE_MODEL_PARALLEL_RANK = None
_VIRTUAL_PIPELINE_MODEL_PARALLEL_WORLD_SIZE = None
_PIPELINE_MODEL_PARALLEL_SPLIT_RANK = None

# These values enable us to change the mpu sizes on the fly.
_MPU_TENSOR_MODEL_PARALLEL_WORLD_SIZE = None
_MPU_PIPELINE_MODEL_PARALLEL_WORLD_SIZE = None
_MPU_TENSOR_MODEL_PARALLEL_RANK = None
_MPU_PIPELINE_MODEL_PARALLEL_RANK = None

# A list of ranks that have a copy of the embedding.
_EMBEDDING_GLOBAL_RANKS = None

# A list of global ranks for each pipeline group to ease calculation of the source
# rank when broadcasting from the first or last pipeline stage.
_PIPELINE_GLOBAL_RANKS = None
</code></pre>
<h3 id="22-åˆå§‹åŒ–ä»£ç ">2.2 åˆå§‹åŒ–ä»£ç </h3>
<p>æˆ‘ä»¬é¦–å…ˆæŠŠ initialize_model_parallel ä»£ç æ‘˜å½•å‡ºæ¥ã€‚initialize_model_parallel ä½œç”¨å°±æ˜¯å¯¹æ¨¡å‹è¿›è¡Œåˆ†ç»„ï¼Œç„¶ååˆå§‹åŒ–è¿›ç¨‹ç»„ç›¸å…³çš„å„ç§å…¨å±€å˜é‡ã€‚</p>
<pre><code class="language-python">def initialize_model_parallel(tensor_model_parallel_size_=1,
                              pipeline_model_parallel_size_=1,
                              virtual_pipeline_model_parallel_size_=None,
                              pipeline_model_parallel_split_rank_=None):
    """
    Initialize model data parallel groups.

    Arguments:
        tensor_model_parallel_size: number of GPUs used for tensor model parallelism.
        pipeline_model_parallel_size: number of GPUs used for pipeline model parallelism.
        virtual_pipeline_model_parallel_size: number of virtual stages (interleaved
                                              pipeline).
        pipeline_model_parallel_split_rank: for models with both encoder and decoder,
                                            rank in pipeline with split point.


    Let's say we have a total of 16 GPUs denoted by g0 ... g15 and we
    use 2 GPUs to parallelize the model tensor, and 4 GPUs to parallelize
    the model pipeline. The present function will
    create 8 tensor model-parallel groups, 4 pipeline model-parallel groups
    and 8 data-parallel groups as:
        8 data_parallel groups:
            [g0, g2], [g1, g3], [g4, g6], [g5, g7], [g8, g10], [g9, g11], [g12, g14], [g13, g15]
        8 tensor model-parallel groups:
            [g0, g1], [g2, g3], [g4, g5], [g6, g7], [g8, g9], [g10, g11], [g12, g13], [g14, g15]
        4 pipeline model-parallel groups:
            [g0, g4, g8, g12], [g1, g5, g9, g13], [g2, g6, g10, g14], [g3, g7, g11, g15]
    Note that for efficiency, the caller should make sure adjacent ranks
    are on the same DGX box. For example if we are using 2 DGX-1 boxes
    with a total of 16 GPUs, rank 0 to 7 belong to the first box and
    ranks 8 to 15 belong to the second box.
    """
    if torch.distributed.get_rank() == 0:
        print('&gt; initializing tensor model parallel with size {}'.format(
            tensor_model_parallel_size_))
        print('&gt; initializing pipeline model parallel with size {}'.format(
            pipeline_model_parallel_size_))
    # Get world size and rank. Ensure some consistencies.
    world_size = torch.distributed.get_world_size()
    tensor_model_parallel_size = min(tensor_model_parallel_size_, world_size)
    pipeline_model_parallel_size = min(pipeline_model_parallel_size_, world_size)
    ensure_divisibility(world_size,
                        tensor_model_parallel_size * pipeline_model_parallel_size)
    data_parallel_size = world_size // (tensor_model_parallel_size *
                                        pipeline_model_parallel_size)

    num_tensor_model_parallel_groups = world_size // tensor_model_parallel_size
    num_pipeline_model_parallel_groups = world_size // pipeline_model_parallel_size
    num_data_parallel_groups = world_size // data_parallel_size

    if virtual_pipeline_model_parallel_size_ is not None:
        global _VIRTUAL_PIPELINE_MODEL_PARALLEL_RANK
        global _VIRTUAL_PIPELINE_MODEL_PARALLEL_WORLD_SIZE
        _VIRTUAL_PIPELINE_MODEL_PARALLEL_RANK = 0
        _VIRTUAL_PIPELINE_MODEL_PARALLEL_WORLD_SIZE = virtual_pipeline_model_parallel_size_

    if pipeline_model_parallel_split_rank_ is not None:
        global _PIPELINE_MODEL_PARALLEL_SPLIT_RANK
        _PIPELINE_MODEL_PARALLEL_SPLIT_RANK = pipeline_model_parallel_split_rank_

    rank = torch.distributed.get_rank()

    # Build the data-parallel groups.
    global _DATA_PARALLEL_GROUP
    all_data_parallel_group_ranks = []
    for i in range(pipeline_model_parallel_size):
        start_rank = i * num_pipeline_model_parallel_groups
        end_rank = (i + 1) * num_pipeline_model_parallel_groups
        for j in range(tensor_model_parallel_size):
            ranks = range(start_rank + j, end_rank,
                          tensor_model_parallel_size)
            all_data_parallel_group_ranks.append(list(ranks))
            group = torch.distributed.new_group(ranks)
            if rank in ranks:
                _DATA_PARALLEL_GROUP = group

    # Build the model-parallel groups.
    global _MODEL_PARALLEL_GROUP
    for i in range(data_parallel_size):
        ranks = [data_parallel_group_ranks[i]
                 for data_parallel_group_ranks in all_data_parallel_group_ranks]
        group = torch.distributed.new_group(ranks)
        if rank in ranks:
            _MODEL_PARALLEL_GROUP = group

    # Build the tensor model-parallel groups.
    global _TENSOR_MODEL_PARALLEL_GROUP
    for i in range(num_tensor_model_parallel_groups):
        ranks = range(i * tensor_model_parallel_size,
                      (i + 1) * tensor_model_parallel_size)
        group = torch.distributed.new_group(ranks)
        if rank in ranks:
            _TENSOR_MODEL_PARALLEL_GROUP = group

    # Build the pipeline model-parallel groups and embedding groups
    # (first and last rank in each pipeline model-parallel group).
    global _PIPELINE_MODEL_PARALLEL_GROUP
    global _PIPELINE_GLOBAL_RANKS
    global _EMBEDDING_GROUP
    global _EMBEDDING_GLOBAL_RANKS
    for i in range(num_pipeline_model_parallel_groups):
        ranks = range(i, world_size,
                      num_pipeline_model_parallel_groups)
        group = torch.distributed.new_group(ranks)
        if rank in ranks:
            _PIPELINE_MODEL_PARALLEL_GROUP = group
            _PIPELINE_GLOBAL_RANKS = ranks
        # Setup embedding group (to exchange gradients between
        # first and last stages).
        if len(ranks) &gt; 1:
            embedding_ranks = [ranks[0], ranks[-1]]
            if pipeline_model_parallel_split_rank_ is not None and \
                    pipeline_model_parallel_split_rank_ not in embedding_ranks:
                embedding_ranks = [ranks[0],
                                   ranks[pipeline_model_parallel_split_rank_],
                                   ranks[-1]]
        else:
            embedding_ranks = ranks
        group = torch.distributed.new_group(embedding_ranks)
        if rank in embedding_ranks:
            _EMBEDDING_GROUP = group
        if rank in ranks:
            _EMBEDDING_GLOBAL_RANKS = embedding_ranks
</code></pre>
<h2 id="0x03-åˆ‡åˆ†æ ·ä¾‹">0x03 åˆ‡åˆ†æ ·ä¾‹</h2>
<p>æˆ‘ä»¬ä½¿ç”¨æ³¨é‡Šå†…å®¹æ¥è¿›è¡Œå­¦ä¹ å¦‚ä½•åˆ‡åˆ†æ¨¡å‹ï¼Œå¦‚ä½•æŠŠå¤šç§å¹¶è¡Œæ¨¡å¼ç»„åˆåœ¨ä¸€èµ·ã€‚</p>
<h3 id="31-æ³¨é‡Š">3.1 æ³¨é‡Š</h3>
<p>initialize_model_parallel çš„æ³¨é‡Šå€¼å¾—æˆ‘ä»¬æ·±å…¥å­¦ä¹ ï¼Œå…·ä½“å¦‚ä¸‹ï¼š</p>
<pre><code>Let's say we have a total of 16 GPUs denoted by g0 ... g15 and we
use 2 GPUs to parallelize the model tensor, and 4 GPUs to parallelize
the model pipeline. The present function will
create 8 tensor model-parallel groups, 4 pipeline model-parallel groups
and 8 data-parallel groups as:
    8 data_parallel groups:
        [g0, g2], [g1, g3], [g4, g6], [g5, g7], [g8, g10], [g9, g11], [g12, g14], [g13, g15]
    8 tensor model-parallel groups:
        [g0, g1], [g2, g3], [g4, g5], [g6, g7], [g8, g9], [g10, g11], [g12, g13], [g14, g15]
    4 pipeline model-parallel groups:
        [g0, g4, g8, g12], [g1, g5, g9, g13], [g2, g6, g10, g14], [g3, g7, g11, g15]
Note that for efficiency, the caller should make sure adjacent ranks
are on the same DGX box. For example if we are using 2 DGX-1 boxes
with a total of 16 GPUs, rank 0 to 7 belong to the first box and
ranks 8 to 15 belong to the second box.
</code></pre>
<p>ä»æ³¨é‡Šå¯ä»¥çŸ¥é“å¦‚ä¸‹ä¿¡æ¯ï¼š</p>
<ul>
<li>
<p>å‡å®šç›®å‰æœ‰16ä¸ªGPUï¼Œå±äºä¸¤ä¸ªnodeï¼Œrank 0 ï½7 å±äºç¬¬ä¸€ä¸ªèŠ‚ç‚¹ï¼Œrank 8 ï½ 15 å±äºç¬¬äºŒä¸ªèŠ‚ç‚¹ã€‚</p>
</li>
<li>
<p>create 8 tensor model-parallel groups, 4 pipeline model-parallel groupsï¼Œè¿™è¯´æ˜å°†ä¸€ä¸ªå®Œæ•´æ¨¡å‹åˆ‡åˆ†å¦‚ä¸‹ï¼š</p>
<ul>
<li>æ²¿ç€è¡Œæ¨ªå‘åˆ‡äº†ä¸€åˆ€ï¼štensor_model_parallel_size = 16 / 8 = 2ï¼Œå°±æ˜¯2ä¸ª GPUs æ¥è¿›è¡Œæ¨¡å‹å¼ é‡å¹¶è¡Œã€‚</li>
<li>æ²¿ç€åˆ—çºµå‘åˆ‡äº†ä¸‰åˆ€ï¼špipeline_model_parallel_size = 16 /4 = 4ï¼Œå°±æ˜¯4ä¸ªGPUs è¿›è¡Œæµæ°´çº¿å¹¶è¡Œã€‚</li>
<li>å› æ­¤ï¼Œä¸€ä¸ªæ¨¡å‹åˆ†ä¸º8å—ï¼Œæ¯ä¸€å—æ”¾åœ¨ä¸€ä¸ªGPUä¹‹ä¸Šï¼Œå°±æ˜¯8ä¸ªGPUã€‚è€Œé€šè¿‡å¦‚ä¸‹è®¡ç®—å¯ä»¥çŸ¥ 16 GPUs / 8 GPUs = 2 modelsã€‚å³ï¼Œ16å¼ å¡å¯ä»¥æ”¾ç½®ä¸¤ä¸ªå®Œæ•´æ¨¡å‹ã€‚</li>
</ul>
</li>
<li>
<p>å› ä¸ºå¼ é‡æ¨¡å‹å¹¶è¡Œç»„å¤§å°æ˜¯2ï¼Œå³16ä¸ªGPUè¢«åˆ†æˆ8ç»„ï¼Œåˆ™è¿™8ç»„å†…å®¹æ˜¯ [g0, g1], [g2, g3], [g4, g5], [g6, g7], [g8, g9], [g10, g11], [g12, g13], [g14, g15]ã€‚</p>
</li>
<li>
<p>å› ä¸ºæµæ°´çº¿å¹¶è¡Œç»„å¤§å°æ˜¯4ï¼Œå³16ä¸ªGPUè¢«åˆ†æˆ4ç»„ï¼Œåˆ™è¿™4ç»„å†…å®¹æ˜¯[g0, g4, g8, g12], [g1, g5, g9, g13], [g2, g6, g10, g14], [g3, g7, g11, g15]ã€‚</p>
</li>
<li>
<p>å› ä¸ºæ•°æ®å¹¶è¡Œç»„å¤§å°æ˜¯2ï¼Œ16ä¸ªGPUè¢«åˆ†æˆ8ç»„ï¼Œåˆ™è¿™8ç»„å†…å®¹æ˜¯[g0, g2], [g1, g3], [g4, g6], [g5, g7], [g8, g10], [g9, g11], [g12, g14], [g13, g15]ã€‚</p>
</li>
<li>
<p>ä»¥ä¸Šè¿™äº›è¿›ç¨‹ç»„éƒ½æ˜¯é€šè¿‡ torch.distributed.new_group æ¥å®Œæˆï¼Œè¿™æ ·ç»„å†…è¿›ç¨‹ä¹‹é—´å°±çŸ¥é“å“ªäº›è¿›ç¨‹æ˜¯åœ¨åŒä¸€ä¸ªç»„å†…ï¼Œæ˜¯åœ¨ä¸€èµ·è®­ç»ƒçš„ï¼Œä¹ŸçŸ¥é“æ€ä¹ˆé€šä¿¡ã€‚</p>
</li>
</ul>
<h3 id="32-åˆ‡åˆ†æƒ…å†µ">3.2 åˆ‡åˆ†æƒ…å†µ</h3>
<p>æ¨¡å‹åŸå§‹å›¾å¦‚ä¸‹</p>
<p><img src="https://img2022.cnblogs.com/blog/1850883/202202/1850883-20220209204730167-1554249556.png" alt="" loading="lazy"></p>
<p>æ¨¡å‹åˆ‡åˆ†ä¹‹åå¦‚ä¸‹ï¼Œä¸€å…±è¢«åˆ†æˆ8å—ã€‚å…¶ä¸­ï¼Œç¬¬ä¸€å±‚è¢«åˆ‡åˆ†ä¸º Aï¼ŒBï¼Œæ‰€ä»¥ Aï¼ŒB ä¹‹é—´å°±æ˜¯ Tensor Model parallelã€‚åé¢ Cï¼ŒD ä¹‹é—´ä¹Ÿæ˜¯ Tensor Model parallelï¼ŒæŠŠä¸¤å±‚éƒ½åšäº†åˆ‡åˆ†ï¼Œä¾æ¬¡ç±»æ¨ã€‚</p>
<p><img src="https://img2022.cnblogs.com/blog/1850883/202202/1850883-20220209204751060-66512780.png" alt="" loading="lazy"></p>
<p><strong>æˆ‘ä»¬çš„ç›®æ ‡å°±æ˜¯ç”¨ä»£ç æ¥çœ‹çœ‹å¦‚ä½•ç”Ÿæˆæ³¨é‡Šé‡Œé¢çš„å„ç§æ¨¡å‹ç»„</strong>ã€‚</p>
<h3 id="33-åˆ‡åˆ†ç­–ç•¥">3.3 åˆ‡åˆ†ç­–ç•¥</h3>
<p>æˆ‘ä»¬æ¥ä¸‹æ¥çœ‹çœ‹å…·ä½“åˆ‡åˆ†çš„ç­–ç•¥ï¼Œä¹Ÿå°±æ˜¯GPUåˆ†é…ç­–ç•¥ã€‚åˆ‡åˆ†éœ€è¦ç»¼åˆè€ƒè™‘å¤šç§æƒ…å†µï¼Œé¦–å…ˆçœ‹çœ‹æ¨¡å‹å¹¶è¡Œçš„é€šä¿¡çŠ¶å†µã€‚</p>
<ul>
<li><strong>å¼ é‡å¹¶è¡Œ</strong>ï¼šé€šä¿¡å‘ç”Ÿåœ¨æ¯å±‚çš„å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­è¿‡ç¨‹ä¹‹ä¸­ï¼Œé€šä¿¡ç±»å‹æ˜¯all-reduceï¼Œä¸ä½†å•æ¬¡é€šä¿¡æ•°æ®é‡å¤§ï¼Œå¹¶ä¸”é€šä¿¡é¢‘ç¹ã€‚</li>
<li><strong>æµæ°´çº¿å¹¶è¡Œ</strong>ï¼šé€šä¿¡åœ¨æµæ°´çº¿é˜¶æ®µç›¸é‚»çš„åˆ‡åˆ†ç‚¹ä¹‹ä¸Šï¼Œé€šä¿¡ç±»å‹æ˜¯P2Pé€šä¿¡ï¼Œå•è¯é€šä¿¡æ•°æ®é‡è¾ƒå°‘ä½†æ˜¯æ¯”è¾ƒé¢‘ç¹ï¼Œè€Œä¸”å› ä¸ºæµæ°´çº¿çš„ç‰¹ç‚¹ï¼Œä¼šäº§ç”ŸGPUç©ºé—²æ—¶é—´ï¼Œè¿™é‡Œç§°ä¸ºæµæ°´çº¿æ°”æ³¡ï¼ˆBubbleï¼‰ã€‚</li>
</ul>
<p>æˆ‘ä»¬æ¥ä¸‹æ¥çœ‹çœ‹å„ç§å¹¶è¡Œæœºåˆ¶çš„å¯¹æ¯”ã€‚</p>
<ul>
<li><strong>Tensor versus Pipeline Parallelism</strong>. å¼ é‡æ¨¡å‹çš„å¹¶è¡Œæ€§åœ¨èŠ‚ç‚¹å†…æ˜¯æœ€å¥½çš„ï¼Œå› ä¸ºå®ƒä¼šå‡å°‘é€šä¿¡é‡ã€‚å¦ä¸€æ–¹é¢ï¼Œæµæ°´çº¿æ¨¡å‹å¹¶è¡Œä½¿ç”¨æ›´ä¾¿å®œçš„ç‚¹å¯¹ç‚¹é€šä¿¡ï¼Œå¯ä»¥è·¨èŠ‚ç‚¹æ‰§è¡Œï¼Œè€Œä¸ä¼šé™åˆ¶æ•´ä¸ªè®¡ç®—ã€‚ç„¶è€Œï¼Œæµæ°´çº¿å¹¶è¡Œæ€§ä¼šåœ¨æµæ°´çº¿æ°”æ³¡ä¸­èŠ±è´¹å¤§é‡æ—¶é—´ï¼Œå› æ­¤ï¼Œåº”é™åˆ¶æµæ°´çº¿çº§çš„æ€»æ•°ï¼Œä»¥ä¾¿æµæ°´çº¿ä¸­çš„microbatchesæ•°é‡æ˜¯æµæ°´çº¿æ·±åº¦çš„åˆç†å€æ•°ã€‚å½“å¼ é‡å¹¶è¡Œå¤§å°ç­‰äºå•ä¸ªèŠ‚ç‚¹ä¸­çš„GPUæ•°é‡æ—¶ä¼šè¾¾åˆ°å³°å€¼æ€§èƒ½ã€‚</li>
<li><strong>Pipeline versus Data Parallelism.</strong> å¯¹äºæ¯ä¸ªbatch sizeï¼Œååé‡éšç€æµæ°´çº¿å¹¶è¡Œè§„æ¨¡çš„å¢åŠ è€Œé™ä½ã€‚æµæ°´çº¿æ¨¡å‹å¹¶è¡Œåº”è¯¥ä¸»è¦ç”¨äºæ”¯æŒä¸é€‚åˆå•ä¸ª worker çš„å¤§å‹æ¨¡å‹è®­ç»ƒã€‚è€Œæ•°æ®å¹¶è¡Œåº”è¯¥ç”¨äºæ‰©å¤§è®­ç»ƒè§„æ¨¡ã€‚</li>
<li><strong>Tensor versus Data Parallelism.</strong> æ¥ä¸‹æ¥çœ‹çœ‹æ•°æ®å’Œå¼ é‡æ¨¡å‹çš„å¹¶è¡Œæ€§å¯¹æ€§èƒ½çš„å½±å“ã€‚åœ¨è¾ƒå¤§çš„æ‰¹å¤„ç†é‡å’Œå¾®æ‰¹å¤„ç†é‡ä¸º1çš„æƒ…å†µä¸‹ï¼Œæ•°æ®å¹¶è¡Œé€šä¿¡å¹¶ä¸é¢‘ç¹ï¼›å¼ é‡æ¨¡å‹å¹¶è¡Œéœ€è¦å¯¹æ‰¹å¤„ç†ä¸­çš„æ¯ä¸ªå¾®æ‰¹è¿›è¡Œall-to-allé€šä¿¡ã€‚è¿™ç§all-to-allçš„é€šä¿¡ä¸»å¯¼äº†ç«¯åˆ°ç«¯çš„è®­ç»ƒæ—¶é—´ï¼Œç‰¹åˆ«æ˜¯å½“é€šä¿¡éœ€è¦åœ¨å¤šGPUèŠ‚ç‚¹ä¸Šè¿›è¡Œæ—¶ã€‚æ­¤å¤–ï¼Œéšç€å¼ é‡æ¨¡å‹å¹¶è¡Œè§„æ¨¡çš„å¢åŠ ï¼Œæˆ‘ä»¬åœ¨æ¯ä¸ªGPUä¸Šæ‰§è¡Œè¾ƒå°çš„çŸ©é˜µä¹˜æ³•ï¼ˆå› ä¸ºä¼šæŠŠæ¨¡å‹å¼ é‡è¿›è¡Œåˆ‡åˆ†ï¼‰ï¼Œè¿™é™ä½äº†æ¯ä¸ªGPUçš„åˆ©ç”¨ç‡ã€‚</li>
</ul>
<p>æœ€åçœ‹çœ‹ç»“è®º</p>
<ul>
<li>Tensoræ¨¡å‹å¹¶è¡Œè¢«ç”¨äºintra-node transformer å±‚ï¼Œå› ä¸ºå¼ é‡å¹¶è¡Œè®¡ç®—å¯†é›†ä¸”æ˜¯è€—è´¹å¤§é‡å¸¦å®½ï¼Œè¿™æ ·ä¼šåœ¨HGX basedç³»ç»Ÿä¸Šé«˜æ•ˆè¿è¡Œã€‚</li>
<li>Pipeline æ¨¡å‹å¹¶è¡Œä¸»è¦è¢«ç”¨äºinter-node transformer å±‚ï¼Œå› ä¸ºPipeline å¹¶è¡Œçš„é€šä¿¡å¸¦å®½å ç”¨å°‘ï¼Œå…¶å¯ä»¥æœ‰æ•ˆåˆ©ç”¨é›†ç¾¤ä¸­å¤šç½‘å¡è®¾è®¡ã€‚</li>
<li>æ•°æ®å¹¶è¡Œåˆ™åœ¨å‰ä¸¤è€…åŸºç¡€ä¹‹ä¸Šè¿›è¡ŒåŠ æŒï¼Œä½¿å¾—è®­ç»ƒå¯ä»¥æ‰©å±•åˆ°æ›´å¤§è§„æ¨¡å’Œæ›´å¿«çš„é€Ÿåº¦ã€‚æˆ‘ä»¬åº”è¯¥æ³¨æ„åˆ°ï¼Œå°½ç®¡æ•°æ®å¹¶è¡Œå¯ä»¥å¸¦æ¥é«˜æ•ˆçš„æ‰©å±•ï¼Œä½†æˆ‘ä»¬ä¸èƒ½å•ç‹¬ä½¿ç”¨æ•°æ®å¹¶è¡Œæ¥å¤„ç†è®­ç»ƒè¶…å¤§æ¨¡å‹ï¼Œå› ä¸º aï¼‰å†…å­˜å®¹é‡ä¸è¶³ï¼Œbï¼‰æ•°æ®å¹¶è¡Œçš„æ‰©å±•é™åˆ¶ã€‚</li>
</ul>
<h3 id="34-å®éªŒ">3.4 å®éªŒ</h3>
<p>æˆ‘ä»¬æ¥ä¸‹æ¥åšä¸€ä¸ªå®éªŒçœ‹çœ‹ã€‚</p>
<pre><code class="language-python">import torch

world_size = 16
tensor_model_parallel_size = 2 # 2 GPUs to parallelize the model tensor
pipeline_model_parallel_size = 4 # 4 GPUs to parallelize the model pipeline
data_parallel_size = world_size // (tensor_model_parallel_size *
                                    pipeline_model_parallel_size) # 2
num_tensor_model_parallel_groups = world_size // tensor_model_parallel_size # 8
num_pipeline_model_parallel_groups = world_size // pipeline_model_parallel_size # 4
num_data_parallel_groups = world_size // data_parallel_size # 8

# Build the data-parallel groups.
print("------ Build the data-parallel groups -----")
all_data_parallel_group_ranks = []
for i in range(pipeline_model_parallel_size):
    start_rank = i * num_pipeline_model_parallel_groups
    end_rank = (i + 1) * num_pipeline_model_parallel_groups
    for j in range(tensor_model_parallel_size):
        ranks = range(start_rank + j, end_rank,
                      tensor_model_parallel_size)
        all_data_parallel_group_ranks.append(list(ranks))
print(all_data_parallel_group_ranks)

# Build the model-parallel groups.
print("------ Build the model-parallel groups -----")
for i in range(data_parallel_size):
    ranks = [data_parallel_group_ranks[i]
             for data_parallel_group_ranks in all_data_parallel_group_ranks]
    print(list(ranks))

# Build the tensor model-parallel groups.
print("------ Build the tensor model-parallel groups -----")
for i in range(num_tensor_model_parallel_groups):
    ranks = range(i * tensor_model_parallel_size,
                  (i + 1) * tensor_model_parallel_size)
    print(list(ranks))

# Build the pipeline model-parallel groups and embedding groups
# (first and last rank in each pipeline model-parallel group).
print("------ Build the pipeline model-parallel groups -----")
for i in range(num_pipeline_model_parallel_groups):
    ranks = range(i, world_size,
                  num_pipeline_model_parallel_groups)
    print(list(ranks))
</code></pre>
<p>è¾“å‡ºå¦‚ä¸‹ã€‚éœ€è¦æ³¨æ„ï¼Œè¿™é‡Œéƒ½æ˜¯ GPU çš„åºåˆ—å·ï¼Œ[0,2] å°±æ˜¯ [g0, g2]ï¼š</p>
<pre><code class="language-cpp">------ Build the data-parallel groups -----
[[0, 2], [1, 3], [4, 6], [5, 7], [8, 10], [9, 11], [12, 14], [13, 15]]
------ Build the model-parallel groups -----
[0, 1, 4, 5, 8, 9, 12, 13]
[2, 3, 6, 7, 10, 11, 14, 15]
------ Build the tensor model-parallel groups -----
[0, 1]
[2, 3]
[4, 5]
[6, 7]
[8, 9]
[10, 11]
[12, 13]
[14, 15]
------ Build the pipeline model-parallel groups -----
[0, 4, 8, 12]
[1, 5, 9, 13]
[2, 6, 10, 14]
[3, 7, 11, 15]

æˆ‘ä»¬å¯¹æ¯”ä¸€ä¸‹æ³¨é‡Šï¼Œå‘ç°ä»£ç æ‰“å°ç»“æœå¯ä»¥å’Œæ³¨é‡Šå¯¹åº”ä¸Šï¼š
    Let's say we have a total of 16 GPUs denoted by g0 ... g15 and we
    use 2 GPUs to parallelize the model tensor, and 4 GPUs to parallelize
    the model pipeline. The present function will
    create 8 tensor model-parallel groups, 4 pipeline model-parallel groups
    and 8 data-parallel groups as:
        8 data_parallel groups:
            [g0, g2], [g1, g3], [g4, g6], [g5, g7], [g8, g10], [g9, g11], [g12, g14], [g13, g15]
        8 tensor model-parallel groups:
            [g0, g1], [g2, g3], [g4, g5], [g6, g7], [g8, g9], [g10, g11], [g12, g13], [g14, g15]
        4 pipeline model-parallel groups:
            [g0, g4, g8, g12], [g1, g5, g9, g13], [g2, g6, g10, g14], [g3, g7, g11, g15]
</code></pre>
<p>æˆ‘ä»¬æ¥ä¸‹æ¥ä¼šè¿›è¡Œå…·ä½“åˆ†æã€‚</p>
<h2 id="0x04-èµ·å§‹çŠ¶æ€">0x04 èµ·å§‹çŠ¶æ€</h2>
<h3 id="41-gpu-çŠ¶å†µ">4.1 GPU çŠ¶å†µ</h3>
<p>ä»æ³¨é‡Šä¸­å¯ä»¥çœ‹åˆ°ï¼š</p>
<pre><code>Note that for efficiency, the caller should make sure adjacent ranks are on the same DGX box. For example if we are using 2 DGX-1 boxes with a total of 16 GPUs, rank 0 to 7 belong to the first box and ranks 8 to 15 belong to the second box.
</code></pre>
<p>æ„æ€å°±æ˜¯ï¼šè°ƒç”¨è€…éœ€è¦ç¡®ä¿ç›¸é‚»çš„rankåœ¨åŒä¸€ä¸ªèŠ‚ç‚¹ä¸Šï¼Œæˆ‘ä»¬ä¾‹å­æœ‰ä¸¤ä¸ªNodeï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªNodeæ‹¥æœ‰ GPU 0 ï½ 7ï¼Œå°±æ˜¯ rank 0 ï½ 7ï¼Œç¬¬äºŒä¸ªNodeæ˜¯ GPU 8ï½15ï¼Œå°±æ˜¯ rank 8 ï½ 15ã€‚</p>
<p>å…·ä½“å¦‚ä¸‹ï¼Œè¿™é‡Œæ¯è¡Œ4ä¸ªGPUï¼Œæ˜¯å› ä¸º 4 GPUs to parallelize the model pipelineï¼Œæ‰€ä»¥<u>æµæ°´çº¿æ¯ä¸ªstageæ˜¯4ä¸ªGPU</u>ã€‚</p>
<p><img src="https://img2022.cnblogs.com/blog/1850883/202202/1850883-20220209204806543-1100351212.jpg" alt="" loading="lazy"></p>
<h3 id="42-ç¬¦å·è¯´æ˜">4.2 ç¬¦å·è¯´æ˜</h3>
<p>ä¸‹é¢æ˜¯è®ºæ–‡ä¹‹ä¸­æåˆ°çš„ä¸€äº›ç¬¦å·ï¼Œè¿™é‡Œæœ‰å¿…è¦å†å–å‡ºæ¥æ¸©ä¹ ä¸€ä¸‹ï¼š</p>
<ul>
<li>
<p>(ğ‘, ğ‘¡, ğ‘‘): Parallelization dimensions.</p>
</li>
<li>
<p>ğ‘ for the pipeline-modelparallel size,</p>
</li>
<li>
<p>ğ‘¡ for the tensor-model-parallel size, and ğ‘‘ for the data-parallel size.</p>
</li>
<li>
<p>ğ‘›: Number of GPUs. We require ğ‘ Â· ğ‘¡ Â· ğ‘‘ = ğ‘›.</p>
</li>
</ul>
<h3 id="43-åˆå§‹åˆ†ç»„">4.3 åˆå§‹åˆ†ç»„</h3>
<p>ä¾æ®æ³¨é‡Šï¼Œæˆ‘ä»¬å¾—å‡ºç›®å‰åˆ†ç»„æƒ…å†µå’Œä¸€äº›å…¨å±€ä¿¡æ¯ã€‚</p>
<ul>
<li>ä¸€å…±16ä¸ªGPUï¼Œæ‰€ä»¥ world_size ä¸º 16ã€‚å°±æ˜¯ Notation ä¹‹ä¸­çš„ nã€‚</li>
<li>ä½¿ç”¨ä¸¤ä¸ªGPUè¿›è¡Œ model tensor å¹¶è¡Œï¼Œæ‰€ä»¥  tensor_model_parallel_size = 2ã€‚å°±æ˜¯  Notation ä¹‹ä¸­çš„ tã€‚</li>
<li>ä½¿ç”¨å››ä¸ªGPUè¿›è¡Œæ¨¡å‹æµæ°´çº¿å¹¶è¡Œï¼Œæ‰€ä»¥ pipeline_model_parallel_size = 4ã€‚å°±æ˜¯ Notation ä¹‹ä¸­çš„ pã€‚å…¶å®ï¼Œå°±æ˜¯æµæ°´çº¿æ·±åº¦ä¸º 4ï¼Œå³ï¼Œ4 ä¸ª GPU æ˜¯ä¸²è¡Œçš„ã€‚</li>
<li>ä¾æ®ä¸Šé¢å®šä¹‰ï¼Œd = n / ( t * p) = 2ï¼Œå°±æ˜¯ data_parallel_size = 2ã€‚å› ä¸º t * p å°±æ˜¯ä¸€ä¸ªæ¨¡å‹æ‰€éœ€è¦çš„ GPUï¼Œd = (æ€» GPU / ä¸€ä¸ªæ¨¡å‹éœ€è¦çš„ GPU)ï¼Œç»“æœæ˜¯è¿™äº›GPUå¯ä»¥è®­ç»ƒ d ä¸ªæ¨¡å‹ï¼Œå°±æ˜¯å¯ä»¥ç”¨ d ä¸ª mini-batches è¿›è¡Œè¿™ä¸ª dä¸ªæ¨¡å‹ä¸€èµ·è®­ç»ƒï¼Œæ‰€ä»¥æ•°æ®å¹¶è¡Œåº¦ä¸º dã€‚</li>
</ul>
<p>æ¥ä¸‹æ¥ç»“åˆä»£ç çœ‹çœ‹éœ€è¦åˆ†æˆå¤šå°‘ä¸ªprocess groupsï¼Œä»–ä»¬åœ¨ä»£ç ä¹‹ä¸­çš„å˜é‡æ˜¯ä»€ä¹ˆã€‚</p>
<ul>
<li>num_tensor_model_parallel_groups å°±æ˜¯ä» tensor model å¹¶è¡Œè§’åº¦çœ‹ï¼Œåˆ†æˆ8 ä¸ªè¿›ç¨‹roupã€‚</li>
<li>num_pipeline_model_parallel_groups = world_size // pipeline_model_parallel_size å°±æ˜¯ä» model å¹¶è¡Œè§’åº¦çœ‹ï¼Œåˆ†æˆ 4 ä¸ª è¿›ç¨‹groupã€‚</li>
<li>num_data_parallel_groups = world_size // data_parallel_size  å°±æ˜¯ä»data å¹¶è¡Œè§’åº¦çœ‹ï¼Œåˆ†æˆ8 ä¸ª è¿›ç¨‹groupã€‚å°±æ˜¯ä¼šæœ‰ 8 ä¸ª DDPï¼Œæ¯ä¸ª DDP åŒ…æ‹¬ 2 ä¸ª rankã€‚</li>
<li>è¿˜æœ‰ä¸€ä¸ª _MODEL_PARALLEL_GROUPï¼Œ</li>
</ul>
<p>å…·ä½“å¦‚ä¸‹ï¼š</p>
<pre><code class="language-python">world_size = 16
tensor_model_parallel_size = 2 # 2 GPUs to parallelize the model tensor
pipeline_model_parallel_size = 4 # 4 GPUs to parallelize the model pipeline
data_parallel_size = world_size // (tensor_model_parallel_size *
                                    pipeline_model_parallel_size) # 2
num_tensor_model_parallel_groups = world_size // tensor_model_parallel_size # 8
num_pipeline_model_parallel_groups = world_size // pipeline_model_parallel_size # 4
num_data_parallel_groups = world_size // data_parallel_size # 8
</code></pre>
<h2 id="0x05-tensor-model-parallel">0x05 Tensor model-parallel</h2>
<p>æœ¬èŠ‚æˆ‘ä»¬åˆ†æçš„æ˜¯ï¼Œå¦‚ä½•å°† Node ä¸Šçš„ GPU åˆ†ç»™ tensor model å¹¶è¡Œç»„ã€‚</p>
<h3 id="51-åˆ†ç»„">5.1 åˆ†ç»„</h3>
<p>å¯¹äºæ³¨é‡Šä¾‹å­ï¼Œ16 / 2 = 8ï¼Œåˆ†æˆ 8 ä¸ªè¿›ç¨‹ç»„ï¼Œæ¯ä¸ªç»„ ä¸¤ä¸ª rankã€‚è¿™äº›åˆ†ç»„åˆ†åˆ«æ˜¯ï¼š[g0, g1], [g2, g3], [g4, g5], [g6, g7], [g8, g9], [g10, g11], [g12, g13], [g14, g15]ï¼Œæˆ‘ä»¬å¾—åˆ°äº†å¦‚ä¸‹ä¿¡æ¯ï¼š</p>
<ul>
<li>
<p>[g0, g1] å°±æ˜¯æŸä¸€å±‚åˆ†åˆ‡ä¸º2åŠï¼Œåˆ†åˆ«è¢« g0, g1 æ¥æ‰§è¡Œï¼Œ[g2, g3] è¡¨ç¤ºå¦ä¸€å±‚è¢«åˆ†ä¸ºä¸¤å±‚ï¼Œåˆ†åˆ«è¢« g2ï¼Œg3 æ¥æ‰§è¡Œã€‚</p>
</li>
<li>
<p>æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæ¯ä¸€ä¸ª tensor-model-parallel groupçš„ rankä¸€å®šæ˜¯ç›¸é‚»çš„ï¼Œæ¯”å¦‚ [g0, g1], [g2, g3]ã€‚</p>
</li>
<li>
<p>æ³¨æ„ï¼Œ0 ~ 7 ä¸ä»£è¡¨æ˜¯åŒä¸€ä¸ªæ¨¡å‹ã€‚0 ~ 7 æ˜¯åŒä¸€ä¸ª Node ä¸Šçš„ GPUï¼Œè¿™ç‚¹å®¹æ˜“è¢«æ··æ·†ã€‚</p>
</li>
</ul>
<p>æˆ‘ä»¬å†çœ‹çœ‹ä»£ç ï¼š</p>
<pre><code class="language-python">    # Build the tensor model-parallel groups.
    global _TENSOR_MODEL_PARALLEL_GROUP
    for i in range(num_tensor_model_parallel_groups): # 8
        ranks = range(i * tensor_model_parallel_size,
                      (i + 1) * tensor_model_parallel_size)
        group = torch.distributed.new_group(ranks) # å°±æœ‰ç”Ÿæˆ 8 ç»„
        if rank in ranks: 
            # å¦‚æœæœ¬rankåœ¨æŸä¸€listä¹‹ä¸­ï¼Œå³1 åœ¨ [0,1] ä¹‹ä¸­ï¼Œåˆ™æœ¬ rank å°±å±äº new_group([0,1])
            _TENSOR_MODEL_PARALLEL_GROUP = group 
</code></pre>
<p>æˆ‘ä»¬å®éªŒä¹‹ä¸­åœ¨è¿™é‡Œå¾—åˆ°ï¼š</p>
<pre><code>------ Build the tensor model-parallel groups -----
[0, 1]
[2, 3]
[4, 5]
[6, 7]
[8, 9]
[10, 11]
[12, 13]
[14, 15]

</code></pre>
<p>å¯¹åº”æˆ‘ä»¬å›¾ä¸Šå¦‚ä¸‹ï¼Œæ¯ä¸ª tensor model group ç”¨ä¸€ä¸ªè™šçº¿å°çŸ©å½¢æ¡†æ ‡ç¤ºï¼Œä¸€å…±8ä¸ªï¼š</p>
<p><img src="https://img2022.cnblogs.com/blog/1850883/202202/1850883-20220209204816668-1998353940.jpg" alt="" loading="lazy"></p>
<p>_TENSOR_MODEL_PARALLEL_GROUP = group å°±è®°å½•äº†æœ¬rankçš„è¿›ç¨‹ç»„ä¿¡æ¯ï¼Œæ¯”å¦‚ <strong>rank 2</strong>ï¼Œå®ƒçš„ _TENSOR_MODEL_PARALLEL_GROUP å†…å®¹å°±æ˜¯ï¼šgroup([g2, g3])ã€‚</p>
<h3 id="52-ä½¿ç”¨">5.2 ä½¿ç”¨</h3>
<p>æˆ‘ä»¬æ¥ä¸‹æ¥çœ‹çœ‹å¦‚ä½•ä½¿ç”¨ã€‚</p>
<p>get_tensor_model_parallel_group è¿”å›äº†è‡ªå·± rank å¯¹åº”çš„ tensor model groupã€‚</p>
<pre><code class="language-python">def get_tensor_model_parallel_group():
    """Get the tensor model parallel group the caller rank belongs to."""
    return _TENSOR_MODEL_PARALLEL_GROUP

</code></pre>
<p>åœ¨ megatron/mpu/mappings.py ä¹‹ä¸­æœ‰å¯¹ tensor model group çš„ä½¿ç”¨ï¼š</p>
<pre><code class="language-python">def _reduce(input_):
    """All-reduce the input tensor across model parallel group."""

    # Bypass the function if we are using only 1 GPU.
    if get_tensor_model_parallel_world_size()==1:
        return input_

    # All-reduce.
    torch.distributed.all_reduce(input_, group=get_tensor_model_parallel_group())

    return input_

</code></pre>
<p>å°±æ˜¯<u>å½“æµæ°´çº¿åå‘ä¼ æ’­æ—¶å€™ï¼Œåˆ©ç”¨ _TENSOR_MODEL_PARALLEL_GROUP è¿›è¡Œåœ¨ç»„å†…è¿›è¡Œé›†åˆé€šä¿¡</u>ã€‚</p>
<h2 id="0x06-pipe-parallel">0x06 Pipe-parallel</h2>
<p>æœ¬èŠ‚æˆ‘ä»¬åˆ†æçš„æ˜¯ï¼Œå¦‚ä½•å°† Node ä¸Šçš„ GPU åˆ†ç»™ pipeline model å¹¶è¡Œç»„ã€‚</p>
<h3 id="61-åˆ†ç»„">6.1 åˆ†ç»„</h3>
<p>ä»æ³¨é‡Šå¯ä»¥çœ‹åˆ°ï¼Œæµæ°´çº¿åˆ†ç»„å°±æ˜¯æŠŠè¿™ä¸ª16ä¸ªGPU åˆ†æˆ 4 ç»„ï¼Œæ¯ç»„ 4 ä¸ª GPUï¼Œå¾—åˆ°   [g0, g4, g8, g12], [g1, g5, g9, g13], [g2, g6, g10, g14], [g3, g7, g11, g15]ï¼Œæˆ‘ä»¬å¾—åˆ°äº†å¦‚ä¸‹ä¿¡æ¯ï¼š</p>
<ul>
<li>
<p>æ¯ç»„çš„å››ä¸ªGPUè¿›è¡Œæ¨¡å‹æµæ°´çº¿å¹¶è¡Œï¼Œæ‰€ä»¥ pipeline_model_parallel_size = 4ã€‚å°±æ˜¯ Notation ä¹‹ä¸­çš„ pã€‚å…¶å®ï¼Œå°±æ˜¯æµæ°´çº¿æ·±åº¦ä¸º 4ï¼Œ æ¯ç»„å†… 4 ä¸ª GPU æ˜¯ä¸²è¡Œçš„ã€‚å³ï¼Œ [g0, g4, g8, g12] è¿™4ä¸ª GPUæ˜¯ä¸²è¡Œçš„ã€‚</p>
</li>
<li>
<p>å†çœ‹çœ‹æµæ°´çº¿çš„æ¯ä¸€å±‚ï¼Œå«æœ‰ 16 / 4 = 4 ä¸ª GPUï¼Œèƒ½çœ‹åˆ°ç¬¬ä¸€å±‚æ˜¯ 0 ~ 4ï¼Œç¬¬äºŒå±‚æ˜¯ 5 ~ 8ï¼Œ......ã€‚</p>
</li>
<li>
<p>å¯ä»¥çœ‹åˆ°ï¼Œæµæ°´çº¿çš„ groupæ˜¯éš” n // pä¸ªå–ä¸€ä¸ªï¼Œæ¯”å¦‚[0, 4, 8, 12]ã€‚</p>
</li>
<li>
<p>å¯¹äºæµæ°´çº¿æ¯ä¸ªstageï¼Œåˆ™æ˜¯stage i çš„ rank èŒƒå›´æ˜¯ï¼š[(i-1) * n//p, (i) * n//p]ï¼Œå³ rank 2 æ‰€åœ¨çš„stage çš„rankæ˜¯ [0,1,2,3]ã€‚</p>
</li>
<li>
<p>_PIPELINE_MODEL_PARALLEL_GROUP å¾—åˆ°äº†æœ¬rankå¯¹åº”çš„æµæ°´çº¿è¿›ç¨‹ç»„ã€‚</p>
</li>
<li>
<p>_PIPELINE_GLOBAL_RANKS å¾—åˆ°äº†è¿›ç¨‹ç»„çš„ranksã€‚</p>
</li>
<li>
<p><u>å‡å¦‚æœ¬è¿›ç¨‹æ˜¯ rank 2ï¼Œåˆ™æµæ°´çº¿è¿›ç¨‹ç»„ ranks æ˜¯ [g2, g6, g10, g14]</u>ã€‚</p>
</li>
</ul>
<p>å…·ä½“ä»£ç å¦‚ä¸‹ï¼š</p>
<pre><code class="language-python">    # Build the pipeline model-parallel groups and embedding groups
    # (first and last rank in each pipeline model-parallel group).
    global _PIPELINE_MODEL_PARALLEL_GROUP
    global _PIPELINE_GLOBAL_RANKS
    global _EMBEDDING_GROUP
    for i in range(num_pipeline_model_parallel_groups): # 4
        ranks = range(i, world_size, # æ¯éš” n // pä¸ªå–ä¸€ä¸ª
                      num_pipeline_model_parallel_groups)
        group = torch.distributed.new_group(ranks)
        if rank in ranks:
            _PIPELINE_MODEL_PARALLEL_GROUP = group
            _PIPELINE_GLOBAL_RANKS = ranks
        # Setup embedding group (to exchange gradients between
        # first and last stages).
        if len(ranks) &gt; 1:
            embedding_ranks = [ranks[0], ranks[-1]]
        else:
            embedding_ranks = ranks
        group = torch.distributed.new_group(embedding_ranks)
        if rank in embedding_ranks:
            _EMBEDDING_GROUP = group

</code></pre>
<p>æˆ‘ä»¬æ‹“å±•ä¹‹å‰å›¾å¦‚ä¸‹ï¼Œç°åœ¨çœ‹åˆ°å¢åŠ äº† 4 æ¡ä»ä¸Šåˆ°ä¸‹çš„è™šçº¿ç®­å¤´ï¼Œåˆ†åˆ«å¯¹åº”äº† 4 ç»„æµæ°´çº¿ä¸²è¡Œã€‚æ¨ªå‘å±‚æ˜¯ä» Stage 0 ~ Stage 3ã€‚</p>
<p><img src="https://img2022.cnblogs.com/blog/1850883/202202/1850883-20220209204825760-164293937.jpg" alt="" loading="lazy"></p>
<h3 id="62-ä½¿ç”¨">6.2 ä½¿ç”¨</h3>
<p>æ¥ä¸‹æ¥çœ‹çœ‹å¦‚ä½•ä½¿ç”¨ã€‚</p>
<p>get_pipeline_model_parallel_group è¿”å›äº†è‡ªå·± rank å¯¹åº”çš„ pipeline model groupã€‚</p>
<pre><code class="language-python">def get_pipeline_model_parallel_group():
    """Get the pipeline model parallel group the caller rank belongs to."""
    return _PIPELINE_MODEL_PARALLEL_GROUP

</code></pre>
<p>å…·ä½“ä½¿ç”¨æ˜¯åœ¨ megatron/p2p_communication.pyï¼Œ_communicate ä¹‹ä¸­ä¼šç”¨æµæ°´çº¿ç»„ä¿¡æ¯æ¥è¿›è¡Œé€šä¿¡ã€‚è¿™é‡Œçœç•¥äº†å¤§éƒ¨åˆ†ä»£ç ã€‚</p>
<pre><code class="language-python">def _communicate(tensor_send_next, tensor_send_prev, recv_prev, recv_next,
                 use_ring_exchange=False, tensor_shape=None,
                 override_scatter_gather_tensors_in_pipeline=False,
                 dtype_=None):
    """Communicate tensors between stages. Used as helper method in other
    communication methods that are used in megatron/schedules.py.
    """

    # Send tensors in both the forward and backward directions as appropriate.
    if use_ring_exchange: # è¿™é‡Œä½¿ç”¨get_pipeline_model_parallel_group è¿›è¡Œé€šä¿¡
        torch.distributed.ring_exchange(tensor_send_prev=tensor_send_prev,
                                        tensor_recv_prev=tensor_recv_prev,
                                        tensor_send_next=tensor_send_next,
                                        tensor_recv_next=tensor_recv_next,
                                        group=mpu.get_pipeline_model_parallel_group())
    else:
        ops = []
        if tensor_send_prev is not None:
            send_prev_op = torch.distributed.P2POp(
                torch.distributed.isend, tensor_send_prev,
                mpu.get_pipeline_model_parallel_prev_rank()) # å¾—åˆ°æµæ°´çº¿å‰ä¸€ä¸ªrank
            ops.append(send_prev_op)
        if tensor_recv_prev is not None:
            recv_prev_op = torch.distributed.P2POp(
                torch.distributed.irecv, tensor_recv_prev,
                mpu.get_pipeline_model_parallel_prev_rank())
            ops.append(recv_prev_op)
        if tensor_send_next is not None:
            send_next_op = torch.distributed.P2POp(
                torch.distributed.isend, tensor_send_next,
                mpu.get_pipeline_model_parallel_next_rank()) # å¾—åˆ°æµæ°´çº¿ä¸‹ä¸€ä¸ªrank
            ops.append(send_next_op)
        if tensor_recv_next is not None:
            recv_next_op = torch.distributed.P2POp(
                torch.distributed.irecv, tensor_recv_next,
                mpu.get_pipeline_model_parallel_next_rank())
            ops.append(recv_next_op)


</code></pre>
<h4 id="621-ä¸Šä¸‹æ¸¸rank">6.2.1 ä¸Šä¸‹æ¸¸rank</h4>
<p>å…·ä½“å¦‚ä½•å¾—åˆ°æµæ°´çº¿ä¸Šä¸‹æ¸¸çš„rankï¼Ÿæ˜¯é€šè¿‡ get_pipeline_model_parallel_next_rank å’Œ get_pipeline_model_parallel_prev_rank æ¥å®Œæˆã€‚å…¶ä¸­_PIPELINE_GLOBAL_RANKS å¾—åˆ°äº†è¿›ç¨‹ç»„çš„ranksï¼Œå‡å¦‚æœ¬è¿›ç¨‹æ˜¯ rank 2ï¼Œåˆ™æµæ°´çº¿è¿›ç¨‹ç»„ ranks æ˜¯ [g2, g6, g10, g14]ã€‚</p>
<pre><code class="language-python">def get_pipeline_model_parallel_next_rank():
    rank_in_pipeline = get_pipeline_model_parallel_rank()
    world_size = get_pipeline_model_parallel_world_size()
    return _PIPELINE_GLOBAL_RANKS[(rank_in_pipeline + 1) % world_size]

def get_pipeline_model_parallel_prev_rank():
    rank_in_pipeline = get_pipeline_model_parallel_rank()
    world_size = get_pipeline_model_parallel_world_size()
    return _PIPELINE_GLOBAL_RANKS[(rank_in_pipeline - 1) % world_size]

</code></pre>
<h4 id="622-world-size">6.2.2 world size</h4>
<p>get_pipeline_model_parallel_world_size å¾—åˆ°äº†è¿›ç¨‹ç»„çš„ world sizeã€‚</p>
<pre><code class="language-python">def get_pipeline_model_parallel_world_size():
    """Return world size for the pipeline model parallel group."""
    global _MPU_PIPELINE_MODEL_PARALLEL_WORLD_SIZE
    if _MPU_PIPELINE_MODEL_PARALLEL_WORLD_SIZE is not None:
        return _MPU_PIPELINE_MODEL_PARALLEL_WORLD_SIZE
    return torch.distributed.get_world_size(group=get_pipeline_model_parallel_group())

</code></pre>
<h2 id="0x07-data-parallel">0x07 Data-parallel</h2>
<p>æˆ‘ä»¬æ¥ä¸‹æ¥çœ‹çœ‹æ•°æ®å¹¶è¡Œã€‚</p>
<h3 id="71-åˆ†ç»„">7.1 åˆ†ç»„</h3>
<p>å¯¹äºæ³¨é‡Šä¾‹å­ï¼Œ16 / 2 = 8ï¼Œåˆ†æˆ 8 ä¸ªè¿›ç¨‹ç»„ï¼Œæ¯ä¸ªç»„ ä¸¤ä¸ª rankã€‚è¿™äº›åˆ†ç»„åˆ†åˆ«æ˜¯ï¼š[g0, g2], [g1, g3], [g4, g6], [g5, g7], [g8, g10], [g9, g11], [g12, g14], [g13, g15]ï¼Œæˆ‘ä»¬å¾—åˆ°äº†å¦‚ä¸‹ä¿¡æ¯ï¼š</p>
<ul>
<li>ä¾æ®ä¸Šé¢åˆ†æï¼Œ t * p å°±æ˜¯ä¸€ä¸ªæ¨¡å‹æ‰€éœ€è¦çš„ GPUï¼Œå› æ­¤ï¼Œd = (æ€» GPU æ•°ç›® / ä¸€ä¸ªæ¨¡å‹éœ€è¦çš„ GPU æ•°ç›®) =  n / ( t * p)ï¼Œå°±æ˜¯è¯´ï¼Œç›®å‰æä¾›çš„è¿™ n ä¸ªGPUå¯ä»¥åŒæ—¶è®­ç»ƒ d ä¸ªæ¨¡å‹ï¼Œå°±æ˜¯å¯ä»¥ç”¨ d ä¸ª mini-batches è¾“å…¥åˆ°è¿™ d ä¸ªæ¨¡å‹ä¸€èµ·è®­ç»ƒï¼Œæ‰€ä»¥æ•°æ®å¹¶è¡Œåº¦ä¸º dã€‚</li>
<li>å¯¹åº”æ³¨é‡Šä¾‹å­ï¼Œå°±æ˜¯data_parallel_size = 16 / (2 * 4) = 2ã€‚</li>
<li>rank 2 å¯¹åº”çš„æ•°æ®å¹¶è¡Œè¿›ç¨‹ç»„æ˜¯[g0, g2]ã€‚</li>
</ul>
<p>æˆ‘ä»¬å†çœ‹çœ‹ç”¨ä»£ç æ€ä¹ˆç¡®å®šæœ‰å“ªäº›groupï¼Œæ¯ä¸ªgroupé‡Œé¢åŒ…å«ä»€ä¹ˆã€‚</p>
<ul>
<li>é¦–å…ˆï¼Œæµæ°´çº¿è¢«åˆ†æˆäº† p ä¸ª stageï¼Œå¯¹äºæµæ°´çº¿æ¯ä¸ªstageï¼Œå…¶æœ‰ n // p ä¸ªGPUï¼Œstage i çš„ rank èŒƒå›´æ˜¯ï¼š[i * n//p, (i+1) * n//p]ï¼Œå³ rank 2æ‰€åœ¨çš„stage çš„rankæ˜¯ [0,1,2,3]ã€‚</li>
<li>å…¶æ¬¡ï¼Œåœ¨æ¯ä¸€ä¸ªstageä¹‹ä¸­ï¼Œ<code>ranks = range(start_rank + j, end_rank, tensor_model_parallel_size)</code> ï¼Œæ„æ€æ˜¯è¿™stageçš„n//pä¸ªGPUsä¸­ï¼Œæ¯éš” t ä¸ªå–ä¸€ä¸ªä½œä¸ºæ•°æ®å¹¶è¡Œ group ä¹‹ä¸­çš„ä¸€ä»½å­ï¼Œå› æ­¤æ¯ä¸ªdata-parallel groupå¤§å°ä¸º n // p // t = dã€‚</li>
</ul>
<p>å…·ä½“ä»£ç å¦‚ä¸‹ï¼š</p>
<pre><code class="language-python">    # Build the data-parallel groups.
    global _DATA_PARALLEL_GROUP
    assert _DATA_PARALLEL_GROUP is None, \
        'data parallel group is already initialized'
    all_data_parallel_group_ranks = []
    for i in range(pipeline_model_parallel_size): # éå†æµæ°´çº¿æ·±åº¦
        start_rank = i * num_pipeline_model_parallel_groups # æ‰¾åˆ°æ¯ä¸ªstageçš„èµ·å§‹rank
        end_rank = (i + 1) * num_pipeline_model_parallel_groups # æ‰¾åˆ°æ¯ä¸ªstageçš„ç»ˆæ­¢rank
        for j in range(tensor_model_parallel_size): # éå†tensor modelåˆ†ç»„size
            ranks = range(start_rank + j, end_rank, # æ¯éš” t ä¸ªå–ä¸€ä¸ªä½œä¸ºæ•°æ®å¹¶è¡Œgroupä¸­çš„ä¸€ä»½å­
                          tensor_model_parallel_size)
            all_data_parallel_group_ranks.append(list(ranks))
            group = torch.distributed.new_group(ranks)
            if rank in ranks:
                _DATA_PARALLEL_GROUP = group

</code></pre>
<p>æ‰“å°è¾“å‡ºå¦‚ä¸‹ï¼Œå’Œæ³¨é‡Šä¸€è‡´ã€‚</p>
<pre><code class="language-python">------ Build the data-parallel groups -----
[[0, 2], [1, 3], [4, 6], [5, 7], [8, 10], [9, 11], [12, 14], [13, 15]]

</code></pre>
<p>å¯¹åº”å›¾ç‰‡æ‹“å±•å¦‚ä¸‹ï¼šå…¶ä¸­ï¼Œæ¯ä¸ªæ–°å¢çš„åŒç®­å¤´å¯¹åº”ä¸€ä¸ªDDPï¼ˆä¸¤ä¸ªrankï¼‰ï¼Œæ¯”å¦‚[2, 3]å¯¹åº”ä¸€ä¸ªDDPã€‚</p>
<p><img src="https://img2022.cnblogs.com/blog/1850883/202202/1850883-20220209204840550-2116456050.jpg" alt="" loading="lazy"></p>
<h3 id="72-å¦‚ä½•ä½¿ç”¨">7.2 å¦‚ä½•ä½¿ç”¨</h3>
<p>æˆ‘ä»¬æ¥ä¸‹æ¥çœ‹çœ‹å¦‚ä½•ä½¿ç”¨ã€‚</p>
<p>get_data_parallel_group ä¼šå¾—åˆ°æœ¬rankå¯¹åº”çš„ _DATA_PARALLEL_GROUPã€‚</p>
<pre><code class="language-python">def get_data_parallel_group():
    """Get the data parallel group the caller rank belongs to."""
    return _DATA_PARALLEL_GROUP

</code></pre>
<p>åœ¨ allreduce_gradientsä¹‹ä¸­ï¼Œä¼šå¯¹æœ¬æ•°æ®å¹¶è¡Œç»„è¿›è¡Œall-reduceã€‚</p>
<pre><code class="language-python">    def allreduce_gradients(self):
        """Reduce gradients across data parallel ranks."""
        # If we have buffers, simply reduce the data in the buffer.
        if self._grad_buffers is not None:
            for _, buffer_ in self._grad_buffers.items():
                buffer_.data /= mpu.get_data_parallel_world_size() # æ•°æ®å¹¶è¡Œ world size
                torch.distributed.all_reduce(
                    buffer_.data, group=mpu.get_data_parallel_group()) # æ•°æ®å¹¶è¡Œç»„
        else:
            # Otherwise, bucketize and all-reduce
            buckets = {}
            # Pack the buckets.
            for param in self.module.parameters():
                if param.requires_grad and param.grad is not None:
                    tp = param.data.type()
                    if tp not in buckets:
                        buckets[tp] = []
                    buckets[tp].append(param)
                    param.main_grad = param.grad

            # For each bucket, all-reduce and copy all-reduced grads.
            for tp in buckets:
                bucket = buckets[tp]
                grads = [param.grad.data for param in bucket]
                coalesced = _flatten_dense_tensors(grads)
                coalesced /= mpu.get_data_parallel_world_size()
                torch.distributed.all_reduce(
                    coalesced, group=mpu.get_data_parallel_group())
                for buf, synced in zip(grads, _unflatten_dense_tensors(
                        coalesced, grads)):
                    buf.copy_(synced)

</code></pre>
<h2 id="0x08-æ¨¡å‹ç»„">0x08 æ¨¡å‹ç»„</h2>
<p>å‰é¢å®éªŒä¸­ï¼Œæˆ‘ä»¬å¾—åˆ°æ¨¡å‹å¹¶è¡Œç»„å¦‚ä¸‹ï¼š[0, 1, 4, 5, 8, 9, 12, 13] [2, 3, 6, 7, 10, 11, 14, 15]ã€‚ç”Ÿæˆä»£ç å¦‚ä¸‹ï¼š</p>
<pre><code class="language-python">    # Build the model-parallel groups.
    global _MODEL_PARALLEL_GROUP
    for i in range(data_parallel_size):
        ranks = [data_parallel_group_ranks[i]
                 for data_parallel_group_ranks in all_data_parallel_group_ranks]
        group = torch.distributed.new_group(ranks)
        if rank in ranks:
            _MODEL_PARALLEL_GROUP = group

</code></pre>
<p>_MODEL_PARALLEL_GROUP ä¼šå¾—åˆ°æœ¬rankå¯¹åº”çš„æ¨¡å‹ç»„ã€‚</p>
<pre><code class="language-python">def get_model_parallel_group():
    """Get the model parallel group the caller rank belongs to."""
    return _MODEL_PARALLEL_GROUP

</code></pre>
<p>è¿™é‡Œæ˜¯è£å‰ªæ¢¯åº¦ä¼šç”¨åˆ°ï¼Œå°±æ˜¯åœ¨æœ¬æ¨¡å‹çš„å…¨éƒ¨rankä¹‹ä¸­è¿›è¡Œæ¢¯åº¦è£å‰ªç›¸å…³æ“ä½œã€‚</p>
<pre><code class="language-python">def clip_grad_norm_fp32(parameters, max_norm, norm_type=2):
    """Clips gradient norm of an iterable of parameters whose gradients
       are in fp32.

    This is adapted from torch.nn.utils.clip_grad.clip_grad_norm_ and
    added functionality to handle model parallel parameters. Note that
    the gradients are modified in place.

    Arguments:
        parameters (Iterable[Tensor] or Tensor): an iterable of Tensors or a
            single Tensor that will have gradients normalized
        max_norm (float or int): max norm of the gradients
        norm_type (float or int): type of the used p-norm. Can be ``'inf'`` for
            infinity norm.

    Returns:
        Total norm of the parameters (viewed as a single vector).
    """

    if isinstance(parameters, torch.Tensor):
        parameters = [parameters]

    # Filter parameters based on:
    #   - grad should not be none
    #   - parameter should not be shared
    #   - should not be a replica due to tensor model parallelism
    grads = []
    grads_for_norm = []
    for param in parameters:
        grad_not_none = param.grad is not None
        is_not_shared = param_is_not_shared(param)
        is_not_tp_duplicate = param_is_not_tensor_parallel_duplicate(param)
        grad = param.grad.detach()
        if grad_not_none:
            # Make sure the grads are in fp32
            grads.append(grad)
        if grad_not_none and is_not_shared and is_not_tp_duplicate:
            grads_for_norm.append(grad)

    # Norm parameters.
    max_norm = float(max_norm)
    norm_type = float(norm_type)
    total_norm = 0.0

    # Calculate norm.
    if norm_type == inf:
        total_norm = max(grad.abs().max() for grad in grads_for_norm)
        total_norm_cuda = torch.cuda.FloatTensor([float(total_norm)])
        # Take max across all model-parallel GPUs.
        torch.distributed.all_reduce(total_norm_cuda,
                                     op=torch.distributed.ReduceOp.MAX,
                                     group=mpu.get_model_parallel_group()) # æ¨¡å‹ç»„ä¿¡æ¯
        total_norm = total_norm_cuda[0].item()

    else:
        if norm_type == 2.0:
            dummy_overflow_buf = torch.cuda.IntTensor([0])
            # Use apex's multi-tensor applier for efficiency reasons.
            # Multi-tensor applier takes a function and a list of list
            # and performs the operation on that list all in one kernel.
            grad_norm, _ = multi_tensor_applier(
                amp_C.multi_tensor_l2norm,
                dummy_overflow_buf,
                [grads_for_norm],
                False # no per-parameter norm
            )
            # Since we will be summing across data parallel groups,
            # we need the pow(norm-type).
            total_norm = grad_norm ** norm_type

        else:
            for grad in grads_for_norm:
                grad_norm = torch.norm(grad, norm_type)
                total_norm += grad_norm ** norm_type

        # Sum across all model-parallel GPUs.
        torch.distributed.all_reduce(total_norm,
                                     op=torch.distributed.ReduceOp.SUM,
                                     group=mpu.get_model_parallel_group()) # æ¨¡å‹ç»„ä¿¡æ¯
        total_norm = total_norm.item() ** (1.0 / norm_type)

    # Scale.
    clip_coeff = max_norm / (total_norm + 1.0e-6)
    if clip_coeff &lt; 1.0:
        dummy_overflow_buf = torch.cuda.IntTensor([0])
        multi_tensor_applier(amp_C.multi_tensor_scale,
                             dummy_overflow_buf,
                             [grads, grads],
                             clip_coeff)

    return total_norm

</code></pre>
<p>ä¹‹å‰çš„å›¾å¦‚ä¸‹ï¼Œåˆ©ç”¨çœ‹åˆ°åˆ†æˆä¸¤ç»„ï¼Œå·¦è¾¹æ˜¯Model 0 å¯¹åº”çš„å…¨éƒ¨ranksï¼Œå³é¢æ˜¯model 1 çš„ranksã€‚</p>
<p><img src="https://img2022.cnblogs.com/blog/1850883/202202/1850883-20220209204849363-1362616656.jpg" alt="" loading="lazy"></p>
<h2 id="0x09-å¦‚ä½•æŠŠæ¨¡å‹åˆ†åˆ°gpu">0x09 å¦‚ä½•æŠŠæ¨¡å‹åˆ†åˆ°GPU</h2>
<p>æˆ‘ä»¬æœ€åè¿˜æœ‰ä¸€ä¸ªé—®é¢˜æ²¡æœ‰æ¶‰åŠï¼Œå°±æ˜¯å¦‚ä½•æŠŠæ¨¡å‹åˆ†å—æ”¾åˆ°å¯¹åº”çš„GPUä¹‹ä¸Šã€‚å°±æ˜¯å¦‚ä½•ä¸æœ€åˆåˆ†æˆAï¼ŒBï¼Œ..., H çš„é‚£ä¸ªå›¾å¯¹åº”èµ·æ¥ã€‚å…¶å®ï¼Œ<u>ä¸æ˜¯æ ¹æ®æ¨¡å‹æ¥æŠŠæ¨¡å‹éƒ¨åˆ†æ‹·è´åˆ°å¯¹åº”çš„rankæˆ–è€…GPUï¼Œè€Œæ˜¯rankæˆ–è€…GPUä¸»åŠ¨è¿‡æ¥æ‹·è´è‡ªå·±å¯¹åº”çš„å±‚</u>ã€‚</p>
<ul>
<li>å› ä¸ºè°ƒç”¨äº† mpu.initialize_model_parallel æ¥è®¾ç½®æ¨¡å‹å¹¶è¡Œï¼Œæ•°æ®å¹¶è¡Œç­‰å„ç§è¿›ç¨‹ç»„ï¼Œæ‰€ä»¥æ¯ä¸ª rank å¯¹åº”çš„è¿›ç¨‹éƒ½æœ‰è‡ªå·±çš„å…¨å±€å˜é‡ï¼Œå…·ä½“å…¶å®å°±æ˜¯è¿›ç¨‹è‡ªåŠ¨å°±è¢«æ˜ å°„åˆ°GPUä¸Šäº†ã€‚æ¯”å¦‚ rank 2 å¯¹åº”çš„è¿›ç¨‹åœ¨å¯åŠ¨ä¹‹åæ‰çŸ¥é“è‡ªå·±æ˜¯ rank 2ï¼Œç„¶åä»åˆå§‹åŒ–çš„å…¨å±€å˜é‡ä¹‹ä¸­çŸ¥é“è‡ªå·±çš„ data_parallel group æ˜¯ [g0, g2]ï¼Œtensor model-parallel group æ˜¯[g2, g3]ï¼Œpipeline model-parallel group æ˜¯ [g2, g6, g10, g14]ã€‚</li>
<li>ParallelTransformer çš„åˆå§‹åŒ–ä¹‹ä¸­ï¼Œoffset å°±æ˜¯æ ¹æ® rank çŸ¥é“è‡ªå·±åº”è¯¥ç”Ÿæˆæ¨¡å‹çš„é‚£äº›å±‚ï¼Œç„¶åé€šè¿‡ self.layers = torch.nn.ModuleList([build_layer(i + 1 + offset) for i in range(self.num_layers)]) æ¥ç”Ÿæˆå¯¹åº”çš„å±‚ã€‚</li>
<li>get_model æ–¹æ³•ä¹Ÿä¼šæ ¹æ®è‡ªå·±çš„ pipeline rank å’Œ is_pipeline_first_stage æ¥çŸ¥é“æ˜¯ä¸æ˜¯ç¬¬ä¸€å±‚æˆ–è€…æœ€åä¸€å±‚ï¼Œç„¶ååšç›¸åº”å¤„ç†ã€‚</li>
<li>æœ€åæŠŠæ¨¡å‹å‚æ•°æ‹·è´åˆ°äº†è‡ªå·±å¯¹åº”çš„ GPU ä¹‹ä¸Šã€‚</li>
</ul>
<p>å…·ä½“ ParallelTransformer åˆå§‹åŒ–ä»£ç å¦‚ä¸‹ï¼š</p>
<pre><code class="language-python">class ParallelTransformer(MegatronModule):
    """Transformer class."""

    def __init__(self, init_method, output_layer_init_method,
                 layer_type=LayerType.encoder,
                 self_attn_mask_type=AttnMaskType.padding,
                 pre_process=True, post_process=True):
        super(ParallelTransformer, self).__init__()
        args = get_args()
        
        # çœç•¥ä»£ç 
        
        # Transformer layers.
        def build_layer(layer_number):
            return ParallelTransformerLayer(
                init_method,
                output_layer_init_method,
                layer_number,
                layer_type=layer_type,
                self_attn_mask_type=self_attn_mask_type)
      
        # ä¸‹é¢ offset å°±æ˜¯æ ¹æ®rankçŸ¥é“è‡ªå·±åº”è¯¥ç”Ÿæˆæ¨¡å‹çš„é‚£äº›å±‚
        if args.virtual_pipeline_model_parallel_size is not None:
            # Number of layers in each model chunk is the number of layers in the stage,
            # divided by the number of model chunks in a stage.
            self.num_layers = self.num_layers // args.virtual_pipeline_model_parallel_size
            # With 8 layers, 2 stages, and 4 model chunks, we want an assignment of
            # layers to stages like (each list is a model chunk):
            # Stage 0: [0]  [2]  [4]  [6]
            # Stage 1: [1]  [3]  [5]  [7]
            # With 8 layers, 2 stages, and 2 virtual stages, we want an assignment of
            # layers to stages like (each list is a model chunk):
            # Stage 0: [0, 1]  [4, 5]
            # Stage 1: [2, 3]  [6, 7]
            offset = mpu.get_virtual_pipeline_model_parallel_rank() * (
                args.num_layers // args.virtual_pipeline_model_parallel_size) + \
                (mpu.get_pipeline_model_parallel_rank() * self.num_layers)
        else:
            # Each stage gets a contiguous set of layers.
            offset = mpu.get_pipeline_model_parallel_rank() * self.num_layers

        self.layers = torch.nn.ModuleList(
            [build_layer(i + 1 + offset) for i in range(self.num_layers)])

        if self.post_process:
            # Final layer norm before output.
            self.final_layernorm = LayerNorm(
                args.hidden_size,
                eps=args.layernorm_epsilon)

</code></pre>
<p>æ‰€ä»¥ï¼Œæœ€ç»ˆæ•ˆæœå¦‚ä¸‹ï¼Œå…¶ä¸­åŒåå­æ¨¡å—å…·æœ‰åŒæ ·çš„å‚æ•°ï¼Œå¯ä»¥æ•°æ®å¹¶è¡Œï¼Œå³ä¸¤ä¸ªAå¯ä»¥æ•°æ®å¹¶è¡Œã€‚ä¸€åˆ—ä¸Šçš„å±‚ä¹‹é—´å¯ä»¥æµæ°´çº¿ä¸²è¡Œï¼Œæ¯”å¦‚  A--&gt; C --&gt; E --&gt; G å°±æ˜¯ä¸²è¡Œï¼Œè€Œä¸€ä¸ªæ¨ªè¡Œ4ä¸ªæ˜¯æµæ°´çº¿çš„ä¸€ä¸ªstageï¼Œå…¶ä¸­ä»0å¼€å§‹ï¼Œæ¨ªå‘ç›¸é‚»ä¸¤ä¸ªGPUæ˜¯ tensor model å¹¶è¡Œã€‚</p>
<p><img src="https://img2022.cnblogs.com/blog/1850883/202202/1850883-20220209204859943-717373290.jpg" alt="" loading="lazy"></p>
<h2 id="0xff-å‚è€ƒ">0xFF å‚è€ƒ</h2>
<p><a href="https://zhuanlan.zhihu.com/p/388830967" target="_blank" rel="noopener nofollow">[ç»†è¯»ç»å…¸]Megatronè®ºæ–‡å’Œä»£ç è¯¦ç»†åˆ†æ(2)</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/366906920" target="_blank" rel="noopener nofollow">[ç»†è¯»ç»å…¸]Megatronè®ºæ–‡å’Œä»£ç è¯¦ç»†åˆ†æ(1)</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/405883984" target="_blank" rel="noopener nofollow">Megatron-LMæºç é˜…è¯»ï¼ˆä¸€ï¼‰</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/407094090" target="_blank" rel="noopener nofollow">Megatron-LMæºç é˜…è¯»ï¼ˆäºŒï¼‰</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/381326200" target="_blank" rel="noopener nofollow">megatronå­¦ä¹ æ€»ç»“</a></p>
<p><a href="https://developer.nvidia.com/gtc/2020/video/s21496" target="_blank" rel="noopener nofollow">GTC 2020: Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</a></p>
<p><a href="https://www.zhihu.com/question/42770716" target="_blank" rel="noopener nofollow">å¦‚ä½•è¯„ä»· NVIDIA å‘å¸ƒçš„ DGX-1ï¼Ÿ</a></p>

</div>
<div class="clear"></div>
<div id="blog_post_info_block" role="contentinfo">
    <div id="blog_post_info"></div>
    <div class="clear"></div>
    <div id="post_next_prev"></div>
</div>
            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="1183.9247430124467" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2022-02-10 18:42">2022-02-10 18:42</span>&nbsp;
<a href="https://www.cnblogs.com/rossiXYZ">ç½—è¥¿çš„æ€è€ƒ</a>&nbsp;
é˜…è¯»(<span id="post_view_count">8368</span>)&nbsp;
è¯„è®º(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(15876714);return false;">æ”¶è—</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '15876714', targetLink: 'https://www.cnblogs.com/rossiXYZ/p/15876714.html', title: '[æºç è§£æ] æ¨¡å‹å¹¶è¡Œåˆ†å¸ƒå¼è®­ç»ƒ Megatron (4) --- å¦‚ä½•è®¾ç½®å„ç§å¹¶è¡Œ' })">ä¸¾æŠ¥</a>
</div>
        </div>
        <script>
    var cb_entryId = 15876714, cb_entryCreatedDate = '2022-02-10 18:42', cb_postType = 1, cb_postTitle = '[æºç è§£æ] æ¨¡å‹å¹¶è¡Œåˆ†å¸ƒå¼è®­ç»ƒ Megatron (4) --- å¦‚ä½•è®¾ç½®å„ç§å¹¶è¡Œ';
    var allowComments = true, cb_blogId = 556264, cb_blogApp = 'rossiXYZ', cb_blogUserGuid = '3d1961d5-3b13-4975-9d25-08d753a9a8fd';
    mermaidRender.render()
    markdown_highlight()
    zoomManager.apply("#cnblogs_post_body img:not(.code_img_closed):not(.code_img_opened)");    
</script>
        <a id="!comments"></a>
<div id="blog-comments-placeholder"></div>
<div id="comment_form" class="commentform">
    <a name="commentform"></a>
    <div id="divCommentShow"></div>
    <div id="comment_nav"> 
        <div class="comment-nav-right">
            <span id="span_refresh_tips"></span><a href="#" onclick="return RefreshPage();">åˆ·æ–°é¡µé¢</a><a href="#top">è¿”å›é¡¶éƒ¨</a>
        </div>
    </div>
    <div id="comment_form_container"></div>
    <div class="ad_text_commentbox" id="ad_text_under_commentbox"></div>
        <div id="cnblogs_ch"></div>
    <div id="opt_under_post"></div>
        <div id="blog_c1" class="under-post-card">
            <a href="https://www.doubao.com?channel=cnblogs&amp;source=hw_db_cnblogs&amp;type=lunt&amp;theme=bianc" rel="nofollow" target="_blank" onclick="countCreativeClicks('C1-å­—èŠ‚-è±†åŒ…')">
                <img src="https://img2024.cnblogs.com/blog/35695/202412/35695-20241201072501456-2052907165.jpg" onload="countCreativeImpressions('C1-å­—èŠ‚-è±†åŒ…')" alt="" />
                <span id="c1_impression" style="display:none"></span>
            </a>
        </div>
    <div id="under_post_card1"></div>
    <div id="under_post_card2"></div>
    <div id="HistoryToday" class="under-post-card"></div>
    <script type="text/javascript">
        var commentManager = new blogCommentManager();
        commentManager.renderComments(0);
        fixPostBody();
        window.footnoteTipManager.generateFootnoteTips();

            window.tocManager.displayDisableTocTips = false;
            window.tocManager.generateToc();
            
                setTimeout(function() { countViews(cb_blogId, cb_entryId); }, 50);
            
            deliverT2();
            deliverC1C2();
            loadNewsAndKb();
            
                LoadPostCategoriesTags(cb_blogId, cb_entryId);
            
            LoadPostInfoBlock(cb_blogId, cb_entryId, cb_blogApp, cb_blogUserGuid);
            GetPrevNextPost(cb_entryId, cb_blogId, cb_entryCreatedDate, cb_postType);
            loadOptUnderPost();
            GetHistoryToday(cb_blogId, cb_blogApp, cb_entryCreatedDate);
                </script>
</div>

    </div>
</div>
            </div>
        </div>

        <div id="sideBar">
            <div id="sideBarMain">
                <div id="sidebar_news" class="newsItem">
    
<h3 class="catListTitle">å…¬å‘Š</h3>
<div id="blog-news" class="sidebar-news">
    <div id="sidebar_news_container">
    </div>
</div>
<script>loadBlogNews();</script> 
</div>
<div id="sidebar_c3"></div>
                <div id="calendar"><div id="blog-calendar" style="display:none"></div></div>                
                <script>loadBlogDefaultCalendar();</script>
                <div id="leftcontentcontainer">
                    <!-- begin:SingleColumn -->
                    <div id="blog-sidecolumn"></div>
                    <script>loadBlogSideColumn();</script>
                    <!-- end:  SingleColumn -->
                </div>
            </div>
        </div>
        <div class="clear"></div>
    </div>
    <div class="clear"></div>
    <div id="footer">
        <!--done-->
Copyright &copy; 2025 ç½—è¥¿çš„æ€è€ƒ
<br /><span id="poweredby">Powered by .NET 9.0 on Kubernetes</span>

    </div>
</div>


    

    <input type="hidden" id="antiforgery_token" value="CfDJ8Ct_7-Gh-gZNte6RB_khjDrjjXbghP2tz563j1juagvPIksZVx7IUiT_WoIYtVbeAdPq6ukGtHAcm_8HeiMrIo0lJSpHo1_ZTkfs0gWzy3uR8z1IvU87nIlATurRe_x7yo3al6yeanzRODDYNbtoaBU" />
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-M95P3TTWJZ"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-M95P3TTWJZ');
</script>
<script defer src="https://hm.baidu.com/hm.js?866c9be12d4a814454792b1fd0fed295"></script>
</body>
</html>
